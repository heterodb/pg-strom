@ja:#Fluentd連携
@en:#connect with Fluentd

@ja{
本章では、Apache Arrowデータ形式を介したFluentdとの連携と、IoT/M2Mログデータの効率的な取り込みについて説明します。
}
@en{
This chapter introduces the cooperation with Fluentd via Apache Arrow data format for the efficient importing of IoT/M2M log data.
}

@ja:##概要
@en:##Overview

@ja{
IoT/M2Mとよばれる技術領域においては、PCやサーバだけでなく、携帯電話や自動車、各種センサーなどのデバイスが生成した大量のログデータを蓄積し、これを分析するためのソフトウェアが数多く開発されています。大量のデバイスが時々刻々と生成するデータは非常に大きなサイズになりがちで、これを実用的な時間内に処理するには、特別な工夫が必要となるからです。

PG-Stromの各種機能は、こういった規模のログデータを高速に処理するために設計・実装されています。
しかし一方で、こうした規模のデータの検索・集計が可能な状態にするためにデータを移送し、データベースに取り込むには時間がかかりがちです。
そこで、PG-StromにはFluentd向けにApache Arrow形式でログデータを出力する`fluent-plugin-arrow-file`モジュールを同梱し、ログデータのインポートという問題に対処を試みています。
}
@en{
In the technological domain known as IoT/M2M, various software has been developed to store and analyze the large amount of log data generated by devices such as cell phones, automobiles, and various sensors, as well as PCs and servers.
This is because the data generated by a large number of devices from time-by-time tend to grow up huge, and special architecture/technology is required to process it in a practical amount of time.

PG-Strom's features are designed and implemented for high-speed processing of log data of such a scale.
On the other hand, it tends to be a time-consuming job to transfer and import these data into a database in order to make it searchable/summarizable on such a scale.
Therefore, PG-Strom includes a `fluent-plugin-arrow-file` module for Fluentd that outputs the data in Apache Arrow format, and tries to deal with the problem.
}

![Fluentd with PG-Strom Overview](./img/fluentd_overview.png)

@ja{
Fluentdは古橋貞之氏によって開発されたログ収集ツールで、SyslogのようなサーバログからIoT/M2M機器のデバイスログに至るまで、多種多様なログデータを集積・保存するために事実上のスタンダードとして利用されているソフトウェアです。
Rubyで記述されたプラグインの追加により、ログデータの入出力や加工を自在にカスタマイズする事が可能で、2022年現在、800種類を越えるプラグインが公式Webサイトで紹介されています。

PG-Stromが取り扱うことのできるデータ形式は、PostgreSQL Heap形式（トランザクショナル行データ）とApache Arrow形式（構造化列データ）の２種類ですが、IoT/M2M領域で想定されるような、時々刻々と大量のデータが発生するようなワークロードに対しては、Apache Arrow形式を用いた方が合理的です。
}
@en{
Fluentd is a log collection tool developed by Sadayuki Furuhashi. It is the de facto standard for collecting and storing a wide variety of log data, ranging from server logs like Syslog to device logs of IoT/M2M devices.
Fluentd allows customization of the input/output and processing of log data by adding plugins written in Ruby. As of 2022, more than 800 plugins have been introduced on the official website.

PG-Strom can handle two types of data formats: PostgreSQL Heap format (transactional row data) and Apache Arrow format (structured column data). The Apache Arrow format is suitable for workloads like those in the IoT/M2M, where huge amounts of data are generated time-by-time.
}


@ja:##arrow-file プラグイン
@en:##arrow-file plugin

@ja{
以下では、Fluentdが収集したログデータをApache Arrow形式ファイルとして出力し、これをPG-Stromで参照するというアプローチについて説明します。
また、Fluentdのパッケージには、Treasure Data社の提供する安定版`fluentd`を使用するものとします。

PG-Stromに同梱のFluentd向けOutputプラグインの`fluent-plugin-arrow-file`モジュールを利用すると、Fluentdが収集したログデータを、指定したスキーマ構造を持つApache Arrow形式ファイルとして書き出すことができます。PG-StromのArrow_Fdw機能を使用すればこのApache Arrow形式ファイルを外部テーブルとして参照する事ができ、また保存先のストレージが適切に設定されていれば、GPU-Direct SQLを用いた高速な読み出しも可能です。

この方法には以下のメリットがあります。
- Fluentd が出力したデータをそのままPG-Stromで読み出せるため、改めてDBへデータをインポートする必要がない。
- 列データ形式であるため、検索・集計処理に伴うデータの読み出し（I/O負荷）を必要最小限に抑える事ができる。
- 古くなったログデータのアーカイブも、OS上のファイル移動のみで完了できる。

一方で、Apache Arrow形式で性能上のメリットを得るには、Record Batchのサイズをある程度大きくしなければならないため、ログの発生頻度が小さく、一定サイズのログが溜まるまでに時間のかかる場合には、PostgreSQLのテーブルに出力させるなど、別の方法を試した方がよりリアルタイムに近いログ分析が可能でしょう。
}

@en{
This chapter describes the approach to write out the log data collected by Fluentd in Apache Arrow format, and to refere it with PG-Strom.
We assume `fluentd` here, that is a stable version of Fluentd provided by Treasure Data.

PG-Strom includes the `fluent-plugin-arrow-file` module. This allows Fluentd to write out the log data it collects as an Apache Arrow format file with a specified schema structure.
Using PG-Strom's Arrow_Fdw, this Apache Arrow format file can be accessed as an foreign table.
In addition, GPU-Direct SQL allows to load these files extremely fast, if the storage system is appropriately configured.

This has the following advantages:

- There is no need to import data into the DB because PG-Strom directly accesses the files output by Fluentd.
- The data readout (I/O load) for the searching and summarizing process can be kept to a minimum because of the columnar data format.
- You can archive outdated log data only by moving files on the OS.

On the other hand, in cases that it takes a long time to store a certain size of log (for example, log generation is rare), another method such as outputting to a PostgreSQL table is suitable for more real-time log analysis.
This is because the size of the Record Batch needs to be reasonably large to acquire the performance benefits of the Apache Arrow format.
}

@ja:###内部構造
@en:###Internals

@ja{
Fluentdのプラグインにはいくつかカテゴリがあり、外部からログを受け取るInputプラグイン、ログを成形するParserプラグイン、受信したログを一時的に蓄積するBufferプラグイン、ログを出力するOutputプラグイン、などの種類があります。

arrow-fileはOutputプラグインの一つですが、これはBufferプラグインから渡されたログデータの固まり(chunk)を、コンフィグで指定されたスキーマ構造を持つApache Arrow形式で書き出す役割を担っています。
}
@en{
There are several types of plugins for Fluentd: Input plugins to receive logs from outside, Parser plugins to shape the logs, Buffer plugins to temporarily store the received logs, and Output plugins to output the logs.
The arrow-file plugin is categorized as Output plugin. It writes out a "chunk" of log data passed from the Buffer plugin in Apache Arrow format with the schema structure specified in the configuration.
}

![Fluentd Components](./img/fluentd_components.png)

@ja{
Input/Parserプラグインの役割は、外部から受け取ったログを共通の形式に変換し、BufferプラグインやOutputプラグインが入力データの形式を意識することなく扱えるようにすることです。

これは内部的には、ログの振り分けに利用することのできる識別子の`tag`、ログのタイムスタンプ`time`、および生ログを成形した連想配列である`record`の組です。

arrow-fileプラグインは、`tag`、`time`の各フィールドと、`record`連想配列の各要素（一部を省略することも可能）を列として持つApache Arrow形式ファイルへの書き出しを行います。
そのため、出力先のファイル名とスキーマ定義情報（連想配列の要素と列/型のマッピング）は必須の設定パラメータです。
}
@en{
The Input/Parser plugin is responsible for converting the received logs into a common format so that the Buffer and Output plugins can handle the input data without being aware of its format.

The common format is a pair of `tag`, an identifier that can be used to sort the logs, a log timestamp `time`, and `record`, an associative array formed from the raw logs.
The arrow-file plugin writes to an Apache Arrow format file with the `tag` and `time` fields and each element of the `record` associative array as a column (some may be omitted).
Therefore, the output destination file name and schema definition information (mapping of associative array elements to columns/types) are mandatory configuration parameters.
}

@ja:##インストール
@en:##Installation

@ja{
使用しているLinuxディストリビューション用の`fluentd`パッケージをインストールします。
また、arrow-fileプラグインのインストールには`rake-compiler`モジュールが必要ですので、予めインストールしておきます。

詳しくは[こちら](https://docs.fluentd.org/installation/install-by-rpm)を参照してください。
}
@en{
Install the `fluentd` package for Linux distribution you are using.
The `rake-compiler` module is required to install the arrow-file plugin, so please install it before.
}

```
$ curl -fsSL https://toolbelt.treasuredata.com/sh/install-redhat-fluent-package5-lts.sh | sh

$ sudo /opt/fluent/bin/fluent-gem install rake-compiler
```

@ja{
次に、PG-Stromのソースコードをダウンロードし、`fluentd` ディレクトリ以下の物件をビルドします。
}
@en{
Next, download the source code for PG-Strom and build arrow-file plugin in the `fluentd` directory.
}

```
$ git clone https://github.com/heterodb/pg-strom.git
$ cd pg-strom/fluentd
$ make FLUENT=1 gem
$ sudo make FLUENT=1 install
```

@ja{
Fluentdのプラグインがインストールされている事を確認するため、以下のコマンドを実行します。
}
@en{
To confirm that the Fluentd plugin is installed, run the following command.
}

```
$ /opt/fluent/bin/fluent-gem list | grep arrow
fluent-plugin-arrow-file (0.3)
```

@ja:##設定
@en:##Configuration

@ja{
前述の通り、arrow-fileプラグインを動作させるには、出力先のパス名とスキーマ定義を設定することが最低限必要です。

これに加えて、Apache Arrowファイルの構造上、Record Batchと呼ばれるデータの固まりはある程度大きなサイズで区切っておいた方が、検索・集計処理を行う際の処理性能を引き出しやすいです。
arrow-fileプラグインは、Bufferプラグインから渡されるchunkごとにRecord Batchを作成するため、Bufferプラグイン側のバッファサイズはこれに準じた設定を行うべきです。デフォルトでは 256MB のバッファサイズを取るように設定されています。
}

@en{
As mentioned above, the arrow-file plugin requires the output path of the file and the schema definition at least.

In addition to this, in order to acquire the best performance for searching and aggregation processing, the single chunk of data inside the Apache Arrow file, called the Record Batch, needs to be reasonably large in size.
The arrow-file plugin creates a Record Batch for each chunk passed from the Buffer plugin.
Therefore, the buffer size of the Buffer plugin should be set with the size of the Record Batch in mind. By the default, the Buffer plugin is set to take a buffer size of 256MB.
}

@ja{
arrow-fileプラグインの設定パラメータは以下の通りです。
}
@en{
The configuration parameters for the arrow-file plugin are as follows:
}
@ja{
`path` [type: `String` ] (必須パラメータ)
:    arrow-fileプラグインがログを出力するファイル名を指定します。
:    このパラメータは必須で、以下の書式文字を含める事ができます。

|書式|説明|
|----|----|
|`%Y`|現在の年を西暦4桁で表現した数値で置き換えます。|
|`%y`|現在の年の西暦下2桁で表現した数値で置き換えます。|
|`%m`|現在の月を 01～12 で表した2桁の数値で置き換えます。|
|`%d`|現在の日を 01～31 で表した2桁の数値で置き換えます。|
|`%H`|現在時刻の時を00～23で表した2桁の数値で置き換えます。|
|`%M`|現在時刻の分を00～59で表した2桁の数値で置き換えます。|
|`%S`|現在時刻の秒を00～59で表した2桁の数値で置き換えます。|
|`%p`|現在の Fluentd プロセスのPIDで置き換えます。|

書式文字列はチャンクを書き出すタイミングで評価され、同名のApache Arrow形式ファイルが存在する場合には、Record Batchを追記します。存在しない場合はApache Arrow形式ファイルを新規作成し、最初のRecord Batchを書き出します。

ただし、既存のApache Arrowファイルのサイズが後述の`filesize_threshold`設定値を越えている場合は、既存ファイルをリネームした後、新規にファイルを作成します。

    （例）`path /tmp/arrow_logs/my_logs_%y%m%d.%p.log`

出力先のApache Arrowファイルは、チャンクを書き出すたびにフッタ領域を更新して全てのRecord Batchをポイントします。したがって、生成されたApache Arrowファイルは即座に読み出すことができますが、アクセス競合を避けるためには`lockf(3)`を用いて排他処理を行う必要があります。
}
@en{
`path` [type: `String` ] (Required)
:    Specify the file path of the destination.
:    This is a required parameter, and you can use placeholders shown below.

|Placeholders|Description|
|----|----|
|`%Y`|The current year expressed as a four-digit number.|
|`%y`|The current year expressed as a number in the last two digits of the year.|
|`%m`|A two-digit number representing the current month, 01-12.|
|`%d`|A two-digit number representing the current day from 01 to 31.|
|`%H`|A two-digit number representing the hour of the current time 00-23.|
|`%M`|A two-digit number representing the minute of the current time, 00-59.|
|`%S`|A two-digit number representing the second of the current time, 00-59.|
|`%p`|The PID of the current Fluentd process.|

The placeholder is replaced when the chunk is written out. If an Apache Arrow format file of the same name exists, the Record Batch will be appended to it. If it does not exist, a new Apache Arrow format file is created and the first Record Batch is written out.

However, if the size of the existing Apache Arrow file exceeds the `filesize_threshold` setting described below, rename the existing file and then create a new one.

(Example) `path /tmp/arrow_logs/my_logs_%y%m%d.%p.log`

The output Apache Arrow file updates the footer area to point to all Record Batches each time a chunk is written out. Therefore, the generated Apache Arrow file can be read immediately. However, to avoid access conflicts, exclusive handling is required using `lockf(3)`.
}
@ja{
`schema_defs` [type: `String` ] (必須パラメータ)
:    `fluent-plugin-arrow-file`がログデータを出力する際の、Apache Arrow形式ファイルのスキーマ定義を指定します。
:    このパラメータは必須で、以下の形式で記述された文字列によりスキーマ構造を定義します。

- `schema_defs := column_def1[,column_def2 ...]`
- `column_def := <column_name>=<column_type>[;<column_attrs>]`
    - `<column_name>`は列の名前です。Fluentdからarrow-fileに渡される連想配列のキー値と一致している必要があります。
    - `<column_type>`は列のデータ型です。以下の表を参照してください。
    - `<column_attrs>`は列の付加属性です。現時点では以下の属性のみがサポートされています。
        - `stat_enabled` ... 列の統計情報を収集し、Record Batchごとの最大値/最小値を`max_values=...`および`min_values=...`カスタムメタデータとして埋め込みます。

（例）`schema_defs "ts=Timestamp;stat_enabled,dev_id=Uint32,temperature=Float32,humidity=Float32"`

***arrow-fileプラグインのサポートするデータ型***

|データ型|説明|
|--------|----|
|`Int8` `Int16` `Int32` `Int64`|符号付き整数型で、それぞれ指定したビット幅を持ちます。|
|`Uint8` `Uint16` `Uint32` `Uint64`|符号なし整数型で、それぞれ指定したビット幅を持ちます。|
|`Float16` `Float32` `Float64`|浮動小数点型で、それぞれ半精度(16bit)、単精度(32bit)、倍精度(64bit)の幅を持ちます。|
|`Decimal` `Decimal128`|128bit固定小数点型です。256bit固定小数点型は現在未サポートです。|
|`Timestamp` `Timestamp[sec]` `Timestamp[ms]` `Timestamp[us]` `Timestamp[ns]`|タイムスタンプ型です。精度を指定することができ、省略した場合は暗黙に`[us]`を付加したものとして扱われます。|
|`Time` `Time[sec]` `Time[ms]` `Time[us]` `Time[ns]`|時刻型です。精度を指定することができ、省略した場合は暗黙に`[sec]`を付加したものとして扱われます。|
|`Date` `Date[Day]` `Date[ms]`|日付型です。精度を指定することができ、省略した場合は暗黙に`[day]`を付加したものとして扱われます。
|`Utf8`|文字列型です。|
|`Ipaddr4`|IPv4アドレス型です。実際には`byteWidth=4`である`FixedSizeBinary`型に、`pg_type=pg_catalog.inet`というカスタムメタデータを付与します。|
|`Ipaddr6`|IPv6アドレス型です。実際には`byteWidth=16`である`FixedSizeBinary`型に、`pg_type=pg_catalog.inet`というカスタムメタデータを付与します。|
}
@en{
`schema_defs` [type: `String` ]  (Required)
:    Specify the schema definition of the Apache Arrow format file output by `fluent-plugin-arrow-file`.
:    This is a required parameter, and define the schema structure using strings in the following format.

- `schema_defs := column_def1[,column_def2 ...]`
- `column_def := <column_name>=<column_type>[;<column_attrs>]`
    - `<column_name>` is the name of the column, which must match the key value of the associative array passed from Fluentd to arrow-file plugin.
    - `<column_type>` is the data type of the column. See the following table.
    - `<column_attrs>` is an additional attribute for columns. At this time, only the following attributes are supported.
        - `stat_enabled` ... The statistics for the configured columns will be collected and the maximum/minimum values for each Record Batch will be set as custom metadata in the form of `max_values=...` and `min_values=...`.

(Example) `schema_defs "ts=Timestamp;stat_enabled,dev_id=Uint32,temperature=Float32,humidity=Float32"`

***Data types supported by the arrow-file plugin***

|Data types|Description|
|--------|----|
|`Int8` `Int16` `Int32` `Int64`|Signed integer with the specified bit width.|
|`Uint8` `Uint16` `Uint32` `Uint64`|Unsigned integer with the specified bit width|
|`Float16` `Float32` `Float64`|Floating point number with half-precision(16bit), single-precision(32bit) and double-precision(64bit).|
|`Decimal` `Decimal128`|128-bit fixed decimal; 256-bit fixed decimal is currently not supported.|
|`Timestamp` `Timestamp[sec]` `Timestamp[ms]` `Timestamp[us]` `Timestamp[ns]`|Timestamp type with the specified precision. If the precision is omitted, it is treated the same as `[us]`.|
|`Time` `Time[sec]` `Time[ms]` `Time[us]` `Time[ns]`|Time type with the specified precision. If the precision is omitted, it is treated the same as `[sec]`.|
|`Date` `Date[Day]` `Date[ms]`|Date type with the specified precision. If the precision is omitted, it is treated the same as `[day]`.|
|`Utf8`|String type.|
|`Ipaddr4`|IP address(IPv4) type. This is represented as a `FixedSizeBinary` type with `byteWidth=4` and custom metadata `pg_type=pg_catalog.inet`.|
|`Ipaddr6`|IP address(IPv6) type. This is represented as a `FixedSizeBinary` type with `byteWidth=16` and custom metadata `pg_type=pg_catalog.inet`.|

}
@ja{
`ts_column` [type: `String` / default: なし]
:    指定した列の値を（`record`連想配列からではなく）Fluentdから渡されたログのタイムスタンプ値より取得します。
:    通常、このオプションで指定する列は`Timestamp`などの日付時刻型を持っており、また`stat_enabled`属性と併用することで検索処理の高速化が期待できます。
}
@en{
`ts_column` [type: `String` / default: unspecified]
:    Specify a column name to set the timestamp value of the log passed from Fluentd (not from `record`).
:    This parameter is usually a date-time type such as `Timestamp`, and the `stat_enabled` attribute should also be specified to achieve fast search.
}
@ja{
`tag_column` [type: `String` / default: なし]
:    指定した列の値を（`record`連想配列からではなく）Fluentdから渡されたログのタグ値より取得します。
:    通常、このオプションで指定する列は`Utf8`などの文字列型を持っています。
}
@en{
`tag_column` [type: `String` / default: unspecified]
:    Specify a column name to set the tag value of the log passed from Fluentd (not from `record`).
:    This parameter is usually a string type such as `utf8`.
}
@ja{
`filesize_threshold` [type: `Integer` / default: 10000]
:    `fluent-plugin-arrow-file`が出力先ファイルを切り替える閾値をMB単位で設定します。
:    デフォルトではファイルサイズが約10GBを越えた辺りで出力先を切り替えます。
}
@en{
`filesize_threshold` [type: `Integer` / default: 10000]
:    Specify the threshold for switching the output destination file in MB.
:    By default, the output destination is switched when the file size exceeds about 10GB.
}

@ja:##使用例
@en:##Example

@ja{
簡単な例として、ローカルのApache Httpdサーバのログを監視し、それをフィールド毎にパースしてApache Arrow形式ファイルに書き込みます。

`<source>`で`/var/log/httpd/access_log`をデータソースとして指定しているほか、`apache2`のParseプラグインを用いて、host, user, time, method, path, code, size, referer, agentの各フィールドを切り出しています。

これらはarrow-fileプラグインに連想配列として渡され、`<match>`内の`schema_defs`には、これらのフィールドに対応するApache Arrowファイルの列定義を記述しています。

また、ここでは簡単な使用例を示すことが目的ですので、`<buffer>`タグでチャンクサイズを最大4MB / 200行に縮小し、最大でも10秒でOutputプラグインに渡すよう設定しています。
}

@en{
As a simple example, this chapter shows a configuration for monitoring the log of a local Apache Httpd server, parsing it field by field, and writing it to an Apache Arrow format file.

By setting `<source>`, `/var/log/httpd/access_log` will be the data source. Then, the `apache2` Parse plugin will cut out the fields: host, user, time, method, path, code, size, referer, agent.

The fields are then passed to the arrow-file plugin as an associative array. In `schema_defs` in `<match>`, the column definitions of the Apache Arrow file corresponding to the fields are set.
For simplicity of explanation, the chunk size is set to a maximum of 4MB / 200 lines in the `<buffer>` tag, and it is set to pass to the Output plugin in 10 seconds at most.
}

@ja: `/etc/fluent/fluentd.conf`の設定例
@en: ***Example configuration of `/etc/fluent/fluentd.conf`***
```
<source>
  @type  tail
  path /var/log/httpd/access_log
  pos_file /var/log/fluent/httpd_access.pos
  tag httpd
  format apache2
  <parse>
    @type  apache2
    expression /^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>(?:[^\"]|\\.)*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>(?:[^\"]|\\.)*)" "(?<agent>(?:[^\"]|\\.)*)")?$/
    time_format %d/%b/%Y:%H:%M:%S %z
  </parse>
</source>

<match httpd>
  @type  arrow_file
  path /tmp/mytest%Y%m%d.%p.arrow
  schema_defs "ts=Timestamp[sec],host=Utf8,method=Utf8,path=Utf8,code=Int32,size=Int32,referer=Utf8,agent=Utf8"
  ts_column "ts"
  <buffer>
    flush_interval 10s
    chunk_limit_size 4MB
    chunk_limit_records 200
  </buffer>
</match>
```

@ja{
`fluentd`を起動します。
}
@en{
Start the `fluentd` service.
}
```
$ sudo systemctl start fluentd
```

@ja{
以下のように、Apache Httpdのログが `path` で設定した `/tmp/mytest%Y%m%d.%p.arrow` が展開された先である `/tmp/mytest20220124.3206341.arrow` に書き出されています。
}
@en{
See the following output. The placeholder for `/tmp/mytest%Y%m%d.%p.arrow` set in `path` is replaced and the Apache Httpd log is written to `/tmp/mytest20220124.3206341.arrow`.
}

```
$ arrow2csv /tmp/mytest20220124.3206341.arrow --head --offset 300 --limit 10
"ts","host","method","path","code","size","referer","agent"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/js/theme_extra.js",200,195,"http://buri/docs/ja/fluentd/","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/js/theme.js",200,4401,"http://buri/docs/ja/fluentd/","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/img/fluentd_overview.png",200,121459,"http://buri/docs/ja/fluentd/","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/search/main.js",200,3027,"http://buri/docs/ja/fluentd/","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/fonts/Lato/lato-regular.woff2",200,182708,"http://buri/docs/ja/css/theme.css","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/fonts/fontawesome-webfont.woff2?v=4.7.0",200,77160,"http://buri/docs/ja/css/theme.css","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/fonts/RobotoSlab/roboto-slab-v7-bold.woff2",200,67312,"http://buri/docs/ja/css/theme.css","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:42","192.168.77.95","GET","/docs/ja/fonts/Lato/lato-bold.woff2",200,184912,"http://buri/docs/ja/css/theme.css","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:43","192.168.77.95","GET","/docs/ja/search/worker.js",200,3724,"http://buri/docs/ja/fluentd/","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
"2022-01-24 06:13:43","192.168.77.95","GET","/docs/ja/img/favicon.ico",200,1150,"http://buri/docs/ja/fluentd/","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"

```

@ja{
これを PG-Strom のArrow_Fdwを用いてPostgreSQLにマッピングしてみます。
}
@en{
Let's map the output file to PostgreSQL using PG-Strom's Arrow_Fdw.
}

```
postgres=# IMPORT FOREIGN SCHEMA mytest
           FROM SERVER arrow_fdw INTO public
           OPTIONS (file '/tmp/mytest20220124.3206341.arrow');
IMPORT FOREIGN SCHEMA

postgres=# SELECT ts, host, path FROM mytest WHERE code = 404;
         ts          |     host      |         path
---------------------+---------------+----------------------
 2022-01-24 12:02:06 | 192.168.77.73 | /~kaigai/ja/fluentd/
(1 row)

postgres=# EXPLAIN SELECT ts, host, path FROM mytest WHERE code = 404;
                                  QUERY PLAN
------------------------------------------------------------------------------
 Custom Scan (GpuScan) on mytest  (cost=4026.12..4026.12 rows=3 width=72)
   GPU Filter: (code = 404)
   referenced: ts, host, path, code
   files0: /tmp/mytest20220124.3206341.arrow (read: 128.00KB, size: 133.94KB)
(4 rows)
```

@ja{
生成された Apache Arrow ファイルを外部テーブルとしてマッピングし、これをSQLから参照しています。

Fluentd側で成形されたログの各フィールドを参照する検索条件を与える事ができます。
上記の例では、HTTPステータスコード404のログを検索し、1件がヒットしています。
}
@en{
The example above shows how the generated Apache Arrow file can be mapped as an external table and accessed with SQL.

Search conditions can be given to search each field of the log formed on the Fluentd side.
In the example above, the log with HTTP status code 404 is searched and one record is found.
}
