{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This chapter introduces the overview of PG-Strom, and developer's community. What is PG-Strom? PG-Strom is an extension module of PostgreSQL designed for version 15 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set. Its core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications. PG-Strom has two storage options. The first one is the heap storage system of PostgreSQL. It is not always optimal for aggregation / analysis workloads because of its row data format, on the other hands, it has an advantage to run aggregation workloads without data transfer from the transactional database. The other one is Apache Arrow files, that have structured columnar format. Even though it is not suitable for update per row basis, it enables to import large amount of data efficiently, and efficiently search / aggregate the data through foreign data wrapper (FDW). One of the characteristic feature of PG-Strom is GPUDirect SQL that bypasses the CPU/RAM to read the data from NVME / NVME-oF to the GPU directly. SQL processing on the GPU maximizes the bandwidth of these devices. PG-Strom v3.0 newly supports NVIDIA GPUDirect Storage, it allows to support SDS (Software Defined Storage) over the NVME-oF protocol and shared filesystems. The v3.0 newly supports execution of some PostGIS function and GiST index search on the GPU side. Along with the GPU cache, that duplicates the table contents often updated very frequently, it enables search / analysis processing based on the real-time locational information. The v5.0 entirely revised the software architecture design, like update of the process model (multi-process to multi-thread), or switch from the CUDA C++ native code to the portable pseudo code. It improves overall performance and stability. License and Copyright PG-Strom is an open source software distributed under the PostgreSQL License. See LICENSE for the license details. Community Please post your questions, requests and trouble reports to the Discussion of GitHub\u306e . Please pay attention it is a public board for world wide. So, it is your own responsibility not to disclose confidential information. The primary language of the discussion board is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach (JP) prefix on the subject like, for non-Japanese speakers to skip messages. Bug or troubles report If you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue at the PG-Strom Issue Tracker . Please ensure the items below on bug reports. Whether you can reproduce the same problem on the latest revision? Hopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software. Whether you can reproduce the same problem if PG-Strom is disabled? GUC option pg_strom.enabled can turn on/off PG-Strom. Is there any known issues on the issue tracker of GitHub? Please don't forget to search closed issues The information below are helpful for bug-reports. Steps to reproduce the problem (Data and Query) Output of EXPLAIN VERBOSE for the queries in trouble. Data structure of the tables involved with \\d+ <table name> on psql command. Log messages (verbose messages are more helpful) Status of GUC options you modified from the default configurations. Hardware configuration - GPU model and host RAM size especially. If you are not certain whether the strange behavior on your site is bug or not, please report it to the discussion board prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information. New features proposition If you have any ideas of new features, please open a new issue with feature tag at the PG-Strom Issue Tracker , then have a discussion with other developers. A preferable design proposal will contain the items below. What is your problem to solve / improve? How much serious is it on your workloads / user case? Way to implement your idea? Expected downside, if any. Once we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed. Once a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again. On the development stage, please attach patch file on the issue ticket. We don't use pull request. Support Policy The PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release. Please note that it is volunteer based community support policy, so our support is best effort and no SLA definition. If you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).","title":"Home"},{"location":"#home","text":"This chapter introduces the overview of PG-Strom, and developer's community.","title":"Home"},{"location":"#what-is-pg-strom","text":"PG-Strom is an extension module of PostgreSQL designed for version 15 or later. By utilization of GPU (Graphic Processor Unit) device which has thousands cores per chip, it enables to accelerate SQL workloads for data analytics or batch processing to big data set. Its core features are GPU code generator that automatically generates GPU program according to the SQL commands and asynchronous parallel execution engine to run SQL workloads on GPU device. The latest version supports SCAN (evaluation of WHERE-clause), JOIN and GROUP BY workloads. In the case when GPU-processing has advantage, PG-Strom replaces the vanilla implementation of PostgreSQL and transparentlly works from users and applications. PG-Strom has two storage options. The first one is the heap storage system of PostgreSQL. It is not always optimal for aggregation / analysis workloads because of its row data format, on the other hands, it has an advantage to run aggregation workloads without data transfer from the transactional database. The other one is Apache Arrow files, that have structured columnar format. Even though it is not suitable for update per row basis, it enables to import large amount of data efficiently, and efficiently search / aggregate the data through foreign data wrapper (FDW). One of the characteristic feature of PG-Strom is GPUDirect SQL that bypasses the CPU/RAM to read the data from NVME / NVME-oF to the GPU directly. SQL processing on the GPU maximizes the bandwidth of these devices. PG-Strom v3.0 newly supports NVIDIA GPUDirect Storage, it allows to support SDS (Software Defined Storage) over the NVME-oF protocol and shared filesystems. The v3.0 newly supports execution of some PostGIS function and GiST index search on the GPU side. Along with the GPU cache, that duplicates the table contents often updated very frequently, it enables search / analysis processing based on the real-time locational information. The v5.0 entirely revised the software architecture design, like update of the process model (multi-process to multi-thread), or switch from the CUDA C++ native code to the portable pseudo code. It improves overall performance and stability.","title":"What is PG-Strom?"},{"location":"#license-and-copyright","text":"PG-Strom is an open source software distributed under the PostgreSQL License. See LICENSE for the license details.","title":"License and Copyright"},{"location":"#community","text":"Please post your questions, requests and trouble reports to the Discussion of GitHub\u306e . Please pay attention it is a public board for world wide. So, it is your own responsibility not to disclose confidential information. The primary language of the discussion board is English. On the other hands, we know major portion of PG-Strom users are Japanese because of its development history, so we admit to have a discussion on the list in Japanese language. In this case, please don't forget to attach (JP) prefix on the subject like, for non-Japanese speakers to skip messages.","title":"Community"},{"location":"#bug-or-troubles-report","text":"If you got troubles like incorrect results, system crash / lockup, or something strange behavior, please open a new issue at the PG-Strom Issue Tracker . Please ensure the items below on bug reports. Whether you can reproduce the same problem on the latest revision? Hopefully, we recommend to test on the latest OS, CUDA, PostgreSQL and related software. Whether you can reproduce the same problem if PG-Strom is disabled? GUC option pg_strom.enabled can turn on/off PG-Strom. Is there any known issues on the issue tracker of GitHub? Please don't forget to search closed issues The information below are helpful for bug-reports. Steps to reproduce the problem (Data and Query) Output of EXPLAIN VERBOSE for the queries in trouble. Data structure of the tables involved with \\d+ <table name> on psql command. Log messages (verbose messages are more helpful) Status of GUC options you modified from the default configurations. Hardware configuration - GPU model and host RAM size especially. If you are not certain whether the strange behavior on your site is bug or not, please report it to the discussion board prior to the open a new issue ticket. Developers may be able to suggest you next action - like a request for extra information.","title":"Bug or troubles report"},{"location":"#new-features-proposition","text":"If you have any ideas of new features, please open a new issue with feature tag at the PG-Strom Issue Tracker , then have a discussion with other developers. A preferable design proposal will contain the items below. What is your problem to solve / improve? How much serious is it on your workloads / user case? Way to implement your idea? Expected downside, if any. Once we could make a consensus about its necessity, coordinator will attach accepted tag and the issue ticket is used to track rest of the development. Elsewhere, the issue ticket got rejected tag and closed. Once a proposal got rejected, we may have different decision in the future. If comprehensive circumstance would be changed, you don't need to hesitate revised proposition again. On the development stage, please attach patch file on the issue ticket. We don't use pull request.","title":"New features proposition"},{"location":"#support-policy","text":"The PG-Strom development team will support the latest release which are distributed from the HeteroDB Software Distribution Center only. So, people who met troubles needs to ensure the problems can be reproduced with the latest release. Please note that it is volunteer based community support policy, so our support is best effort and no SLA definition. If you need commercial support, contact to HeteroDB,Inc (contact@heterodbcom).","title":"Support Policy"},{"location":"arrow_fdw/","text":"Apache Arrow (Columnar Store) Overview PostgreSQL tables internally consist of 8KB blocks 1 , and block contains tuples which is a data structure of all the attributes and metadata per row. It collocates date of a row closely, so it works effectively for INSERT/UPDATE-major workloads, but not suitable for summarizing or analytics of mass-data. It is not usual to reference all the columns in a table on mass-data processing, and we tend to reference a part of columns in most cases. In this case, the storage I/O bandwidth consumed by unreferenced columns are waste, however, we have no easy way to fetch only particular columns referenced from the row-oriented data structure. In case of column oriented data structure, in an opposite manner, it has extreme disadvantage on INSERT/UPDATE-major workloads, however, it can pull out maximum performance of storage I/O on mass-data processing workloads because it can loads only referenced columns. From the standpoint of processor efficiency also, column-oriented data structure looks like a flat array that pulls out maximum bandwidth of memory subsystem for GPU, by special memory access pattern called Coalesced Memory Access. What is Apache Arrow? Apache Arrow is a data format of structured data to save in columnar-form and to exchange other applications. Some applications for big-data processing support the format, and it is easy for self-developed applications to use Apache Arrow format since they provides libraries for major programming languages like C,C++ or Python. Apache Arrow format file internally contains Schema portion to define data structure, and one or more RecordBatch to save columnar-data based on the schema definition. For data types, it supports integers, strint (variable-length), date/time types and so on. Indivisual columnar data has its internal representation according to the data types. Data representation in Apache Arrow is not identical with the representation in PostgreSQL. For example, epoch of timestamp in Arrow is 1970-01-01 and it supports multiple precision. In contrast, epoch of timestamp in PostgreSQL is 2001-01-01 and it has microseconds accuracy. Arrow_Fdw allows to read Apache Arrow files on PostgreSQL using foreign table mechanism. If an Arrow file contains 8 of record batches that has million items for each column data, for example, we can access 8 million rows on the Arrow files through the foreign table. Operations Creation of foreign tables Usually it takes the 3 steps below to create a foreign table. Define a foreign-data-wrapper using CREATE FOREIGN DATA WRAPPER command Define a foreign server using CREATE SERVER command Define a foreign table using CREATE FOREIGN TABLE command The first 2 steps above are included in the CREATE EXTENSION pg_strom command. All you need to run individually is CREATE FOREIGN TABLE command last. CREATE FOREIGN TABLE flogdata ( ts timestamp, sensor_id int, signal1 smallint, signal2 smallint, signal3 smallint, signal4 smallint, ) SERVER arrow_fdw OPTIONS (file '/path/to/logdata.arrow'); Data type of columns specified by the CREATE FOREIGN TABLE command must be matched to schema definition of the Arrow files to be mapped. Arrow_Fdw also supports a useful manner using IMPORT FOREIGN SCHEMA statement. It automatically generates a foreign table definition using schema definition of the Arrow files. It specifies the foreign table name, schema name to import, and path name of the Arrow files using OPTION-clause. Schema definition of Arrow files contains data types and optional column name for each column. It declares a new foreign table using these information. IMPORT FOREIGN SCHEMA flogdata FROM SERVER arrow_fdw INTO public OPTIONS (file '/path/to/logdata.arrow'); Foreign table options Arrow_Fdw supports the options below. Foreign Table Options file=PATHNAME It maps an Arrow file specified on the foreign table. files=PATHNAME1[,PATHNAME2...] It maps multiple Arrow files specified by comma (,) separated files list on the foreign table. dir=DIRNAME It maps all the Arrow files in the directory specified on the foreign table. suffix=SUFFIX When dir option is given, it maps only files with the specified suffix, like .arrow` for example. parallel_workers=N_WORKERS It tells the number of workers that should be used to assist a parallel scan of this foreign table; equivalent to parallel_workers storage parameter at normal tables. pattern=PATTERN Maps only files specified by the file , files , or dir option that match the PATTERN , including wildcards, to the foreign table. The following wildcards can be used: ? ... matches any 1 character. * ... matches any string of 0 or more characters. ${KEY} ... matches any string of 0 or more characters. @{KEY} ... matches any numeric string of 0 or more characters. An interesting use of this option is to refer to a portion of a file name matched by the wildcard ${KEY} or @{KEY} as a virtual column. For more information, see the '''Arrow_Fdw virtual column''' section below. Foreign Column Options field=FIELD It specifies the field name of the Arrow file to map to that column. In the default, Arrow_Fdw maps the first occurrence of a field that has the same column name as this foreign table's column name. virtual=KEY It configures the column is a virtual column. KEY specifies the wildcard key name in the pattern specified by the pattern option of the foreign table option. A virtual column allows to refer to the part of the file name pattern that matches KEY in a query. virtual_metadata=KEY It specifies that the column is a virtual column. KEY specifies a KEY-VALUE pair embedded in the CustomMetadata field of the Arrow file. If the specified KEY-VALUE pair is not found, the column returns a NULL value. There are two types of CustomMetadata in Arrow files: embedded in the schema (corresponding to a PostgreSQL table) and embedded in the field (corresponding to a PostgreSQL column). For example, you can reference CustomMetadata embedded in a field by writing the field name separated by the . character before the KEY value, such as lo_orderdate.max_values . If there is no field name, it will be treated as a KEY-VALUE pair embedded in the schema. virtual_metadata_split=KEY It specifies that the column is a virtual column. KEY specifies the KEY-VALUE pair embedded in the CustomMetadata field of the Arrow file. If the specified KEY-VALUE pair is not found, this column returns a NULL value. The difference from virtual_metadata is that the values of the CustomMetadata field are separated by a delimiter( , ) and applied to each Record Batch in order from the beginning. For example, if the specified CustomMetadata value is Tokyo,Osaka,Kyoto,Yokohama , the row read from RecordBatch-0 will display 'Tokyo' , the row read from RecordBatch-1 will display 'Osaka' , and the row read from RecordBatch-2 will display 'Osaka' as the value of this virtual column. Data type mapping Arrow data types are mapped on PostgreSQL data types as follows. Int mapped to either of int1 , int2 , int4 or int8 according to the bitWidth attribute. is_signed attribute shall be ignored. int1 is an enhanced data type by PG-Strom. FloatingPoint mapped to either of float2 , float4 or float8 according to the precision attribute. float2 is an enhanced data type by PG-Strom. Utf8 , LargeUtf8 mapped to text data type Binary , LargeBinary mapped to bytea data type Decimal mapped to numeric data type Date mapped to date data type; to be adjusted as if it has unit=Day precision. Time mapped to time data type; to be adjusted as if it has unit=MicroSecond precision. Timestamp mapped to timestamp data type; to be adjusted as if it has unit=MicroSecond precision. Interval mapped to interval data type. List , LargeList mapped to 1-dimensional array of the element data type. Struct mapped to compatible composite data type; that shall be defined preliminary. FixedSizeBinary mapped to char(n) data type according to the byteWidth attribute. If pg_type=TYPENAME is configured, PG-Strom may assign the configured data type. Right now, inet and macaddr are supported. Union , Map , Duration Right now, PG-Strom cannot map these Arrow data types onto any of PostgreSQL data types. How to read EXPLAIN EXPLAIN command show us information about Arrow files reading. The example below is an output of query execution plan that includes f_lineorder foreign table that mapps an Arrow file of 503GB. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM f_lineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=14535261.08..14535261.09 rows=1 width=8) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=14535261.06..14535261.07 rows=1 width=32) GPU Projection: pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint) GPU Scan Quals: ((f_lineorder.lo_discount >= 1) AND (f_lineorder.lo_discount <= 3) AND (f_lineorder.lo_quantity < 25)) [rows: 5999990000 -> 9999983] GPU Join Quals [1]: (f_lineorder.lo_orderdate = date1.d_datekey) ... [nrows: 9999983 -> 1428010] GPU Outer Hash [1]: f_lineorder.lo_orderdate GPU Inner Hash [1]: date1.d_datekey referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount file0: /opt/nvme/f_lineorder_s999.arrow (read: 89.41GB, size: 502.92GB) GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 (cost=0.00..78.95 rows=365 width=4) Filter: (d_year = 1993) (12 rows) According to the EXPLAIN output, we can see Custom Scan (GpuPreAgg) scans f_lineorder foreign table. file0 item shows the filename ( /opt/nvme/lineorder_s999.arrow ) on behalf of the foreign table and its size. If multiple files are mapped, any files are individually shown, like file1 , file2 , ... The referenced item shows the list of referenced columns. We can see this query touches lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns. In addition, GPU-Direct SQL: enabled (GPU-0) shows us the scan on f_lineorder uses GPU-Direct SQL mechanism. VERBOSE option outputs more detailed information. =# EXPLAIN VERBOSE SELECT sum(lo_extendedprice*lo_discount) as revenue FROM f_lineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=14535261.08..14535261.09 rows=1 width=8) Output: pgstrom.sum_int((pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint))) -> Custom Scan (GpuPreAgg) on public.f_lineorder (cost=14535261.06..14535261.07 rows=1 width=32) Output: (pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint)) GPU Projection: pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint) GPU Scan Quals: ((f_lineorder.lo_discount >= 1) AND (f_lineorder.lo_discount <= 3) AND (f_lineorder.lo_quantity < 25)) [rows: 5999990000 -> 9999983] GPU Join Quals [1]: (f_lineorder.lo_orderdate = date1.d_datekey) ... [nrows: 9999983 -> 1428010] GPU Outer Hash [1]: f_lineorder.lo_orderdate GPU Inner Hash [1]: date1.d_datekey referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount file0: /opt/nvme/f_lineorder_s999.arrow (read: 89.41GB, size: 502.92GB) lo_orderdate: 22.35GB lo_quantity: 22.35GB lo_extendedprice: 22.35GB lo_discount: 22.35GB GPU-Direct SQL: enabled (GPU-0) KVars-Slot: <slot=0, type='int4', expr='f_lineorder.lo_discount'>, <slot=1, type='int4', expr='f_lineorder.lo_quantity'>, <slot=2, type='int8', expr='(f_lineorder.lo_extendedprice * f_lineorder.lo_discount)'>, <slot=3, type='int4', expr='f_lineorder.lo_extendedprice'>, <slot=4, type='int4', expr='f_lineorder.lo_orderdate'>, <slot=5, type='int4', expr='date1.d_datekey'> KVecs-Buffer: nbytes: 51200, ndims: 3, items=[kvec0=<0x0000-27ff, type='int4', expr='lo_discount'>, kvec1=<0x2800-4fff, type='int4', expr='lo_quantity'>, kvec2=<0x5000-77ff, type='int4', expr='lo_extendedprice'>, kvec3=<0x7800-9fff, type='int4', expr='lo_orderdate'>, kvec4=<0xa000-c7ff, type='int4', expr='d_datekey'>] LoadVars OpCode: {Packed items[0]={LoadVars(depth=0): kvars=[<slot=4, type='int4' resno=6(lo_orderdate)>, <slot=1, type='int4' resno=9(lo_quantity)>, <slot=3, type='int4' resno=10(lo_extendedprice)>, <slot=0, type='int4' resno=12(lo_discount)>]}, items[1]={LoadVars(depth=1): kvars=[<slot=5, type='int4' resno=1(d_datekey)>]}} MoveVars OpCode: {Packed items[0]={MoveVars(depth=0): items=[<slot=0, offset=0x0000-27ff, type='int4', expr='lo_discount'>, <slot=3, offset=0x5000-77ff, type='int4', expr='lo_extendedprice'>, <slot=4, offset=0x7800-9fff, type='int4', expr='lo_orderdate'>]}}, items[1]={MoveVars(depth=1): items=[<offset=0x0000-27ff, type='int4', expr='lo_discount'>, <offset=0x5000-77ff, type='int4', expr='lo_extendedprice'>]}}} Scan Quals OpCode: {Bool::AND args=[{Func(bool)::int4ge args=[{Var(int4): slot=0, expr='lo_discount'}, {Const(int4): value='1'}]}, {Func(bool)::int4le args=[{Var(int4): slot=0, expr='lo_discount'}, {Const(int4): value='3'}]}, {Func(bool)::int4lt args=[{Var(int4): slot=1, expr='lo_quantity'}, {Const(int4): value='25'}]}]} Join Quals OpCode: {Packed items[1]={JoinQuals: {Func(bool)::int4eq args=[{Var(int4): kvec=0x7800-a000, expr='lo_orderdate'}, {Var(int4): slot=5, expr='d_datekey'}]}}} Join HashValue OpCode: {Packed items[1]={HashValue arg={Var(int4): kvec=0x7800-a000, expr='lo_orderdate'}}} Partial Aggregation OpCode: {AggFuncs <psum::int[slot=2, expr='(lo_extendedprice * lo_discount)']> arg={SaveExpr: <slot=2, type='int8'> arg={Func(int8)::int8 arg={Func(int4)::int4mul args=[{Var(int4): kvec=0x5000-7800, expr='lo_extendedprice'}, {Var(int4): kvec=0x0000-2800, expr='lo_discount'}]}}}} Partial Function BufSz: 16 -> Seq Scan on public.date1 (cost=0.00..78.95 rows=365 width=4) Output: date1.d_datekey Filter: (date1.d_year = 1993) (28 rows) The verbose output additionally displays amount of column-data to be loaded on reference of columns. The load of lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns needs to read 89.41GB in total. It is 17.8% towards the filesize (502.93GB). Arrow_Fdw Virtual Column Arrow_Fdw allows to map multiple Apache Arrow files with compatible schema structures to a single foreign table. For example, if `dir '/opt/arrow/mydata' is configured for foreign table option, all files under that directory will be mapped. When you are converting the contents of a transactional database into an Apache Arrow file, we often dump them to separate files by year and month or specific categories, and its file names reflects these properties. The example below shows an example to convert the transactional table lineorder into Arrow files by year of lo_orderdate and by category of lo_shipmode . $ for s in RAIL AIR TRUCK SHIP FOB MAIL; do for y in 1993 1994 1995 1996 1997; do pg2arrow -d ssbm -c \"SELECT * FROM lineorder_small \\ WHERE lo_orderdate between ${y}0101 and ${y}1231 \\ AND lo_shipmode = '${s}'\" \\ -o /opt/arrow/mydata/f_lineorder_${y}_${s}.arrow done done $ ls /opt/arrow/mydata/ f_lineorder_1993_AIR.arrow f_lineorder_1995_RAIL.arrow f_lineorder_1993_FOB.arrow f_lineorder_1995_SHIP.arrow f_lineorder_1993_MAIL.arrow f_lineorder_1995_TRUCK.arrow f_lineorder_1993_RAIL.arrow f_lineorder_1996_AIR.arrow f_lineorder_1993_SHIP.arrow f_lineorder_1996_FOB.arrow f_lineorder_1993_TRUCK.arrow f_lineorder_1996_MAIL.arrow f_lineorder_1994_AIR.arrow f_lineorder_1996_RAIL.arrow f_lineorder_1994_FOB.arrow f_lineorder_1996_SHIP.arrow f_lineorder_1994_MAIL.arrow f_lineorder_1996_TRUCK.arrow f_lineorder_1994_RAIL.arrow f_lineorder_1997_AIR.arrow f_lineorder_1994_SHIP.arrow f_lineorder_1997_FOB.arrow f_lineorder_1994_TRUCK.arrow f_lineorder_1997_MAIL.arrow f_lineorder_1995_AIR.arrow f_lineorder_1997_RAIL.arrow f_lineorder_1995_FOB.arrow f_lineorder_1997_SHIP.arrow f_lineorder_1995_MAIL.arrow f_lineorder_1997_TRUCK.arrow All these Apache Arrow files have the same schema structure and can be mapped to a single foreign table using the dir option. Also, the Arrow file that has '1995' token in the file name only contains records with lo_orderdate in the range 19950101 to 19951231. The Arrow file that has 'RAIL' token in the file name only contains records with lo_shipmode of RAIL . In other words, even if you define the Arrow_Fdw foreign table that maps these multiple Arrow files, when reading data from a file whose file name includes 1995, it is assumed that the value of lo_orderdate is in the range of 19950101 to 19951231. It is possible for the optimizer to utilize this knowledge. In Arrow_Fdw, you can refer to part of the file name as a column by using the foreign table option pattern . This is called a virtual column and is configured as follows. =# IMPORT FOREIGN SCHEMA f_lineorder FROM SERVER arrow_fdw INTO public OPTIONS (dir '/opt/arrow/mydata', pattern 'f_lineorder_@{year}_${shipping}.arrow'); IMPORT FOREIGN SCHEMA =# \\d f_lineorder Foreign table \"public.f_lineorder\" Column | Type | Collation | Nullable | Default | FDW options --------------------+---------------+-----------+----------+---------+---------------------- lo_orderkey | numeric | | | | lo_linenumber | integer | | | | lo_custkey | numeric | | | | lo_partkey | integer | | | | lo_suppkey | numeric | | | | lo_orderdate | integer | | | | lo_orderpriority | character(15) | | | | lo_shippriority | character(1) | | | | lo_quantity | numeric | | | | lo_extendedprice | numeric | | | | lo_ordertotalprice | numeric | | | | lo_discount | numeric | | | | lo_revenue | numeric | | | | lo_supplycost | numeric | | | | lo_tax | numeric | | | | lo_commit_date | character(8) | | | | lo_shipmode | character(10) | | | | year | bigint | | | | (virtual 'year') shipping | text | | | | (virtual 'shipping') Server: arrow_fdw FDW options: (dir '/opt/arrow/mydata', pattern 'f_lineorder_@{year}_${shipping}.arrow') This foreign table option pattern contains two wildcards. @{year} matches a numeric string larger than or equal to 0 characters, and ${shipping} matches a string larger than or equal to 0 characters. The patterns that match this part of the file name can be referenced in the part specified by the virtual column option. In this case, IMPORT FOREIGN SCHEMA automatically adds column definitions, in addition to the fields contained in the Arrow file itself, as well as the virtual column year (a bigint column) that references the wildcard @{year} , and the virtual column shipping that references the wildcard ${shipping} . =# SELECT lo_orderkey, lo_orderdate, lo_shipmode, year, shipping FROM f_lineorder WHERE year = 1995 AND shipping = 'AIR' LIMIT 10; lo_orderkey | lo_orderdate | lo_shipmode | year | shipping -------------+--------------+-------------+------+---------- 637892 | 19950512 | AIR | 1995 | AIR 638243 | 19950930 | AIR | 1995 | AIR 638273 | 19951214 | AIR | 1995 | AIR 637443 | 19950805 | AIR | 1995 | AIR 637444 | 19950803 | AIR | 1995 | AIR 637510 | 19950831 | AIR | 1995 | AIR 637504 | 19950726 | AIR | 1995 | AIR 637863 | 19950802 | AIR | 1995 | AIR 637892 | 19950512 | AIR | 1995 | AIR 637987 | 19950211 | AIR | 1995 | AIR (10 rows) In other words, you can know what values the virtual columns have before reading the Arrow file mapped by the Arrow_Fdw foreign table. By this feature, if it is obvious that there is no match at all from the search conditions before reading a certain Arrow file, it is possible to skip reading the file itself. See the query and its EXPLAIN ANALYZE output below. This aggregation query reads the f_lineorder foreign table, filters it by some conditions, and then aggregates the total value of lo_extendedprice * lo_discount . At that time, the conditional clause WHERE year = 1994 is added. This is effectively the same as WHERE lo_orderdate BETWEEN 19940101 AND 19942131 , but since year is a virtual column, you can determine whether a matching row exists before reading the Arrow files. In fact, looking at the Stats-Hint: line, 12 Record-Batches were loaded due to the condition (year = 1994) , but 48 Record-Batches were skipped. This is a simple but extremely effective means of reducing I/O load. =# EXPLAIN ANALYZE SELECT sum(lo_extendedprice*lo_discount) as revenue FROM f_lineorder WHERE year = 1994 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------------------------------------- Aggregate (cost=421987.07..421987.08 rows=1 width=32) (actual time=82.914..82.915 rows=1 loops=1) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=421987.05..421987.06 rows=1 width=32) \\ (actual time=82.901..82.903 rows=2 loops=1) GPU Projection: pgstrom.psum(((lo_extendedprice * lo_discount))::double precision) GPU Scan Quals: ((year = 1994) AND (lo_discount <= '3'::numeric) AND \\ (lo_quantity < '25'::numeric) AND \\ (lo_discount >= '1'::numeric)) [plan: 65062080 -> 542, exec: 13001908 -> 1701726] referenced: lo_quantity, lo_extendedprice, lo_discount, year Stats-Hint: (year = 1994) [loaded: 12, skipped: 48] file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 99.53MB, size: 427.16MB) file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 99.52MB, size: 427.13MB) file2: /opt/arrow/mydata/f_lineorder_1994_FOB.arrow (read: 99.18MB, size: 425.67MB) : : : : file27: /opt/arrow/mydata/f_lineorder_1997_MAIL.arrow (read: 99.23MB, size: 425.87MB) file28: /opt/arrow/mydata/f_lineorder_1995_MAIL.arrow (read: 99.16MB, size: 425.58MB) file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 99.24MB, size: 425.91MB) GPU-Direct SQL: enabled (N=2,GPU0,1; direct=76195, ntuples=13001908) Planning Time: 2.402 ms Execution Time: 83.857 ms (39 rows) How to make Arrow files This section introduces the way to transform dataset already stored in PostgreSQL database system into Apache Arrow file. Using PyArrow+Pandas A pair of PyArrow module, developed by Arrow developers community, and Pandas data frame can dump PostgreSQL database into an Arrow file. The example below reads all the data in table t0 , then write out them into /tmp/t0.arrow . import pyarrow as pa import pandas as pd X = pd.read_sql(sql=\"SELECT * FROM t0\", con=\"postgresql://localhost/postgres\") Y = pa.Table.from_pandas(X) f = pa.RecordBatchFileWriter('/tmp/t0.arrow', Y.schema) f.write_table(Y,1000000) # RecordBatch for each million rows f.close() Please note that the above operation once keeps query result of the SQL on memory, so should pay attention on memory consumption if you want to transfer massive rows at once. Using Pg2Arrow On the other hand, pg2arrow command, developed by PG-Strom Development Team, enables us to write out query result into Arrow file. This tool is designed to write out massive amount of data into storage device like NVME-SSD. It fetch query results from PostgreSQL database system, and write out Record Batches of Arrow format for each data size specified by the -s|--segment-size option. Thus, its memory consumption is relatively reasonable. pg2arrow command is distributed with PG-Strom. It shall be installed on the bin directory of PostgreSQL related utilities. $ pg2arrow --help Usage: pg2arrow [OPTION] [database] [username] General options: -d, --dbname=DBNAME Database name to connect to -c, --command=COMMAND SQL command to run -t, --table=TABLENAME Equivalent to '-c SELECT * FROM TABLENAME' (-c and -t are exclusive, either of them must be given) --inner-join=SUB_COMMAND --outer-join=SUB_COMMAND -o, --output=FILENAME result file in Apache Arrow format --append=FILENAME result Apache Arrow file to be appended (--output and --append are exclusive. If neither of them are given, it creates a temporary file.) -S, --stat[=COLUMNS] embeds min/max statistics for each record batch COLUMNS is a comma-separated list of the target columns if partially enabled. Arrow format options: -s, --segment-size=SIZE size of record batch for each Connection options: -h, --host=HOSTNAME database server host -p, --port=PORT database server port -u, --user=USERNAME database user name -w, --no-password never prompt for password -W, --password force password prompt Other options: --dump=FILENAME dump information of arrow file --progress shows progress of the job --set=NAME:VALUE config option to set before SQL execution --help shows this message Report bugs to <pgstrom@heterodbcom>. The -h or -U option specifies the connection parameters of PostgreSQL, like psql or pg_dump . The simplest usage of this command is running a SQL command specified by -c|--command option on PostgreSQL server, then write out results into the file specified by -o|--output option in Arrow format. --append option is available, instead of -o|--output option. It means appending data to existing Apache Arrow file. In this case, the target Apache Arrow file must have fully identical schema definition towards the specified SQL command. The example below reads all the data in table t0 , then write out them into the file /tmp/t0.arrow . $ pg2arrow -U kaigai -d postgres -c \"SELECT * FROM t0\" -o /tmp/t0.arrow Although it is an option for developers, --dump <filename> prints schema definition and record-batch location and size of Arrow file in human readable form. --progress option enables to show progress of the task. It is useful when a huge table is transformed to Apache Arrow format. Advanced Usage SSDtoGPU Direct SQL In case when all the Arrow files mapped on the Arrow_Fdw foreign table satisfies the terms below, PG-Strom enables SSD-to-GPU Direct SQL to load columnar data. Arrow files are on NVME-SSD volume. NVME-SSD volume is managed by Ext4 filesystem. Total size of Arrow files exceeds the pg_strom.nvme_strom_threshold configuration. Partition configuration Arrow_Fdw foreign tables can be used as a part of partition leafs. Usual PostgreSQL tables can be mixtured with Arrow_Fdw foreign tables. So, pay attention Arrow_Fdw foreign table does not support any writer operations. And, make boundary condition of the partition consistent to the contents of the mapped Arrow file. It is a responsibility of the database administrators. A typical usage scenario is processing of long-standing accumulated log-data. Unlike transactional data, log-data is mostly write-once and will never be updated / deleted. Thus, by migration of the log-data after a lapse of certain period into Arrow_Fdw foreign table that is read-only but rapid processing, we can accelerate summarizing and analytics workloads. In addition, log-data likely have timestamp, so it is quite easy design to add partition leafs periodically, like monthly, weekly or others. The example below defines a partitioned table that mixes a normal PostgreSQL table and Arrow_Fdw foreign tables. The normal PostgreSQL table, is read-writable, is specified as default partition, so DBA can migrate only past log-data into Arrow_Fdw foreign table under the database system operations. CREATE TABLE lineorder ( lo_orderkey numeric, lo_linenumber integer, lo_custkey numeric, lo_partkey integer, lo_suppkey numeric, lo_orderdate integer, lo_orderpriority character(15), lo_shippriority character(1), lo_quantity numeric, lo_extendedprice numeric, lo_ordertotalprice numeric, lo_discount numeric, lo_revenue numeric, lo_supplycost numeric, lo_tax numeric, lo_commit_date character(8), lo_shipmode character(10) ) PARTITION BY RANGE (lo_orderdate); CREATE TABLE lineorder__now PARTITION OF lineorder default; CREATE FOREIGN TABLE lineorder__1993 PARTITION OF lineorder FOR VALUES FROM (19930101) TO (19940101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1993.arrow'); CREATE FOREIGN TABLE lineorder__1994 PARTITION OF lineorder FOR VALUES FROM (19940101) TO (19950101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1994.arrow'); CREATE FOREIGN TABLE lineorder__1995 PARTITION OF lineorder FOR VALUES FROM (19950101) TO (19960101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1995.arrow'); CREATE FOREIGN TABLE lineorder__1996 PARTITION OF lineorder FOR VALUES FROM (19960101) TO (19970101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1996.arrow'); Below is the query execution plan towards the table. By the query condition lo_orderdate between 19950701 and 19960630 that touches boundary condition of the partition, the partition leaf lineorder__1993 and lineorder__1994 are pruned, so it makes a query execution plan to read other (foreign) tables only. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM lineorder,date1 WHERE lo_orderdate = d_datekey AND lo_orderdate between 19950701 and 19960630 AND lo_discount between 1 and 3 ABD lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=172088.90..172088.91 rows=1 width=32) -> Hash Join (cost=10548.86..172088.51 rows=77 width=64) Hash Cond: (lineorder__1995.lo_orderdate = date1.d_datekey) -> Append (cost=10444.35..171983.80 rows=77 width=67) -> Custom Scan (GpuScan) on lineorder__1995 (cost=10444.35..33671.87 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1995.arrow (size: 892.57MB) -> Custom Scan (GpuScan) on lineorder__1996 (cost=10444.62..33849.21 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1996.arrow (size: 897.87MB) -> Custom Scan (GpuScan) on lineorder__now (cost=11561.33..104462.33 rows=1 width=18) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) -> Hash (cost=72.56..72.56 rows=2556 width=4) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=4) (16 rows) The operation below extracts the data in 1997 from lineorder__now table, then move to a new Arrow_Fdw foreign table. $ pg2arrow -d sample -o /opt/tmp/lineorder_1997.arrow \\ -c \"SELECT * FROM lineorder WHERE lo_orderdate between 19970101 and 19971231\" pg2arrow command extracts the data in 1997 from the lineorder table into a new Arrow file. BEGIN; -- -- remove rows in 1997 from the read-writable table -- DELETE FROM lineorder WHERE lo_orderdate BETWEEN 19970101 AND 19971231; -- -- define a new partition leaf which maps log-data in 1997 -- CREATE FOREIGN TABLE lineorder__1997 PARTITION OF lineorder FOR VALUES FROM (19970101) TO (19980101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1997.arrow'); COMMIT; A series of operations above delete the data in 1997 from lineorder__new that is a PostgreSQL table, then maps an Arrow file ( /opt/tmp/lineorder_1997.arrow ) which contains an identical contents as a foreign table lineorder__1997 . For correctness, block size is configurable on build from 4KB to 32KB. \u21a9","title":"Apache Arrow"},{"location":"arrow_fdw/#apache-arrow-columnar-store","text":"","title":"Apache Arrow (Columnar Store)"},{"location":"arrow_fdw/#overview","text":"PostgreSQL tables internally consist of 8KB blocks 1 , and block contains tuples which is a data structure of all the attributes and metadata per row. It collocates date of a row closely, so it works effectively for INSERT/UPDATE-major workloads, but not suitable for summarizing or analytics of mass-data. It is not usual to reference all the columns in a table on mass-data processing, and we tend to reference a part of columns in most cases. In this case, the storage I/O bandwidth consumed by unreferenced columns are waste, however, we have no easy way to fetch only particular columns referenced from the row-oriented data structure. In case of column oriented data structure, in an opposite manner, it has extreme disadvantage on INSERT/UPDATE-major workloads, however, it can pull out maximum performance of storage I/O on mass-data processing workloads because it can loads only referenced columns. From the standpoint of processor efficiency also, column-oriented data structure looks like a flat array that pulls out maximum bandwidth of memory subsystem for GPU, by special memory access pattern called Coalesced Memory Access.","title":"Overview"},{"location":"arrow_fdw/#what-is-apache-arrow","text":"Apache Arrow is a data format of structured data to save in columnar-form and to exchange other applications. Some applications for big-data processing support the format, and it is easy for self-developed applications to use Apache Arrow format since they provides libraries for major programming languages like C,C++ or Python. Apache Arrow format file internally contains Schema portion to define data structure, and one or more RecordBatch to save columnar-data based on the schema definition. For data types, it supports integers, strint (variable-length), date/time types and so on. Indivisual columnar data has its internal representation according to the data types. Data representation in Apache Arrow is not identical with the representation in PostgreSQL. For example, epoch of timestamp in Arrow is 1970-01-01 and it supports multiple precision. In contrast, epoch of timestamp in PostgreSQL is 2001-01-01 and it has microseconds accuracy. Arrow_Fdw allows to read Apache Arrow files on PostgreSQL using foreign table mechanism. If an Arrow file contains 8 of record batches that has million items for each column data, for example, we can access 8 million rows on the Arrow files through the foreign table.","title":"What is Apache Arrow?"},{"location":"arrow_fdw/#operations","text":"","title":"Operations"},{"location":"arrow_fdw/#creation-of-foreign-tables","text":"Usually it takes the 3 steps below to create a foreign table. Define a foreign-data-wrapper using CREATE FOREIGN DATA WRAPPER command Define a foreign server using CREATE SERVER command Define a foreign table using CREATE FOREIGN TABLE command The first 2 steps above are included in the CREATE EXTENSION pg_strom command. All you need to run individually is CREATE FOREIGN TABLE command last. CREATE FOREIGN TABLE flogdata ( ts timestamp, sensor_id int, signal1 smallint, signal2 smallint, signal3 smallint, signal4 smallint, ) SERVER arrow_fdw OPTIONS (file '/path/to/logdata.arrow'); Data type of columns specified by the CREATE FOREIGN TABLE command must be matched to schema definition of the Arrow files to be mapped. Arrow_Fdw also supports a useful manner using IMPORT FOREIGN SCHEMA statement. It automatically generates a foreign table definition using schema definition of the Arrow files. It specifies the foreign table name, schema name to import, and path name of the Arrow files using OPTION-clause. Schema definition of Arrow files contains data types and optional column name for each column. It declares a new foreign table using these information. IMPORT FOREIGN SCHEMA flogdata FROM SERVER arrow_fdw INTO public OPTIONS (file '/path/to/logdata.arrow');","title":"Creation of foreign tables"},{"location":"arrow_fdw/#foreign-table-options","text":"Arrow_Fdw supports the options below.","title":"Foreign table options"},{"location":"arrow_fdw/#foreign-table-options_1","text":"file=PATHNAME It maps an Arrow file specified on the foreign table. files=PATHNAME1[,PATHNAME2...] It maps multiple Arrow files specified by comma (,) separated files list on the foreign table. dir=DIRNAME It maps all the Arrow files in the directory specified on the foreign table. suffix=SUFFIX When dir option is given, it maps only files with the specified suffix, like .arrow` for example. parallel_workers=N_WORKERS It tells the number of workers that should be used to assist a parallel scan of this foreign table; equivalent to parallel_workers storage parameter at normal tables. pattern=PATTERN Maps only files specified by the file , files , or dir option that match the PATTERN , including wildcards, to the foreign table. The following wildcards can be used: ? ... matches any 1 character. * ... matches any string of 0 or more characters. ${KEY} ... matches any string of 0 or more characters. @{KEY} ... matches any numeric string of 0 or more characters. An interesting use of this option is to refer to a portion of a file name matched by the wildcard ${KEY} or @{KEY} as a virtual column. For more information, see the '''Arrow_Fdw virtual column''' section below.","title":"Foreign Table Options"},{"location":"arrow_fdw/#foreign-column-options","text":"field=FIELD It specifies the field name of the Arrow file to map to that column. In the default, Arrow_Fdw maps the first occurrence of a field that has the same column name as this foreign table's column name. virtual=KEY It configures the column is a virtual column. KEY specifies the wildcard key name in the pattern specified by the pattern option of the foreign table option. A virtual column allows to refer to the part of the file name pattern that matches KEY in a query. virtual_metadata=KEY It specifies that the column is a virtual column. KEY specifies a KEY-VALUE pair embedded in the CustomMetadata field of the Arrow file. If the specified KEY-VALUE pair is not found, the column returns a NULL value. There are two types of CustomMetadata in Arrow files: embedded in the schema (corresponding to a PostgreSQL table) and embedded in the field (corresponding to a PostgreSQL column). For example, you can reference CustomMetadata embedded in a field by writing the field name separated by the . character before the KEY value, such as lo_orderdate.max_values . If there is no field name, it will be treated as a KEY-VALUE pair embedded in the schema. virtual_metadata_split=KEY It specifies that the column is a virtual column. KEY specifies the KEY-VALUE pair embedded in the CustomMetadata field of the Arrow file. If the specified KEY-VALUE pair is not found, this column returns a NULL value. The difference from virtual_metadata is that the values of the CustomMetadata field are separated by a delimiter( , ) and applied to each Record Batch in order from the beginning. For example, if the specified CustomMetadata value is Tokyo,Osaka,Kyoto,Yokohama , the row read from RecordBatch-0 will display 'Tokyo' , the row read from RecordBatch-1 will display 'Osaka' , and the row read from RecordBatch-2 will display 'Osaka' as the value of this virtual column.","title":"Foreign Column Options"},{"location":"arrow_fdw/#data-type-mapping","text":"Arrow data types are mapped on PostgreSQL data types as follows. Int mapped to either of int1 , int2 , int4 or int8 according to the bitWidth attribute. is_signed attribute shall be ignored. int1 is an enhanced data type by PG-Strom. FloatingPoint mapped to either of float2 , float4 or float8 according to the precision attribute. float2 is an enhanced data type by PG-Strom. Utf8 , LargeUtf8 mapped to text data type Binary , LargeBinary mapped to bytea data type Decimal mapped to numeric data type Date mapped to date data type; to be adjusted as if it has unit=Day precision. Time mapped to time data type; to be adjusted as if it has unit=MicroSecond precision. Timestamp mapped to timestamp data type; to be adjusted as if it has unit=MicroSecond precision. Interval mapped to interval data type. List , LargeList mapped to 1-dimensional array of the element data type. Struct mapped to compatible composite data type; that shall be defined preliminary. FixedSizeBinary mapped to char(n) data type according to the byteWidth attribute. If pg_type=TYPENAME is configured, PG-Strom may assign the configured data type. Right now, inet and macaddr are supported. Union , Map , Duration Right now, PG-Strom cannot map these Arrow data types onto any of PostgreSQL data types.","title":"Data type mapping"},{"location":"arrow_fdw/#how-to-read-explain","text":"EXPLAIN command show us information about Arrow files reading. The example below is an output of query execution plan that includes f_lineorder foreign table that mapps an Arrow file of 503GB. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM f_lineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=14535261.08..14535261.09 rows=1 width=8) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=14535261.06..14535261.07 rows=1 width=32) GPU Projection: pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint) GPU Scan Quals: ((f_lineorder.lo_discount >= 1) AND (f_lineorder.lo_discount <= 3) AND (f_lineorder.lo_quantity < 25)) [rows: 5999990000 -> 9999983] GPU Join Quals [1]: (f_lineorder.lo_orderdate = date1.d_datekey) ... [nrows: 9999983 -> 1428010] GPU Outer Hash [1]: f_lineorder.lo_orderdate GPU Inner Hash [1]: date1.d_datekey referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount file0: /opt/nvme/f_lineorder_s999.arrow (read: 89.41GB, size: 502.92GB) GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 (cost=0.00..78.95 rows=365 width=4) Filter: (d_year = 1993) (12 rows) According to the EXPLAIN output, we can see Custom Scan (GpuPreAgg) scans f_lineorder foreign table. file0 item shows the filename ( /opt/nvme/lineorder_s999.arrow ) on behalf of the foreign table and its size. If multiple files are mapped, any files are individually shown, like file1 , file2 , ... The referenced item shows the list of referenced columns. We can see this query touches lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns. In addition, GPU-Direct SQL: enabled (GPU-0) shows us the scan on f_lineorder uses GPU-Direct SQL mechanism. VERBOSE option outputs more detailed information. =# EXPLAIN VERBOSE SELECT sum(lo_extendedprice*lo_discount) as revenue FROM f_lineorder,date1 WHERE lo_orderdate = d_datekey AND d_year = 1993 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=14535261.08..14535261.09 rows=1 width=8) Output: pgstrom.sum_int((pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint))) -> Custom Scan (GpuPreAgg) on public.f_lineorder (cost=14535261.06..14535261.07 rows=1 width=32) Output: (pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint)) GPU Projection: pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint) GPU Scan Quals: ((f_lineorder.lo_discount >= 1) AND (f_lineorder.lo_discount <= 3) AND (f_lineorder.lo_quantity < 25)) [rows: 5999990000 -> 9999983] GPU Join Quals [1]: (f_lineorder.lo_orderdate = date1.d_datekey) ... [nrows: 9999983 -> 1428010] GPU Outer Hash [1]: f_lineorder.lo_orderdate GPU Inner Hash [1]: date1.d_datekey referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount file0: /opt/nvme/f_lineorder_s999.arrow (read: 89.41GB, size: 502.92GB) lo_orderdate: 22.35GB lo_quantity: 22.35GB lo_extendedprice: 22.35GB lo_discount: 22.35GB GPU-Direct SQL: enabled (GPU-0) KVars-Slot: <slot=0, type='int4', expr='f_lineorder.lo_discount'>, <slot=1, type='int4', expr='f_lineorder.lo_quantity'>, <slot=2, type='int8', expr='(f_lineorder.lo_extendedprice * f_lineorder.lo_discount)'>, <slot=3, type='int4', expr='f_lineorder.lo_extendedprice'>, <slot=4, type='int4', expr='f_lineorder.lo_orderdate'>, <slot=5, type='int4', expr='date1.d_datekey'> KVecs-Buffer: nbytes: 51200, ndims: 3, items=[kvec0=<0x0000-27ff, type='int4', expr='lo_discount'>, kvec1=<0x2800-4fff, type='int4', expr='lo_quantity'>, kvec2=<0x5000-77ff, type='int4', expr='lo_extendedprice'>, kvec3=<0x7800-9fff, type='int4', expr='lo_orderdate'>, kvec4=<0xa000-c7ff, type='int4', expr='d_datekey'>] LoadVars OpCode: {Packed items[0]={LoadVars(depth=0): kvars=[<slot=4, type='int4' resno=6(lo_orderdate)>, <slot=1, type='int4' resno=9(lo_quantity)>, <slot=3, type='int4' resno=10(lo_extendedprice)>, <slot=0, type='int4' resno=12(lo_discount)>]}, items[1]={LoadVars(depth=1): kvars=[<slot=5, type='int4' resno=1(d_datekey)>]}} MoveVars OpCode: {Packed items[0]={MoveVars(depth=0): items=[<slot=0, offset=0x0000-27ff, type='int4', expr='lo_discount'>, <slot=3, offset=0x5000-77ff, type='int4', expr='lo_extendedprice'>, <slot=4, offset=0x7800-9fff, type='int4', expr='lo_orderdate'>]}}, items[1]={MoveVars(depth=1): items=[<offset=0x0000-27ff, type='int4', expr='lo_discount'>, <offset=0x5000-77ff, type='int4', expr='lo_extendedprice'>]}}} Scan Quals OpCode: {Bool::AND args=[{Func(bool)::int4ge args=[{Var(int4): slot=0, expr='lo_discount'}, {Const(int4): value='1'}]}, {Func(bool)::int4le args=[{Var(int4): slot=0, expr='lo_discount'}, {Const(int4): value='3'}]}, {Func(bool)::int4lt args=[{Var(int4): slot=1, expr='lo_quantity'}, {Const(int4): value='25'}]}]} Join Quals OpCode: {Packed items[1]={JoinQuals: {Func(bool)::int4eq args=[{Var(int4): kvec=0x7800-a000, expr='lo_orderdate'}, {Var(int4): slot=5, expr='d_datekey'}]}}} Join HashValue OpCode: {Packed items[1]={HashValue arg={Var(int4): kvec=0x7800-a000, expr='lo_orderdate'}}} Partial Aggregation OpCode: {AggFuncs <psum::int[slot=2, expr='(lo_extendedprice * lo_discount)']> arg={SaveExpr: <slot=2, type='int8'> arg={Func(int8)::int8 arg={Func(int4)::int4mul args=[{Var(int4): kvec=0x5000-7800, expr='lo_extendedprice'}, {Var(int4): kvec=0x0000-2800, expr='lo_discount'}]}}}} Partial Function BufSz: 16 -> Seq Scan on public.date1 (cost=0.00..78.95 rows=365 width=4) Output: date1.d_datekey Filter: (date1.d_year = 1993) (28 rows) The verbose output additionally displays amount of column-data to be loaded on reference of columns. The load of lo_orderdate , lo_quantity , lo_extendedprice and lo_discount columns needs to read 89.41GB in total. It is 17.8% towards the filesize (502.93GB).","title":"How to read EXPLAIN"},{"location":"arrow_fdw/#arrow_fdw-virtual-column","text":"Arrow_Fdw allows to map multiple Apache Arrow files with compatible schema structures to a single foreign table. For example, if `dir '/opt/arrow/mydata' is configured for foreign table option, all files under that directory will be mapped. When you are converting the contents of a transactional database into an Apache Arrow file, we often dump them to separate files by year and month or specific categories, and its file names reflects these properties. The example below shows an example to convert the transactional table lineorder into Arrow files by year of lo_orderdate and by category of lo_shipmode . $ for s in RAIL AIR TRUCK SHIP FOB MAIL; do for y in 1993 1994 1995 1996 1997; do pg2arrow -d ssbm -c \"SELECT * FROM lineorder_small \\ WHERE lo_orderdate between ${y}0101 and ${y}1231 \\ AND lo_shipmode = '${s}'\" \\ -o /opt/arrow/mydata/f_lineorder_${y}_${s}.arrow done done $ ls /opt/arrow/mydata/ f_lineorder_1993_AIR.arrow f_lineorder_1995_RAIL.arrow f_lineorder_1993_FOB.arrow f_lineorder_1995_SHIP.arrow f_lineorder_1993_MAIL.arrow f_lineorder_1995_TRUCK.arrow f_lineorder_1993_RAIL.arrow f_lineorder_1996_AIR.arrow f_lineorder_1993_SHIP.arrow f_lineorder_1996_FOB.arrow f_lineorder_1993_TRUCK.arrow f_lineorder_1996_MAIL.arrow f_lineorder_1994_AIR.arrow f_lineorder_1996_RAIL.arrow f_lineorder_1994_FOB.arrow f_lineorder_1996_SHIP.arrow f_lineorder_1994_MAIL.arrow f_lineorder_1996_TRUCK.arrow f_lineorder_1994_RAIL.arrow f_lineorder_1997_AIR.arrow f_lineorder_1994_SHIP.arrow f_lineorder_1997_FOB.arrow f_lineorder_1994_TRUCK.arrow f_lineorder_1997_MAIL.arrow f_lineorder_1995_AIR.arrow f_lineorder_1997_RAIL.arrow f_lineorder_1995_FOB.arrow f_lineorder_1997_SHIP.arrow f_lineorder_1995_MAIL.arrow f_lineorder_1997_TRUCK.arrow All these Apache Arrow files have the same schema structure and can be mapped to a single foreign table using the dir option. Also, the Arrow file that has '1995' token in the file name only contains records with lo_orderdate in the range 19950101 to 19951231. The Arrow file that has 'RAIL' token in the file name only contains records with lo_shipmode of RAIL . In other words, even if you define the Arrow_Fdw foreign table that maps these multiple Arrow files, when reading data from a file whose file name includes 1995, it is assumed that the value of lo_orderdate is in the range of 19950101 to 19951231. It is possible for the optimizer to utilize this knowledge. In Arrow_Fdw, you can refer to part of the file name as a column by using the foreign table option pattern . This is called a virtual column and is configured as follows. =# IMPORT FOREIGN SCHEMA f_lineorder FROM SERVER arrow_fdw INTO public OPTIONS (dir '/opt/arrow/mydata', pattern 'f_lineorder_@{year}_${shipping}.arrow'); IMPORT FOREIGN SCHEMA =# \\d f_lineorder Foreign table \"public.f_lineorder\" Column | Type | Collation | Nullable | Default | FDW options --------------------+---------------+-----------+----------+---------+---------------------- lo_orderkey | numeric | | | | lo_linenumber | integer | | | | lo_custkey | numeric | | | | lo_partkey | integer | | | | lo_suppkey | numeric | | | | lo_orderdate | integer | | | | lo_orderpriority | character(15) | | | | lo_shippriority | character(1) | | | | lo_quantity | numeric | | | | lo_extendedprice | numeric | | | | lo_ordertotalprice | numeric | | | | lo_discount | numeric | | | | lo_revenue | numeric | | | | lo_supplycost | numeric | | | | lo_tax | numeric | | | | lo_commit_date | character(8) | | | | lo_shipmode | character(10) | | | | year | bigint | | | | (virtual 'year') shipping | text | | | | (virtual 'shipping') Server: arrow_fdw FDW options: (dir '/opt/arrow/mydata', pattern 'f_lineorder_@{year}_${shipping}.arrow') This foreign table option pattern contains two wildcards. @{year} matches a numeric string larger than or equal to 0 characters, and ${shipping} matches a string larger than or equal to 0 characters. The patterns that match this part of the file name can be referenced in the part specified by the virtual column option. In this case, IMPORT FOREIGN SCHEMA automatically adds column definitions, in addition to the fields contained in the Arrow file itself, as well as the virtual column year (a bigint column) that references the wildcard @{year} , and the virtual column shipping that references the wildcard ${shipping} . =# SELECT lo_orderkey, lo_orderdate, lo_shipmode, year, shipping FROM f_lineorder WHERE year = 1995 AND shipping = 'AIR' LIMIT 10; lo_orderkey | lo_orderdate | lo_shipmode | year | shipping -------------+--------------+-------------+------+---------- 637892 | 19950512 | AIR | 1995 | AIR 638243 | 19950930 | AIR | 1995 | AIR 638273 | 19951214 | AIR | 1995 | AIR 637443 | 19950805 | AIR | 1995 | AIR 637444 | 19950803 | AIR | 1995 | AIR 637510 | 19950831 | AIR | 1995 | AIR 637504 | 19950726 | AIR | 1995 | AIR 637863 | 19950802 | AIR | 1995 | AIR 637892 | 19950512 | AIR | 1995 | AIR 637987 | 19950211 | AIR | 1995 | AIR (10 rows) In other words, you can know what values the virtual columns have before reading the Arrow file mapped by the Arrow_Fdw foreign table. By this feature, if it is obvious that there is no match at all from the search conditions before reading a certain Arrow file, it is possible to skip reading the file itself. See the query and its EXPLAIN ANALYZE output below. This aggregation query reads the f_lineorder foreign table, filters it by some conditions, and then aggregates the total value of lo_extendedprice * lo_discount . At that time, the conditional clause WHERE year = 1994 is added. This is effectively the same as WHERE lo_orderdate BETWEEN 19940101 AND 19942131 , but since year is a virtual column, you can determine whether a matching row exists before reading the Arrow files. In fact, looking at the Stats-Hint: line, 12 Record-Batches were loaded due to the condition (year = 1994) , but 48 Record-Batches were skipped. This is a simple but extremely effective means of reducing I/O load. =# EXPLAIN ANALYZE SELECT sum(lo_extendedprice*lo_discount) as revenue FROM f_lineorder WHERE year = 1994 AND lo_discount between 1 and 3 AND lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------------------------------------- Aggregate (cost=421987.07..421987.08 rows=1 width=32) (actual time=82.914..82.915 rows=1 loops=1) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=421987.05..421987.06 rows=1 width=32) \\ (actual time=82.901..82.903 rows=2 loops=1) GPU Projection: pgstrom.psum(((lo_extendedprice * lo_discount))::double precision) GPU Scan Quals: ((year = 1994) AND (lo_discount <= '3'::numeric) AND \\ (lo_quantity < '25'::numeric) AND \\ (lo_discount >= '1'::numeric)) [plan: 65062080 -> 542, exec: 13001908 -> 1701726] referenced: lo_quantity, lo_extendedprice, lo_discount, year Stats-Hint: (year = 1994) [loaded: 12, skipped: 48] file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 99.53MB, size: 427.16MB) file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 99.52MB, size: 427.13MB) file2: /opt/arrow/mydata/f_lineorder_1994_FOB.arrow (read: 99.18MB, size: 425.67MB) : : : : file27: /opt/arrow/mydata/f_lineorder_1997_MAIL.arrow (read: 99.23MB, size: 425.87MB) file28: /opt/arrow/mydata/f_lineorder_1995_MAIL.arrow (read: 99.16MB, size: 425.58MB) file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 99.24MB, size: 425.91MB) GPU-Direct SQL: enabled (N=2,GPU0,1; direct=76195, ntuples=13001908) Planning Time: 2.402 ms Execution Time: 83.857 ms (39 rows)","title":"Arrow_Fdw Virtual Column"},{"location":"arrow_fdw/#how-to-make-arrow-files","text":"This section introduces the way to transform dataset already stored in PostgreSQL database system into Apache Arrow file.","title":"How to make Arrow files"},{"location":"arrow_fdw/#using-pyarrowpandas","text":"A pair of PyArrow module, developed by Arrow developers community, and Pandas data frame can dump PostgreSQL database into an Arrow file. The example below reads all the data in table t0 , then write out them into /tmp/t0.arrow . import pyarrow as pa import pandas as pd X = pd.read_sql(sql=\"SELECT * FROM t0\", con=\"postgresql://localhost/postgres\") Y = pa.Table.from_pandas(X) f = pa.RecordBatchFileWriter('/tmp/t0.arrow', Y.schema) f.write_table(Y,1000000) # RecordBatch for each million rows f.close() Please note that the above operation once keeps query result of the SQL on memory, so should pay attention on memory consumption if you want to transfer massive rows at once.","title":"Using PyArrow+Pandas"},{"location":"arrow_fdw/#using-pg2arrow","text":"On the other hand, pg2arrow command, developed by PG-Strom Development Team, enables us to write out query result into Arrow file. This tool is designed to write out massive amount of data into storage device like NVME-SSD. It fetch query results from PostgreSQL database system, and write out Record Batches of Arrow format for each data size specified by the -s|--segment-size option. Thus, its memory consumption is relatively reasonable. pg2arrow command is distributed with PG-Strom. It shall be installed on the bin directory of PostgreSQL related utilities. $ pg2arrow --help Usage: pg2arrow [OPTION] [database] [username] General options: -d, --dbname=DBNAME Database name to connect to -c, --command=COMMAND SQL command to run -t, --table=TABLENAME Equivalent to '-c SELECT * FROM TABLENAME' (-c and -t are exclusive, either of them must be given) --inner-join=SUB_COMMAND --outer-join=SUB_COMMAND -o, --output=FILENAME result file in Apache Arrow format --append=FILENAME result Apache Arrow file to be appended (--output and --append are exclusive. If neither of them are given, it creates a temporary file.) -S, --stat[=COLUMNS] embeds min/max statistics for each record batch COLUMNS is a comma-separated list of the target columns if partially enabled. Arrow format options: -s, --segment-size=SIZE size of record batch for each Connection options: -h, --host=HOSTNAME database server host -p, --port=PORT database server port -u, --user=USERNAME database user name -w, --no-password never prompt for password -W, --password force password prompt Other options: --dump=FILENAME dump information of arrow file --progress shows progress of the job --set=NAME:VALUE config option to set before SQL execution --help shows this message Report bugs to <pgstrom@heterodbcom>. The -h or -U option specifies the connection parameters of PostgreSQL, like psql or pg_dump . The simplest usage of this command is running a SQL command specified by -c|--command option on PostgreSQL server, then write out results into the file specified by -o|--output option in Arrow format. --append option is available, instead of -o|--output option. It means appending data to existing Apache Arrow file. In this case, the target Apache Arrow file must have fully identical schema definition towards the specified SQL command. The example below reads all the data in table t0 , then write out them into the file /tmp/t0.arrow . $ pg2arrow -U kaigai -d postgres -c \"SELECT * FROM t0\" -o /tmp/t0.arrow Although it is an option for developers, --dump <filename> prints schema definition and record-batch location and size of Arrow file in human readable form. --progress option enables to show progress of the task. It is useful when a huge table is transformed to Apache Arrow format.","title":"Using Pg2Arrow"},{"location":"arrow_fdw/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"arrow_fdw/#ssdtogpu-direct-sql","text":"In case when all the Arrow files mapped on the Arrow_Fdw foreign table satisfies the terms below, PG-Strom enables SSD-to-GPU Direct SQL to load columnar data. Arrow files are on NVME-SSD volume. NVME-SSD volume is managed by Ext4 filesystem. Total size of Arrow files exceeds the pg_strom.nvme_strom_threshold configuration.","title":"SSDtoGPU Direct SQL"},{"location":"arrow_fdw/#partition-configuration","text":"Arrow_Fdw foreign tables can be used as a part of partition leafs. Usual PostgreSQL tables can be mixtured with Arrow_Fdw foreign tables. So, pay attention Arrow_Fdw foreign table does not support any writer operations. And, make boundary condition of the partition consistent to the contents of the mapped Arrow file. It is a responsibility of the database administrators. A typical usage scenario is processing of long-standing accumulated log-data. Unlike transactional data, log-data is mostly write-once and will never be updated / deleted. Thus, by migration of the log-data after a lapse of certain period into Arrow_Fdw foreign table that is read-only but rapid processing, we can accelerate summarizing and analytics workloads. In addition, log-data likely have timestamp, so it is quite easy design to add partition leafs periodically, like monthly, weekly or others. The example below defines a partitioned table that mixes a normal PostgreSQL table and Arrow_Fdw foreign tables. The normal PostgreSQL table, is read-writable, is specified as default partition, so DBA can migrate only past log-data into Arrow_Fdw foreign table under the database system operations. CREATE TABLE lineorder ( lo_orderkey numeric, lo_linenumber integer, lo_custkey numeric, lo_partkey integer, lo_suppkey numeric, lo_orderdate integer, lo_orderpriority character(15), lo_shippriority character(1), lo_quantity numeric, lo_extendedprice numeric, lo_ordertotalprice numeric, lo_discount numeric, lo_revenue numeric, lo_supplycost numeric, lo_tax numeric, lo_commit_date character(8), lo_shipmode character(10) ) PARTITION BY RANGE (lo_orderdate); CREATE TABLE lineorder__now PARTITION OF lineorder default; CREATE FOREIGN TABLE lineorder__1993 PARTITION OF lineorder FOR VALUES FROM (19930101) TO (19940101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1993.arrow'); CREATE FOREIGN TABLE lineorder__1994 PARTITION OF lineorder FOR VALUES FROM (19940101) TO (19950101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1994.arrow'); CREATE FOREIGN TABLE lineorder__1995 PARTITION OF lineorder FOR VALUES FROM (19950101) TO (19960101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1995.arrow'); CREATE FOREIGN TABLE lineorder__1996 PARTITION OF lineorder FOR VALUES FROM (19960101) TO (19970101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1996.arrow'); Below is the query execution plan towards the table. By the query condition lo_orderdate between 19950701 and 19960630 that touches boundary condition of the partition, the partition leaf lineorder__1993 and lineorder__1994 are pruned, so it makes a query execution plan to read other (foreign) tables only. =# EXPLAIN SELECT sum(lo_extendedprice*lo_discount) as revenue FROM lineorder,date1 WHERE lo_orderdate = d_datekey AND lo_orderdate between 19950701 and 19960630 AND lo_discount between 1 and 3 ABD lo_quantity < 25; QUERY PLAN -------------------------------------------------------------------------------- Aggregate (cost=172088.90..172088.91 rows=1 width=32) -> Hash Join (cost=10548.86..172088.51 rows=77 width=64) Hash Cond: (lineorder__1995.lo_orderdate = date1.d_datekey) -> Append (cost=10444.35..171983.80 rows=77 width=67) -> Custom Scan (GpuScan) on lineorder__1995 (cost=10444.35..33671.87 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1995.arrow (size: 892.57MB) -> Custom Scan (GpuScan) on lineorder__1996 (cost=10444.62..33849.21 rows=38 width=68) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount files0: /opt/tmp/lineorder_1996.arrow (size: 897.87MB) -> Custom Scan (GpuScan) on lineorder__now (cost=11561.33..104462.33 rows=1 width=18) GPU Filter: ((lo_orderdate >= 19950701) AND (lo_orderdate <= 19960630) AND (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) -> Hash (cost=72.56..72.56 rows=2556 width=4) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=4) (16 rows) The operation below extracts the data in 1997 from lineorder__now table, then move to a new Arrow_Fdw foreign table. $ pg2arrow -d sample -o /opt/tmp/lineorder_1997.arrow \\ -c \"SELECT * FROM lineorder WHERE lo_orderdate between 19970101 and 19971231\" pg2arrow command extracts the data in 1997 from the lineorder table into a new Arrow file. BEGIN; -- -- remove rows in 1997 from the read-writable table -- DELETE FROM lineorder WHERE lo_orderdate BETWEEN 19970101 AND 19971231; -- -- define a new partition leaf which maps log-data in 1997 -- CREATE FOREIGN TABLE lineorder__1997 PARTITION OF lineorder FOR VALUES FROM (19970101) TO (19980101) SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1997.arrow'); COMMIT; A series of operations above delete the data in 1997 from lineorder__new that is a PostgreSQL table, then maps an Arrow file ( /opt/tmp/lineorder_1997.arrow ) which contains an identical contents as a foreign table lineorder__1997 . For correctness, block size is configurable on build from 4KB to 32KB. \u21a9","title":"Partition configuration"},{"location":"brin/","text":"Index Support Overview PostgreSQL supports several index strategies. The default is B-tree that can rapidly fetch records with a particular value. Elsewhere, it also supports Hash, BRIN, GiST, GIN and others that have own characteristics for each. PG-Strom supports only BRIN-index right now. BRIN-index works efficiently on the dataset we can expect physically neighbor records have similar key values, like timestamp values of time-series data. It allows to skip blocks-range if any records in the range obviously don't match to the scan qualifiers, then, also enables to reduce the amount of I/O due to full table scan. PG-Strom also utilizes the feature of BRIN-index, to skip obviously unnecessary blocks from the ones to be loaded to GPU. Configuration No special configurations are needed to use BRIN-index. PG-Strom automatically applies BRIN-index based scan if BRIN-index is configured on the referenced columns and scan qualifiers are suitable to the index. Also see the PostgreSQL Documentation for the BRIN-index feature. By the GUC parameters below, PG-Strom enables/disables usage of BRIN-index. It usually don't need to change from the default configuration, except for debugging or trouble shooting. Parameter Type Default Description pg_strom.enable_brin bool on enables/disables usage of BRIN-index Operations By EXPLAIN , we can check whether BRIN-index is in use. postgres=# EXPLAIN ANALYZE SELECT * FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' AND cat LIKE '%aaa%'; QUERY PLAN -------------------------------------------------------------------------------- Custom Scan (GpuScan) on dt (cost=94810.93..176275.00 rows=169992 width=44) (actual time=1777.819..1901.537 rows=175277 loops=1) GPU Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date) AND (cat ~~ '%aaa%'::text)) Rows Removed by GPU Filter: 4385491 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 0.529 ms Execution time: 2323.063 ms (7 rows) In the example above, BRIN-index is configured on the ymd column. BRIN cond shows the qualifier of BRIN-index for concentration. BRIN skipped shows the number of skipped blocks actually. In this case, 424704 blocks are skipped, then, it filters out 4385491 rows in the loaded blocks by the scan qualifiers. GpuJoin and GpuPreAgg often pulls up its underlying table scan and runs the scan by itself, to reduce inefficient data transfer. In this case, it also uses the BRIN-index to concentrate the scan. The example below shows a usage of BRIN-index in a query which includes GROUP BY. postgres=# EXPLAIN ANALYZE SELECT cat,count(*) FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=6149.78..6151.86 rows=26 width=12) (actual time=427.482..427.499 rows=26 loops=1) Group Key: cat -> Sort (cost=6149.78..6150.24 rows=182 width=12) (actual time=427.465..427.467 rows=26 loops=1) Sort Key: cat Sort Method: quicksort Memory: 26kB -> Custom Scan (GpuPreAgg) on dt (cost=6140.68..6142.95 rows=182 width=12) (actual time=427.331..427.339 rows=26 loops=1) Reduction: Local Outer Scan: dt (cost=4000.00..4011.99 rows=4541187 width=4) (actual time=78.573..415.961 rows=4560768 loops=1) Outer Scan Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) Rows Removed by Outer Scan Filter: 15564 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 30.992 ms Execution time: 818.994 ms (14 rows)","title":"BRIN Index"},{"location":"brin/#index-support","text":"","title":"Index Support"},{"location":"brin/#overview","text":"PostgreSQL supports several index strategies. The default is B-tree that can rapidly fetch records with a particular value. Elsewhere, it also supports Hash, BRIN, GiST, GIN and others that have own characteristics for each. PG-Strom supports only BRIN-index right now. BRIN-index works efficiently on the dataset we can expect physically neighbor records have similar key values, like timestamp values of time-series data. It allows to skip blocks-range if any records in the range obviously don't match to the scan qualifiers, then, also enables to reduce the amount of I/O due to full table scan. PG-Strom also utilizes the feature of BRIN-index, to skip obviously unnecessary blocks from the ones to be loaded to GPU.","title":"Overview"},{"location":"brin/#configuration","text":"No special configurations are needed to use BRIN-index. PG-Strom automatically applies BRIN-index based scan if BRIN-index is configured on the referenced columns and scan qualifiers are suitable to the index. Also see the PostgreSQL Documentation for the BRIN-index feature. By the GUC parameters below, PG-Strom enables/disables usage of BRIN-index. It usually don't need to change from the default configuration, except for debugging or trouble shooting. Parameter Type Default Description pg_strom.enable_brin bool on enables/disables usage of BRIN-index","title":"Configuration"},{"location":"brin/#operations","text":"By EXPLAIN , we can check whether BRIN-index is in use. postgres=# EXPLAIN ANALYZE SELECT * FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' AND cat LIKE '%aaa%'; QUERY PLAN -------------------------------------------------------------------------------- Custom Scan (GpuScan) on dt (cost=94810.93..176275.00 rows=169992 width=44) (actual time=1777.819..1901.537 rows=175277 loops=1) GPU Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date) AND (cat ~~ '%aaa%'::text)) Rows Removed by GPU Filter: 4385491 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 0.529 ms Execution time: 2323.063 ms (7 rows) In the example above, BRIN-index is configured on the ymd column. BRIN cond shows the qualifier of BRIN-index for concentration. BRIN skipped shows the number of skipped blocks actually. In this case, 424704 blocks are skipped, then, it filters out 4385491 rows in the loaded blocks by the scan qualifiers. GpuJoin and GpuPreAgg often pulls up its underlying table scan and runs the scan by itself, to reduce inefficient data transfer. In this case, it also uses the BRIN-index to concentrate the scan. The example below shows a usage of BRIN-index in a query which includes GROUP BY. postgres=# EXPLAIN ANALYZE SELECT cat,count(*) FROM dt WHERE ymd BETWEEN '2018-01-01' AND '2018-12-31' GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- GroupAggregate (cost=6149.78..6151.86 rows=26 width=12) (actual time=427.482..427.499 rows=26 loops=1) Group Key: cat -> Sort (cost=6149.78..6150.24 rows=182 width=12) (actual time=427.465..427.467 rows=26 loops=1) Sort Key: cat Sort Method: quicksort Memory: 26kB -> Custom Scan (GpuPreAgg) on dt (cost=6140.68..6142.95 rows=182 width=12) (actual time=427.331..427.339 rows=26 loops=1) Reduction: Local Outer Scan: dt (cost=4000.00..4011.99 rows=4541187 width=4) (actual time=78.573..415.961 rows=4560768 loops=1) Outer Scan Filter: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) Rows Removed by Outer Scan Filter: 15564 BRIN cond: ((ymd >= '2018-01-01'::date) AND (ymd <= '2018-12-31'::date)) BRIN skipped: 424704 Planning time: 30.992 ms Execution time: 818.994 ms (14 rows)","title":"Operations"},{"location":"fluentd/","text":"connect with Fluentd This chapter introduces the cooperation with Fluentd via Apache Arrow data format for the efficient importing of IoT/M2M log data. Overview In the technological domain known as IoT/M2M, various software has been developed to store and analyze the large amount of log data generated by devices such as cell phones, automobiles, and various sensors, as well as PCs and servers. This is because the data generated by a large number of devices from time-by-time tend to grow up huge, and special architecture/technology is required to process it in a practical amount of time. PG-Strom's features are designed and implemented for high-speed processing of log data of such a scale. On the other hand, it tends to be a time-consuming job to transfer and import these data into a database in order to make it searchable/summarizable on such a scale. Therefore, PG-Strom includes a fluent-plugin-arrow-file module for Fluentd that outputs the data in Apache Arrow format, and tries to deal with the problem. Fluentd is a log collection tool developed by Sadayuki Furuhashi. It is the de facto standard for collecting and storing a wide variety of log data, ranging from server logs like Syslog to device logs of IoT/M2M devices. Fluentd allows customization of the input/output and processing of log data by adding plugins written in Ruby. As of 2022, more than 800 plugins have been introduced on the official website. PG-Strom can handle two types of data formats: PostgreSQL Heap format (transactional row data) and Apache Arrow format (structured column data). The Apache Arrow format is suitable for workloads like those in the IoT/M2M, where huge amounts of data are generated time-by-time. arrow-file plugin This chapter describes the approach to write out the log data collected by Fluentd in Apache Arrow format, and to refere it with PG-Strom. We assume fluentd here, that is a stable version of Fluentd provided by Treasure Data. PG-Strom includes the fluent-plugin-arrow-file module. This allows Fluentd to write out the log data it collects as an Apache Arrow format file with a specified schema structure. Using PG-Strom's Arrow_Fdw, this Apache Arrow format file can be accessed as an foreign table. In addition, GPU-Direct SQL allows to load these files extremely fast, if the storage system is appropriately configured. This has the following advantages: There is no need to import data into the DB because PG-Strom directly accesses the files output by Fluentd. The data readout (I/O load) for the searching and summarizing process can be kept to a minimum because of the columnar data format. You can archive outdated log data only by moving files on the OS. On the other hand, in cases that it takes a long time to store a certain size of log (for example, log generation is rare), another method such as outputting to a PostgreSQL table is suitable for more real-time log analysis. This is because the size of the Record Batch needs to be reasonably large to acquire the performance benefits of the Apache Arrow format. Internals There are several types of plugins for Fluentd: Input plugins to receive logs from outside, Parser plugins to shape the logs, Buffer plugins to temporarily store the received logs, and Output plugins to output the logs. The arrow-file plugin is categorized as Output plugin. It writes out a \"chunk\" of log data passed from the Buffer plugin in Apache Arrow format with the schema structure specified in the configuration. The Input/Parser plugin is responsible for converting the received logs into a common format so that the Buffer and Output plugins can handle the input data without being aware of its format. The common format is a pair of tag , an identifier that can be used to sort the logs, a log timestamp time , and record , an associative array formed from the raw logs. The arrow-file plugin writes to an Apache Arrow format file with the tag and time fields and each element of the record associative array as a column (some may be omitted). Therefore, the output destination file name and schema definition information (mapping of associative array elements to columns/types) are mandatory configuration parameters. Installation Install the fluentd package for Linux distribution you are using. The rake-compiler module is required to install the arrow-file plugin, so please install it before. $ curl -fsSL https://toolbelt.treasuredata.com/sh/install-redhat-fluent-package5-lts.sh | sh $ sudo /opt/fluent/bin/fluent-gem install rake-compiler Next, download the source code for PG-Strom and build arrow-file plugin in the fluentd directory. $ git clone https://github.com/heterodb/pg-strom.git $ cd pg-strom/fluentd $ make FLUENT=1 gem $ sudo make FLUENT=1 install To confirm that the Fluentd plugin is installed, run the following command. $ /opt/fluent/bin/fluent-gem list | grep arrow fluent-plugin-arrow-file (0.3) Configuration As mentioned above, the arrow-file plugin requires the output path of the file and the schema definition at least. In addition to this, in order to acquire the best performance for searching and aggregation processing, the single chunk of data inside the Apache Arrow file, called the Record Batch, needs to be reasonably large in size. The arrow-file plugin creates a Record Batch for each chunk passed from the Buffer plugin. Therefore, the buffer size of the Buffer plugin should be set with the size of the Record Batch in mind. By the default, the Buffer plugin is set to take a buffer size of 256MB. The configuration parameters for the arrow-file plugin are as follows: path [type: String ] (Required) Specify the file path of the destination. This is a required parameter, and you can use placeholders shown below. Placeholders Description %Y The current year expressed as a four-digit number. %y The current year expressed as a number in the last two digits of the year. %m A two-digit number representing the current month, 01-12. %d A two-digit number representing the current day from 01 to 31. %H A two-digit number representing the hour of the current time 00-23. %M A two-digit number representing the minute of the current time, 00-59. %S A two-digit number representing the second of the current time, 00-59. %p The PID of the current Fluentd process. The placeholder is replaced when the chunk is written out. If an Apache Arrow format file of the same name exists, the Record Batch will be appended to it. If it does not exist, a new Apache Arrow format file is created and the first Record Batch is written out. However, if the size of the existing Apache Arrow file exceeds the filesize_threshold setting described below, rename the existing file and then create a new one. (Example) path /tmp/arrow_logs/my_logs_%y%m%d.%p.log The output Apache Arrow file updates the footer area to point to all Record Batches each time a chunk is written out. Therefore, the generated Apache Arrow file can be read immediately. However, to avoid access conflicts, exclusive handling is required using lockf(3) . schema_defs [type: String ] (Required) Specify the schema definition of the Apache Arrow format file output by fluent-plugin-arrow-file . This is a required parameter, and define the schema structure using strings in the following format. schema_defs := column_def1[,column_def2 ...] column_def := <column_name>=<column_type>[;<column_attrs>] <column_name> is the name of the column, which must match the key value of the associative array passed from Fluentd to arrow-file plugin. <column_type> is the data type of the column. See the following table. <column_attrs> is an additional attribute for columns. At this time, only the following attributes are supported. stat_enabled ... The statistics for the configured columns will be collected and the maximum/minimum values for each Record Batch will be set as custom metadata in the form of max_values=... and min_values=... . (Example) schema_defs \"ts=Timestamp;stat_enabled,dev_id=Uint32,temperature=Float32,humidity=Float32\" Data types supported by the arrow-file plugin Data types Description Int8 Int16 Int32 Int64 Signed integer with the specified bit width. Uint8 Uint16 Uint32 Uint64 Unsigned integer with the specified bit width Float16 Float32 Float64 Floating point number with half-precision(16bit), single-precision(32bit) and double-precision(64bit). Decimal Decimal128 128-bit fixed decimal; 256-bit fixed decimal is currently not supported. Timestamp Timestamp[sec] Timestamp[ms] Timestamp[us] Timestamp[ns] Timestamp type with the specified precision. If the precision is omitted, it is treated the same as [us] . Time Time[sec] Time[ms] Time[us] Time[ns] Time type with the specified precision. If the precision is omitted, it is treated the same as [sec] . Date Date[Day] Date[ms] Date type with the specified precision. If the precision is omitted, it is treated the same as [day] . Utf8 String type. Ipaddr4 IP address(IPv4) type. This is represented as a FixedSizeBinary type with byteWidth=4 and custom metadata pg_type=pg_catalog.inet . Ipaddr6 IP address(IPv6) type. This is represented as a FixedSizeBinary type with byteWidth=16 and custom metadata pg_type=pg_catalog.inet . ts_column [type: String / default: unspecified] Specify a column name to set the timestamp value of the log passed from Fluentd (not from record ). This parameter is usually a date-time type such as Timestamp , and the stat_enabled attribute should also be specified to achieve fast search. tag_column [type: String / default: unspecified] Specify a column name to set the tag value of the log passed from Fluentd (not from record ). This parameter is usually a string type such as utf8 . filesize_threshold [type: Integer / default: 10000] Specify the threshold for switching the output destination file in MB. By default, the output destination is switched when the file size exceeds about 10GB. Example As a simple example, this chapter shows a configuration for monitoring the log of a local Apache Httpd server, parsing it field by field, and writing it to an Apache Arrow format file. By setting <source> , /var/log/httpd/access_log will be the data source. Then, the apache2 Parse plugin will cut out the fields: host, user, time, method, path, code, size, referer, agent. The fields are then passed to the arrow-file plugin as an associative array. In schema_defs in <match> , the column definitions of the Apache Arrow file corresponding to the fields are set. For simplicity of explanation, the chunk size is set to a maximum of 4MB / 200 lines in the <buffer> tag, and it is set to pass to the Output plugin in 10 seconds at most. Example configuration of /etc/fluent/fluentd.conf <source> @type tail path /var/log/httpd/access_log pos_file /var/log/fluent/httpd_access.pos tag httpd format apache2 <parse> @type apache2 expression /^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \\[(?<time>[^\\]]*)\\] \"(?<method>\\S+)(?: +(?<path>(?:[^\\\"]|\\\\.)*?)(?: +\\S*)?)?\" (?<code>[^ ]*) (?<size>[^ ]*)(?: \"(?<referer>(?:[^\\\"]|\\\\.)*)\" \"(?<agent>(?:[^\\\"]|\\\\.)*)\")?$/ time_format %d/%b/%Y:%H:%M:%S %z </parse> </source> <match httpd> @type arrow_file path /tmp/mytest%Y%m%d.%p.arrow schema_defs \"ts=Timestamp[sec],host=Utf8,method=Utf8,path=Utf8,code=Int32,size=Int32,referer=Utf8,agent=Utf8\" ts_column \"ts\" <buffer> flush_interval 10s chunk_limit_size 4MB chunk_limit_records 200 </buffer> </match> Start the fluentd service. $ sudo systemctl start fluentd See the following output. The placeholder for /tmp/mytest%Y%m%d.%p.arrow set in path is replaced and the Apache Httpd log is written to /tmp/mytest20220124.3206341.arrow . $ arrow2csv /tmp/mytest20220124.3206341.arrow --head --offset 300 --limit 10 \"ts\",\"host\",\"method\",\"path\",\"code\",\"size\",\"referer\",\"agent\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/js/theme_extra.js\",200,195,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/js/theme.js\",200,4401,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/img/fluentd_overview.png\",200,121459,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/search/main.js\",200,3027,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/Lato/lato-regular.woff2\",200,182708,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/fontawesome-webfont.woff2?v=4.7.0\",200,77160,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/RobotoSlab/roboto-slab-v7-bold.woff2\",200,67312,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/Lato/lato-bold.woff2\",200,184912,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:43\",\"192.168.77.95\",\"GET\",\"/docs/ja/search/worker.js\",200,3724,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:43\",\"192.168.77.95\",\"GET\",\"/docs/ja/img/favicon.ico\",200,1150,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" Let's map the output file to PostgreSQL using PG-Strom's Arrow_Fdw. postgres=# IMPORT FOREIGN SCHEMA mytest FROM SERVER arrow_fdw INTO public OPTIONS (file '/tmp/mytest20220124.3206341.arrow'); IMPORT FOREIGN SCHEMA postgres=# SELECT ts, host, path FROM mytest WHERE code = 404; ts | host | path ---------------------+---------------+---------------------- 2022-01-24 12:02:06 | 192.168.77.73 | /~kaigai/ja/fluentd/ (1 row) postgres=# EXPLAIN SELECT ts, host, path FROM mytest WHERE code = 404; QUERY PLAN ------------------------------------------------------------------------------ Custom Scan (GpuScan) on mytest (cost=4026.12..4026.12 rows=3 width=72) GPU Filter: (code = 404) referenced: ts, host, path, code files0: /tmp/mytest20220124.3206341.arrow (read: 128.00KB, size: 133.94KB) (4 rows) The example above shows how the generated Apache Arrow file can be mapped as an external table and accessed with SQL. Search conditions can be given to search each field of the log formed on the Fluentd side. In the example above, the log with HTTP status code 404 is searched and one record is found.","title":"connect with Fluentd"},{"location":"fluentd/#connect-with-fluentd","text":"This chapter introduces the cooperation with Fluentd via Apache Arrow data format for the efficient importing of IoT/M2M log data.","title":"connect with Fluentd"},{"location":"fluentd/#overview","text":"In the technological domain known as IoT/M2M, various software has been developed to store and analyze the large amount of log data generated by devices such as cell phones, automobiles, and various sensors, as well as PCs and servers. This is because the data generated by a large number of devices from time-by-time tend to grow up huge, and special architecture/technology is required to process it in a practical amount of time. PG-Strom's features are designed and implemented for high-speed processing of log data of such a scale. On the other hand, it tends to be a time-consuming job to transfer and import these data into a database in order to make it searchable/summarizable on such a scale. Therefore, PG-Strom includes a fluent-plugin-arrow-file module for Fluentd that outputs the data in Apache Arrow format, and tries to deal with the problem. Fluentd is a log collection tool developed by Sadayuki Furuhashi. It is the de facto standard for collecting and storing a wide variety of log data, ranging from server logs like Syslog to device logs of IoT/M2M devices. Fluentd allows customization of the input/output and processing of log data by adding plugins written in Ruby. As of 2022, more than 800 plugins have been introduced on the official website. PG-Strom can handle two types of data formats: PostgreSQL Heap format (transactional row data) and Apache Arrow format (structured column data). The Apache Arrow format is suitable for workloads like those in the IoT/M2M, where huge amounts of data are generated time-by-time.","title":"Overview"},{"location":"fluentd/#arrow-file-plugin","text":"This chapter describes the approach to write out the log data collected by Fluentd in Apache Arrow format, and to refere it with PG-Strom. We assume fluentd here, that is a stable version of Fluentd provided by Treasure Data. PG-Strom includes the fluent-plugin-arrow-file module. This allows Fluentd to write out the log data it collects as an Apache Arrow format file with a specified schema structure. Using PG-Strom's Arrow_Fdw, this Apache Arrow format file can be accessed as an foreign table. In addition, GPU-Direct SQL allows to load these files extremely fast, if the storage system is appropriately configured. This has the following advantages: There is no need to import data into the DB because PG-Strom directly accesses the files output by Fluentd. The data readout (I/O load) for the searching and summarizing process can be kept to a minimum because of the columnar data format. You can archive outdated log data only by moving files on the OS. On the other hand, in cases that it takes a long time to store a certain size of log (for example, log generation is rare), another method such as outputting to a PostgreSQL table is suitable for more real-time log analysis. This is because the size of the Record Batch needs to be reasonably large to acquire the performance benefits of the Apache Arrow format.","title":"arrow-file plugin"},{"location":"fluentd/#internals","text":"There are several types of plugins for Fluentd: Input plugins to receive logs from outside, Parser plugins to shape the logs, Buffer plugins to temporarily store the received logs, and Output plugins to output the logs. The arrow-file plugin is categorized as Output plugin. It writes out a \"chunk\" of log data passed from the Buffer plugin in Apache Arrow format with the schema structure specified in the configuration. The Input/Parser plugin is responsible for converting the received logs into a common format so that the Buffer and Output plugins can handle the input data without being aware of its format. The common format is a pair of tag , an identifier that can be used to sort the logs, a log timestamp time , and record , an associative array formed from the raw logs. The arrow-file plugin writes to an Apache Arrow format file with the tag and time fields and each element of the record associative array as a column (some may be omitted). Therefore, the output destination file name and schema definition information (mapping of associative array elements to columns/types) are mandatory configuration parameters.","title":"Internals"},{"location":"fluentd/#installation","text":"Install the fluentd package for Linux distribution you are using. The rake-compiler module is required to install the arrow-file plugin, so please install it before. $ curl -fsSL https://toolbelt.treasuredata.com/sh/install-redhat-fluent-package5-lts.sh | sh $ sudo /opt/fluent/bin/fluent-gem install rake-compiler Next, download the source code for PG-Strom and build arrow-file plugin in the fluentd directory. $ git clone https://github.com/heterodb/pg-strom.git $ cd pg-strom/fluentd $ make FLUENT=1 gem $ sudo make FLUENT=1 install To confirm that the Fluentd plugin is installed, run the following command. $ /opt/fluent/bin/fluent-gem list | grep arrow fluent-plugin-arrow-file (0.3)","title":"Installation"},{"location":"fluentd/#configuration","text":"As mentioned above, the arrow-file plugin requires the output path of the file and the schema definition at least. In addition to this, in order to acquire the best performance for searching and aggregation processing, the single chunk of data inside the Apache Arrow file, called the Record Batch, needs to be reasonably large in size. The arrow-file plugin creates a Record Batch for each chunk passed from the Buffer plugin. Therefore, the buffer size of the Buffer plugin should be set with the size of the Record Batch in mind. By the default, the Buffer plugin is set to take a buffer size of 256MB. The configuration parameters for the arrow-file plugin are as follows: path [type: String ] (Required) Specify the file path of the destination. This is a required parameter, and you can use placeholders shown below. Placeholders Description %Y The current year expressed as a four-digit number. %y The current year expressed as a number in the last two digits of the year. %m A two-digit number representing the current month, 01-12. %d A two-digit number representing the current day from 01 to 31. %H A two-digit number representing the hour of the current time 00-23. %M A two-digit number representing the minute of the current time, 00-59. %S A two-digit number representing the second of the current time, 00-59. %p The PID of the current Fluentd process. The placeholder is replaced when the chunk is written out. If an Apache Arrow format file of the same name exists, the Record Batch will be appended to it. If it does not exist, a new Apache Arrow format file is created and the first Record Batch is written out. However, if the size of the existing Apache Arrow file exceeds the filesize_threshold setting described below, rename the existing file and then create a new one. (Example) path /tmp/arrow_logs/my_logs_%y%m%d.%p.log The output Apache Arrow file updates the footer area to point to all Record Batches each time a chunk is written out. Therefore, the generated Apache Arrow file can be read immediately. However, to avoid access conflicts, exclusive handling is required using lockf(3) . schema_defs [type: String ] (Required) Specify the schema definition of the Apache Arrow format file output by fluent-plugin-arrow-file . This is a required parameter, and define the schema structure using strings in the following format. schema_defs := column_def1[,column_def2 ...] column_def := <column_name>=<column_type>[;<column_attrs>] <column_name> is the name of the column, which must match the key value of the associative array passed from Fluentd to arrow-file plugin. <column_type> is the data type of the column. See the following table. <column_attrs> is an additional attribute for columns. At this time, only the following attributes are supported. stat_enabled ... The statistics for the configured columns will be collected and the maximum/minimum values for each Record Batch will be set as custom metadata in the form of max_values=... and min_values=... . (Example) schema_defs \"ts=Timestamp;stat_enabled,dev_id=Uint32,temperature=Float32,humidity=Float32\" Data types supported by the arrow-file plugin Data types Description Int8 Int16 Int32 Int64 Signed integer with the specified bit width. Uint8 Uint16 Uint32 Uint64 Unsigned integer with the specified bit width Float16 Float32 Float64 Floating point number with half-precision(16bit), single-precision(32bit) and double-precision(64bit). Decimal Decimal128 128-bit fixed decimal; 256-bit fixed decimal is currently not supported. Timestamp Timestamp[sec] Timestamp[ms] Timestamp[us] Timestamp[ns] Timestamp type with the specified precision. If the precision is omitted, it is treated the same as [us] . Time Time[sec] Time[ms] Time[us] Time[ns] Time type with the specified precision. If the precision is omitted, it is treated the same as [sec] . Date Date[Day] Date[ms] Date type with the specified precision. If the precision is omitted, it is treated the same as [day] . Utf8 String type. Ipaddr4 IP address(IPv4) type. This is represented as a FixedSizeBinary type with byteWidth=4 and custom metadata pg_type=pg_catalog.inet . Ipaddr6 IP address(IPv6) type. This is represented as a FixedSizeBinary type with byteWidth=16 and custom metadata pg_type=pg_catalog.inet . ts_column [type: String / default: unspecified] Specify a column name to set the timestamp value of the log passed from Fluentd (not from record ). This parameter is usually a date-time type such as Timestamp , and the stat_enabled attribute should also be specified to achieve fast search. tag_column [type: String / default: unspecified] Specify a column name to set the tag value of the log passed from Fluentd (not from record ). This parameter is usually a string type such as utf8 . filesize_threshold [type: Integer / default: 10000] Specify the threshold for switching the output destination file in MB. By default, the output destination is switched when the file size exceeds about 10GB.","title":"Configuration"},{"location":"fluentd/#example","text":"As a simple example, this chapter shows a configuration for monitoring the log of a local Apache Httpd server, parsing it field by field, and writing it to an Apache Arrow format file. By setting <source> , /var/log/httpd/access_log will be the data source. Then, the apache2 Parse plugin will cut out the fields: host, user, time, method, path, code, size, referer, agent. The fields are then passed to the arrow-file plugin as an associative array. In schema_defs in <match> , the column definitions of the Apache Arrow file corresponding to the fields are set. For simplicity of explanation, the chunk size is set to a maximum of 4MB / 200 lines in the <buffer> tag, and it is set to pass to the Output plugin in 10 seconds at most. Example configuration of /etc/fluent/fluentd.conf <source> @type tail path /var/log/httpd/access_log pos_file /var/log/fluent/httpd_access.pos tag httpd format apache2 <parse> @type apache2 expression /^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \\[(?<time>[^\\]]*)\\] \"(?<method>\\S+)(?: +(?<path>(?:[^\\\"]|\\\\.)*?)(?: +\\S*)?)?\" (?<code>[^ ]*) (?<size>[^ ]*)(?: \"(?<referer>(?:[^\\\"]|\\\\.)*)\" \"(?<agent>(?:[^\\\"]|\\\\.)*)\")?$/ time_format %d/%b/%Y:%H:%M:%S %z </parse> </source> <match httpd> @type arrow_file path /tmp/mytest%Y%m%d.%p.arrow schema_defs \"ts=Timestamp[sec],host=Utf8,method=Utf8,path=Utf8,code=Int32,size=Int32,referer=Utf8,agent=Utf8\" ts_column \"ts\" <buffer> flush_interval 10s chunk_limit_size 4MB chunk_limit_records 200 </buffer> </match> Start the fluentd service. $ sudo systemctl start fluentd See the following output. The placeholder for /tmp/mytest%Y%m%d.%p.arrow set in path is replaced and the Apache Httpd log is written to /tmp/mytest20220124.3206341.arrow . $ arrow2csv /tmp/mytest20220124.3206341.arrow --head --offset 300 --limit 10 \"ts\",\"host\",\"method\",\"path\",\"code\",\"size\",\"referer\",\"agent\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/js/theme_extra.js\",200,195,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/js/theme.js\",200,4401,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/img/fluentd_overview.png\",200,121459,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/search/main.js\",200,3027,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/Lato/lato-regular.woff2\",200,182708,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/fontawesome-webfont.woff2?v=4.7.0\",200,77160,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/RobotoSlab/roboto-slab-v7-bold.woff2\",200,67312,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:42\",\"192.168.77.95\",\"GET\",\"/docs/ja/fonts/Lato/lato-bold.woff2\",200,184912,\"http://buri/docs/ja/css/theme.css\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:43\",\"192.168.77.95\",\"GET\",\"/docs/ja/search/worker.js\",200,3724,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" \"2022-01-24 06:13:43\",\"192.168.77.95\",\"GET\",\"/docs/ja/img/favicon.ico\",200,1150,\"http://buri/docs/ja/fluentd/\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\" Let's map the output file to PostgreSQL using PG-Strom's Arrow_Fdw. postgres=# IMPORT FOREIGN SCHEMA mytest FROM SERVER arrow_fdw INTO public OPTIONS (file '/tmp/mytest20220124.3206341.arrow'); IMPORT FOREIGN SCHEMA postgres=# SELECT ts, host, path FROM mytest WHERE code = 404; ts | host | path ---------------------+---------------+---------------------- 2022-01-24 12:02:06 | 192.168.77.73 | /~kaigai/ja/fluentd/ (1 row) postgres=# EXPLAIN SELECT ts, host, path FROM mytest WHERE code = 404; QUERY PLAN ------------------------------------------------------------------------------ Custom Scan (GpuScan) on mytest (cost=4026.12..4026.12 rows=3 width=72) GPU Filter: (code = 404) referenced: ts, host, path, code files0: /tmp/mytest20220124.3206341.arrow (read: 128.00KB, size: 133.94KB) (4 rows) The example above shows how the generated Apache Arrow file can be mapped as an external table and accessed with SQL. Search conditions can be given to search each field of the log formed on the Fluentd side. In the example above, the log with HTTP status code 404 is searched and one record is found.","title":"Example"},{"location":"gpucache/","text":"GPU Cache Overview GPU has a device memory that is independent of the RAM in the host system, and in order to calculate on the GPU, data must be transferred from the host system or storage device to the GPU device memory once through the PCI-E bus. The same is true when PG-Strom processes SQL queries on the GPU. Internally, the records read from the PostgreSQL table are transferred to the GPU, and then various SQL operations are executed on the GPU. However, these processes take time to read the tables and transfer the data. (In many cases, much longer than the processing on the GPU!) GPU Cache is a function that reserves an area on the GPU device memory in advance and keeps a copy of the PostgreSQL table there. This can be used to execute search/analysis SQL in real time for data that is relatively small(~10GB) and is frequently updated. The log-based synchronization mechanism described below allows GPU Cache to be kept up-to-date without interfering with highly parallel and transactional workloads. Nevertheless, you can process search/analytical SQL workloads on data already loaded on GPU Cache without reading the records from the table again or transferring the data over the PCI-E bus. A typical use case of GPU Cache is to join location data, such as the current position of a mobile device like a car or a cell phone, collected in real time with other data using GPU-PostGIS . The workload of updating location information sent out by many devices is extremely heavy. However, it also needs to be applied on the GPU side without delay in order to perform search/analysis queries based on the latest location information. Although the size is limited, GPU Cache is one option to achieve both high frequency updates and high-performance search/analysis query execution. Architecture GPU Caches needs to satisfy two requirements: highly parallel update-based workloads and search/analytical queries on constantly up-to-date data. In many systems, the CPU and GPU are connected via the PCI-E bus, and there is a reasonable delay in their communication. Therefore, synchronizing GPU Cache every time a row is updated in the target table will significantly degrade the transaction performance. Using GPU Cache allocates a \"REDO Log Buffer\" on the shared memory on the host side in addition to the area on the memory of the GPU. When a SQL command (INSERT, UPDATE, DELETE) is executed to update a table, the updated contents are copied to the REDO Log Buffer by the AFTER ROW trigger. Since this process can be completed by CPU and RAM alone without any GPU call, it has little impact on transaction performance. When a certain amount of unapplied REDO Log Entries accumulate in the REDO Log Buffer, or a certain amount of time has passed since the last write, it is loaded by a background worker process (GPU memory keeper) and applied to GPU Cache. At this time, REDO Log Entries are transferred to the GPU in batches and processed in parallel by thousands of processor cores on the GPU, so delays caused by this process are rarely a problem. Search/analysis queries against the target table in GPU Cache do not load the table data, but use the data mapped from GPU Cache pre-allocated on the GPU device memory. If there are any unapplied REDO Logs at the start of the search/analysis query, they will all be applied to GPU Cache. This means that the results of a search/analysis query scanning the target GPU Cache will return the same results as if it were referring to the table directly, and the query will always be consistent. Configuration To enable GPU Cache, configure a trigger that executes pgstrom.gpucache_sync_trigger() function on AFTER INSERT OR UPDATE OR DELETE for each row. If GPU Cache is used on the replication slave, the invocation mode of this trigger must be ALWAYS . Below is an example to configure GPU Cache on the dpoints table. =# create trigger row_sync after insert or update or delete on dpoints_even for row execute function pgstrom.gpucache_sync_trigger(); =# alter table dpoints_even enable always trigger row_sync; GPU Cache Customize You can customize GPU Cache by specifying an optional string in the form of KEY=VALUE as an argument to GPU Cache line trigger. Please note that where you should specify is not to the syntax trigger. The following SQL statement is an example of creating a GPU Cache whose maximum row count is 2.5 million rows and the size of the REDO Log Buffer is 100MB. =# create trigger row_sync after insert or update or delete on dpoints_even for row execute function pgstrom.gpucache_sync_trigger('max_num_rows=2500000,redo_buffer_size=100m'); The options that can be given to the argument of the line trigger are shown below. gpu_device_id=GPU_ID (default: 0) Specify the target GPU device ID to allocate GPU Cache. max_num_rows=NROWS (default: 10485760) Specify the number of rows that can be allocated on GPU Cache. Just as with PostgreSQL tables, GPU Cache needs to retain updated rows prior to commit for visibility control, so max_num_rows should be specified with some margin. Note that the old version of the updated/deleted row will be released after the transaction is committed. redo_buffer_size=SIZE (default: 160m) Specify the size of REDO Log Buffer. You can use k, m and g as the unit. gpu_sync_interval=SECONDS (default: 5) If the specified time has passed since the last write to the REDO Log Buffer, REDO Log will be applied to the GPU, regardless of the number of rows updated. gpu_sync_threshold=SIZE (default: 25% of redo_buffer_size ) When the unapplied REDO Log in the REDO Log Buffer reaches SIZE bytes, it is applied to the GPU side. You can use k, m and g as the unit. GPU Cache Options Below are GPU Cache related PostgreSQL configuration parameters. pg_strom.enable_gpucache (default: on) This option controls whether search/analytical queries will use GPU Cache or not. If this value is off, the data will be read from the table each time, ignoring GPU Cache even if it is available. Note that this setting has no effect on REDO Log Buffer appending by triggers. pg_strom.gpucache_auto_preload (default: NULL) When PostgreSQL is started/restarted, GPU Cache for the table specified by this parameter will be built in advance. The value should be in the format: DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . To specify multiple tables, separate them by commas. If GPU Cache is not built, the PostgreSQL backend process that first attempts to access GPU Cache will scan the entire target table and transfer it to the GPU. This process usually takes a considerable amount of time. However, by specifying the tables that should be loaded in this option, you can avoid waiting too long the first time you run a search/analysis query. If this parameter is set to '*', it will attempt to load the contents of all tables with GPU Cache into the GPU in order. At this time, the background worker will access all the databases in order, and will return exit code 1 to prompt the postmaster to restart. The server startup log will show that the \"GPUCache Startup Preloader\" exited with exit code 1 as follows, but this is not abnormal. LOG: database system is ready to accept connections LOG: background worker \"GPUCache Startup Preloader\" (PID 856418) exited with exit code 1 LOG: background worker \"GPUCache Startup Preloader\" (PID 856427) exited with exit code 1 LOG: create GpuCacheSharedState dpoints:164c95f71 LOG: gpucache: AllocMemory dpoints:164c95f71 (main_sz=772505600, extra_sz=0) LOG: gpucache: auto preload 'public.dpoints' (DB: postgres) LOG: create GpuCacheSharedState mytest:1773a589b LOG: gpucache: auto preload 'public.mytest' (DB: postgres) LOG: gpucache: AllocMemory mytest:1773a589b (main_sz=675028992, extra_sz=0) Operations Confirm GPU Cache usage GPU Cache is referred to transparently. The user does not need to be aware of the presence or absence of GPU Cache, and PG-Strom will automatically determine and switch the process. The following is the query plan for a query that refers to the table \"dpoints\" which has GPU Cache set. The 3rd row from the bottom, in the \"GPU Cache\" field, shows the basic information about GPU Cache of this table. We can see that the query is executed with referring to GPU Cache and not the \"dpoints\" table. Note that the meaning of each item is as follows: max_num_rows indicates the maximum number of rows that GPU Cache can hold; main indicates the size of the area in GPU Cache for fixed-length fields; extra indicates the size of the area for variable-length data. =# explain select pref, city, count(*) from giscity g, dpoints d where pref = 'Tokyo' and st_contains(g.geom,st_makepoint(d.x, d.y)) group by pref, city; QUERY PLAN -------------------------------------------------------------------------------------------------------- HashAggregate (cost=5638809.75..5638859.99 rows=5024 width=29) Group Key: g.pref, g.city -> Custom Scan (GpuPreAgg) (cost=5638696.71..5638759.51 rows=5024 width=29) Reduction: Local Combined GpuJoin: enabled GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on dpoints d (cost=631923.57..5606933.23 rows=50821573 width=21) Outer Scan: dpoints d (cost=0.00..141628.18 rows=7999618 width=16) Depth 1: GpuGiSTJoin(nrows 7999618...50821573) HeapSize: 3251.36KB IndexFilter: (g.geom ~ st_makepoint(d.x, d.y)) on giscity_geom_idx JoinQuals: st_contains(g.geom, st_makepoint(d.x, d.y)) GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) GPU Cache: NVIDIA Tesla V100-PCIE-16GB [max_num_rows: 12000000, main: 772.51M, extra: 0] -> Seq Scan on giscity g (cost=0.00..8929.24 rows=6353 width=1883) Filter: ((pref)::text = 'Tokyo'::text) (16 rows) Check status of GPU Cache Use the pgstrom.gpucache_info view to check the current state of GPU Cache. =# select * from pgstrom.gpucache_info ; database_oid | database_name | table_oid | table_name | signature | refcnt | corrupted | gpu_main_sz | gpu_extra_sz | redo_write_ts | redo_write_nitems | redo_write_pos | redo_read_nitems | redo_read_pos | redo_sync_pos | config_options --------------+---------------+-----------+------------+------------+--------+-----------+-------------+--------------+----------------------------+-------------------+----------------+------------------+---------------+---------------+------------------------------------------------------------------------------------------------------------------------ 12728 | postgres | 25244 | mytest | 6295279771 | 3 | f | 675028992 | 0 | 2021-05-14 03:00:18.623503 | 500000 | 36000000 | 500000 | 36000000 | 36000000 | gpu_device_id=0,max_num_rows=10485760,redo_buffer_size=167772160,gpu_sync_interval=5000000,gpu_sync_threshold=41943040 12728 | postgres | 25262 | dpoints | 5985886065 | 3 | f | 772505600 | 0 | 2021-05-14 03:00:18.524627 | 8000000 | 576000192 | 8000000 | 576000192 | 576000192 | gpu_device_id=0,max_num_rows=12000000,redo_buffer_size=167772160,gpu_sync_interval=5000000,gpu_sync_threshold=41943040 (2 rows) Note that pgstrom.gpucache_info will only show the status of GPU Caches that have been initially loaded and have space allocated on the GPU device memory at that time. In other words, if the trigger function is set but not yet initially loaded (no one has accessed it yet), the potentially allocated GPU Cache will not be shown yet. The meaning of each field is as follows: database_oid The OID of the database to which the table with GPU Cache enabled exists. database_name The name of the database to which the table with GPU Cache enabled exists. table_oid The OID of the table with GPU Cache enabled. Note that the database this table exists in is not necessarily the database you are connected to. table_name The name of the table with GPU Cache enabled. Note that the database this table exists in is not necessarily the database you are connected to. signature A hash value indicating the uniqueness of GPU Cache. This value may change, for example, before and after executing ALTER TABLE . refcnt Reference counter of the GPU Cache. It does not always reflect the latest value. corrupted Shows whether the GPU Cache is corrupted. gpu_main_sz The size of the area reserved in GPU Cache for fixed-length data. gpu_extra_sz The size of the area reserved in GPU Cache for variable-length data. redo_write_ts The time when the REDO Log Buffer was last updated. redo_write_nitems The total number of REDO Logs in the REDO Log Buffer. redo_write_pos The total size (in bytes) of the REDO Logs in the REDO Log Buffer. redo_read_nitems The total number of REDO Logs read from the REDO Log Buffer and applied to the GPU. redo_read_pos The total size (in bytes) of REDO Logs read from the REDO Log Buffer and applied to the GPU. redo_sync_pos The position of the REDO Log which is scheduled to be applied to GPU Cache by the background worker on the REDO Log Buffer. This is used internally to avoid a situation where many sessions generate asynchronous requests at the same time when the remaining REDO Log Buffer is running out. config_options The optional string to customize GPU Cache. GPU Cache corruption and recovery If and when REDO logs could not be applied on the GPU cache by some reasons, like insertion of more rows than the max_num_rows configuration, or too much consumption of variable-length data buffer, GPU cache moves to the \"corrupted\" state. Once GPU cache gets corrupted, search/analysis SQL does not reference the GPU cache, and table updates stops writing REDO log. (If GPU cache gets corrupted after beginning of a search/analysis SQL unfortunately, this query may raise an error.) The pgstrom.gpucache_recovery(regclass) function recovers the GPU cache from the corrupted state. If you run this function after removal of the cause where REDO logs could not be applied, it runs initial-loading of the GPU cache again, then tries to recover the GPU cache. For example, if GPU cache gets corrupted because you tried to insert more rows than the max_num_rows , you reconfigure the trigger with expanded max_num_rows configuration or you delete a part of rows from the table, then runs pgstrom.gpucache_recovery() function.","title":"GPU Cache"},{"location":"gpucache/#gpu-cache","text":"","title":"GPU Cache"},{"location":"gpucache/#overview","text":"GPU has a device memory that is independent of the RAM in the host system, and in order to calculate on the GPU, data must be transferred from the host system or storage device to the GPU device memory once through the PCI-E bus. The same is true when PG-Strom processes SQL queries on the GPU. Internally, the records read from the PostgreSQL table are transferred to the GPU, and then various SQL operations are executed on the GPU. However, these processes take time to read the tables and transfer the data. (In many cases, much longer than the processing on the GPU!) GPU Cache is a function that reserves an area on the GPU device memory in advance and keeps a copy of the PostgreSQL table there. This can be used to execute search/analysis SQL in real time for data that is relatively small(~10GB) and is frequently updated. The log-based synchronization mechanism described below allows GPU Cache to be kept up-to-date without interfering with highly parallel and transactional workloads. Nevertheless, you can process search/analytical SQL workloads on data already loaded on GPU Cache without reading the records from the table again or transferring the data over the PCI-E bus. A typical use case of GPU Cache is to join location data, such as the current position of a mobile device like a car or a cell phone, collected in real time with other data using GPU-PostGIS . The workload of updating location information sent out by many devices is extremely heavy. However, it also needs to be applied on the GPU side without delay in order to perform search/analysis queries based on the latest location information. Although the size is limited, GPU Cache is one option to achieve both high frequency updates and high-performance search/analysis query execution.","title":"Overview"},{"location":"gpucache/#architecture","text":"GPU Caches needs to satisfy two requirements: highly parallel update-based workloads and search/analytical queries on constantly up-to-date data. In many systems, the CPU and GPU are connected via the PCI-E bus, and there is a reasonable delay in their communication. Therefore, synchronizing GPU Cache every time a row is updated in the target table will significantly degrade the transaction performance. Using GPU Cache allocates a \"REDO Log Buffer\" on the shared memory on the host side in addition to the area on the memory of the GPU. When a SQL command (INSERT, UPDATE, DELETE) is executed to update a table, the updated contents are copied to the REDO Log Buffer by the AFTER ROW trigger. Since this process can be completed by CPU and RAM alone without any GPU call, it has little impact on transaction performance. When a certain amount of unapplied REDO Log Entries accumulate in the REDO Log Buffer, or a certain amount of time has passed since the last write, it is loaded by a background worker process (GPU memory keeper) and applied to GPU Cache. At this time, REDO Log Entries are transferred to the GPU in batches and processed in parallel by thousands of processor cores on the GPU, so delays caused by this process are rarely a problem. Search/analysis queries against the target table in GPU Cache do not load the table data, but use the data mapped from GPU Cache pre-allocated on the GPU device memory. If there are any unapplied REDO Logs at the start of the search/analysis query, they will all be applied to GPU Cache. This means that the results of a search/analysis query scanning the target GPU Cache will return the same results as if it were referring to the table directly, and the query will always be consistent.","title":"Architecture"},{"location":"gpucache/#configuration","text":"To enable GPU Cache, configure a trigger that executes pgstrom.gpucache_sync_trigger() function on AFTER INSERT OR UPDATE OR DELETE for each row. If GPU Cache is used on the replication slave, the invocation mode of this trigger must be ALWAYS . Below is an example to configure GPU Cache on the dpoints table. =# create trigger row_sync after insert or update or delete on dpoints_even for row execute function pgstrom.gpucache_sync_trigger(); =# alter table dpoints_even enable always trigger row_sync;","title":"Configuration"},{"location":"gpucache/#gpu-cache-customize","text":"You can customize GPU Cache by specifying an optional string in the form of KEY=VALUE as an argument to GPU Cache line trigger. Please note that where you should specify is not to the syntax trigger. The following SQL statement is an example of creating a GPU Cache whose maximum row count is 2.5 million rows and the size of the REDO Log Buffer is 100MB. =# create trigger row_sync after insert or update or delete on dpoints_even for row execute function pgstrom.gpucache_sync_trigger('max_num_rows=2500000,redo_buffer_size=100m'); The options that can be given to the argument of the line trigger are shown below. gpu_device_id=GPU_ID (default: 0) Specify the target GPU device ID to allocate GPU Cache. max_num_rows=NROWS (default: 10485760) Specify the number of rows that can be allocated on GPU Cache. Just as with PostgreSQL tables, GPU Cache needs to retain updated rows prior to commit for visibility control, so max_num_rows should be specified with some margin. Note that the old version of the updated/deleted row will be released after the transaction is committed. redo_buffer_size=SIZE (default: 160m) Specify the size of REDO Log Buffer. You can use k, m and g as the unit. gpu_sync_interval=SECONDS (default: 5) If the specified time has passed since the last write to the REDO Log Buffer, REDO Log will be applied to the GPU, regardless of the number of rows updated. gpu_sync_threshold=SIZE (default: 25% of redo_buffer_size ) When the unapplied REDO Log in the REDO Log Buffer reaches SIZE bytes, it is applied to the GPU side. You can use k, m and g as the unit.","title":"GPU Cache Customize"},{"location":"gpucache/#gpu-cache-options","text":"Below are GPU Cache related PostgreSQL configuration parameters. pg_strom.enable_gpucache (default: on) This option controls whether search/analytical queries will use GPU Cache or not. If this value is off, the data will be read from the table each time, ignoring GPU Cache even if it is available. Note that this setting has no effect on REDO Log Buffer appending by triggers. pg_strom.gpucache_auto_preload (default: NULL) When PostgreSQL is started/restarted, GPU Cache for the table specified by this parameter will be built in advance. The value should be in the format: DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . To specify multiple tables, separate them by commas. If GPU Cache is not built, the PostgreSQL backend process that first attempts to access GPU Cache will scan the entire target table and transfer it to the GPU. This process usually takes a considerable amount of time. However, by specifying the tables that should be loaded in this option, you can avoid waiting too long the first time you run a search/analysis query. If this parameter is set to '*', it will attempt to load the contents of all tables with GPU Cache into the GPU in order. At this time, the background worker will access all the databases in order, and will return exit code 1 to prompt the postmaster to restart. The server startup log will show that the \"GPUCache Startup Preloader\" exited with exit code 1 as follows, but this is not abnormal. LOG: database system is ready to accept connections LOG: background worker \"GPUCache Startup Preloader\" (PID 856418) exited with exit code 1 LOG: background worker \"GPUCache Startup Preloader\" (PID 856427) exited with exit code 1 LOG: create GpuCacheSharedState dpoints:164c95f71 LOG: gpucache: AllocMemory dpoints:164c95f71 (main_sz=772505600, extra_sz=0) LOG: gpucache: auto preload 'public.dpoints' (DB: postgres) LOG: create GpuCacheSharedState mytest:1773a589b LOG: gpucache: auto preload 'public.mytest' (DB: postgres) LOG: gpucache: AllocMemory mytest:1773a589b (main_sz=675028992, extra_sz=0)","title":"GPU Cache Options"},{"location":"gpucache/#operations","text":"","title":"Operations"},{"location":"gpucache/#confirm-gpu-cache-usage","text":"GPU Cache is referred to transparently. The user does not need to be aware of the presence or absence of GPU Cache, and PG-Strom will automatically determine and switch the process. The following is the query plan for a query that refers to the table \"dpoints\" which has GPU Cache set. The 3rd row from the bottom, in the \"GPU Cache\" field, shows the basic information about GPU Cache of this table. We can see that the query is executed with referring to GPU Cache and not the \"dpoints\" table. Note that the meaning of each item is as follows: max_num_rows indicates the maximum number of rows that GPU Cache can hold; main indicates the size of the area in GPU Cache for fixed-length fields; extra indicates the size of the area for variable-length data. =# explain select pref, city, count(*) from giscity g, dpoints d where pref = 'Tokyo' and st_contains(g.geom,st_makepoint(d.x, d.y)) group by pref, city; QUERY PLAN -------------------------------------------------------------------------------------------------------- HashAggregate (cost=5638809.75..5638859.99 rows=5024 width=29) Group Key: g.pref, g.city -> Custom Scan (GpuPreAgg) (cost=5638696.71..5638759.51 rows=5024 width=29) Reduction: Local Combined GpuJoin: enabled GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on dpoints d (cost=631923.57..5606933.23 rows=50821573 width=21) Outer Scan: dpoints d (cost=0.00..141628.18 rows=7999618 width=16) Depth 1: GpuGiSTJoin(nrows 7999618...50821573) HeapSize: 3251.36KB IndexFilter: (g.geom ~ st_makepoint(d.x, d.y)) on giscity_geom_idx JoinQuals: st_contains(g.geom, st_makepoint(d.x, d.y)) GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) GPU Cache: NVIDIA Tesla V100-PCIE-16GB [max_num_rows: 12000000, main: 772.51M, extra: 0] -> Seq Scan on giscity g (cost=0.00..8929.24 rows=6353 width=1883) Filter: ((pref)::text = 'Tokyo'::text) (16 rows)","title":"Confirm GPU Cache usage"},{"location":"gpucache/#check-status-of-gpu-cache","text":"Use the pgstrom.gpucache_info view to check the current state of GPU Cache. =# select * from pgstrom.gpucache_info ; database_oid | database_name | table_oid | table_name | signature | refcnt | corrupted | gpu_main_sz | gpu_extra_sz | redo_write_ts | redo_write_nitems | redo_write_pos | redo_read_nitems | redo_read_pos | redo_sync_pos | config_options --------------+---------------+-----------+------------+------------+--------+-----------+-------------+--------------+----------------------------+-------------------+----------------+------------------+---------------+---------------+------------------------------------------------------------------------------------------------------------------------ 12728 | postgres | 25244 | mytest | 6295279771 | 3 | f | 675028992 | 0 | 2021-05-14 03:00:18.623503 | 500000 | 36000000 | 500000 | 36000000 | 36000000 | gpu_device_id=0,max_num_rows=10485760,redo_buffer_size=167772160,gpu_sync_interval=5000000,gpu_sync_threshold=41943040 12728 | postgres | 25262 | dpoints | 5985886065 | 3 | f | 772505600 | 0 | 2021-05-14 03:00:18.524627 | 8000000 | 576000192 | 8000000 | 576000192 | 576000192 | gpu_device_id=0,max_num_rows=12000000,redo_buffer_size=167772160,gpu_sync_interval=5000000,gpu_sync_threshold=41943040 (2 rows) Note that pgstrom.gpucache_info will only show the status of GPU Caches that have been initially loaded and have space allocated on the GPU device memory at that time. In other words, if the trigger function is set but not yet initially loaded (no one has accessed it yet), the potentially allocated GPU Cache will not be shown yet. The meaning of each field is as follows: database_oid The OID of the database to which the table with GPU Cache enabled exists. database_name The name of the database to which the table with GPU Cache enabled exists. table_oid The OID of the table with GPU Cache enabled. Note that the database this table exists in is not necessarily the database you are connected to. table_name The name of the table with GPU Cache enabled. Note that the database this table exists in is not necessarily the database you are connected to. signature A hash value indicating the uniqueness of GPU Cache. This value may change, for example, before and after executing ALTER TABLE . refcnt Reference counter of the GPU Cache. It does not always reflect the latest value. corrupted Shows whether the GPU Cache is corrupted. gpu_main_sz The size of the area reserved in GPU Cache for fixed-length data. gpu_extra_sz The size of the area reserved in GPU Cache for variable-length data. redo_write_ts The time when the REDO Log Buffer was last updated. redo_write_nitems The total number of REDO Logs in the REDO Log Buffer. redo_write_pos The total size (in bytes) of the REDO Logs in the REDO Log Buffer. redo_read_nitems The total number of REDO Logs read from the REDO Log Buffer and applied to the GPU. redo_read_pos The total size (in bytes) of REDO Logs read from the REDO Log Buffer and applied to the GPU. redo_sync_pos The position of the REDO Log which is scheduled to be applied to GPU Cache by the background worker on the REDO Log Buffer. This is used internally to avoid a situation where many sessions generate asynchronous requests at the same time when the remaining REDO Log Buffer is running out. config_options The optional string to customize GPU Cache.","title":"Check status of GPU Cache"},{"location":"gpucache/#gpu-cache-corruption-and-recovery","text":"If and when REDO logs could not be applied on the GPU cache by some reasons, like insertion of more rows than the max_num_rows configuration, or too much consumption of variable-length data buffer, GPU cache moves to the \"corrupted\" state. Once GPU cache gets corrupted, search/analysis SQL does not reference the GPU cache, and table updates stops writing REDO log. (If GPU cache gets corrupted after beginning of a search/analysis SQL unfortunately, this query may raise an error.) The pgstrom.gpucache_recovery(regclass) function recovers the GPU cache from the corrupted state. If you run this function after removal of the cause where REDO logs could not be applied, it runs initial-loading of the GPU cache again, then tries to recover the GPU cache. For example, if GPU cache gets corrupted because you tried to insert more rows than the max_num_rows , you reconfigure the trigger with expanded max_num_rows configuration or you delete a part of rows from the table, then runs pgstrom.gpucache_recovery() function.","title":"GPU Cache corruption and recovery"},{"location":"gpusort/","text":"GPU-Sort This chapter introduces the GPU-Sort and related features. GPU Task Execution To explain the conditions for using GPU-Sort, we first need to know how PG-Strom processes SQL workloads. When GPU-Scan and GPU-Join read a table to be processed, they divide the data into chunks of about 64MB and read it from storage to GPU, evaluate WHERE clauses and JOIN conditions on GPU, and write the processing results back to CPU. At this time, the GPU memory used for Scan/Join processing is released and reused to process the next 64MB chunk. Normally, data reading from storage, Scan/Join processing on GPU, and writing back to CPU are executed concurrently, but GPU memory is still released and reused one after another, so PG-Strom does not use much GPU memory. On the other hand, GPU-PreAgg is different. Due to the nature of the workload, the results of the aggregation process are stored on the GPU (Dam execution), and then written back to the CPU in one go. Therefore, when GPU-PreAgg is completed, almost all of the data is in the GPU memory. Due to the nature of the sorting process, all of the data to be sorted must be loaded onto the GPU in order to execute GPU-Sort. In other words, in the case of GPU-Sort, which does not involve aggregation operations and has only GPU-Scan and GPU-Join as its lower nodes, the execution results must be stored in the GPU memory as in GPU-PreAgg. This is the same execution method used in Inner Pinned Buffer in GpuJoin , and when the results of GPU-Scan/Join are used in the next step, they are kept in the GPU memory instead of being returned to the CPU. Enables GPU-Sort Due to the nature of sorting workloads, all the target data must be stored in GPU memory. Therefore, it is a basic premise that the data to be sorted must fix in the GPU memory capacity, but there is one more thing to consider. Various GPU processes in PG-Strom have a mechanism called CPU-Fallback, which re-executes the processing of operators and SQL functions on the CPU if they cannot be completed on the GPU. Typically, this is the case when variable-length data does not fit into the PostgreSQL block size (8kB) and is stored in an external TOAST table. CPU-Fallback is a function to ensure continuity of processing even for extremely exceptional data, but rows that are subject to CPU-Fallback are stored in the CPU and do not exist in GPU memory, which causes an obstacle when sorting. Therefore, the GPU-Sort function works only when the CPU-Fallback function is disabled, i.e., when pg_strom.cpu_fallback=off is set. If CPU-Fallback is disabled, the complete results of GPU-Scan/Join/PreAgg are guaranteed to be in GPU memory, so PG-Strom can perform parallel sorting based on the Bitonic-Sorting algorithm and return the sorted results to the CPU. When used with a window function that limits the number of rows, such as the LIMIT clause or rank() < 4 , it will reduce the number of data returned to the CPU based on these optimization hints. This should contribute to speedup by reducing the number of data to be processed by the CPU. The following execution plan shows the result of narrowing down the number of rows using a window function ( rank() < 4 ) without enabling GPU-Sort. =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=32013352.01..33893039.51 rows=75187500 width=84) (actual time=13158.987..13335.106 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Sort (cost=32013352.01..32201320.76 rows=75187500 width=76) (actual time=13158.976..13238.136 rows=601500 loops=1) Sort Key: customer.c_region, customer.c_nation, customer.c_city, (pgstrom.sum_numeric((pgstrom.psum(lineorder.lo_revenue)))) Sort Method: quicksort Memory: 76268kB -> HashAggregate (cost=15987574.35..18836475.71 rows=75187500 width=76) (actual time=9990.801..10271.543 rows=601500 loops=1) Group Key: customer.c_region, customer.c_nation, customer.c_city, lineorder.lo_orderdate Planned Partitions: 8 Batches: 1 Memory Usage: 516113kB -> Custom Scan (GpuPreAgg) on lineorder (cost=4967906.38..5907750.13 rows=75187500 width=76) (actual time=9175.476..9352.529 rows=1203000 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.008..519.064 rows=3000000 loops=1) Planning Time: 1.395 ms Execution Time: 13494.808 ms (19 rows) After GPU-PreAgg, HashAggregate is run to aggregate the partial aggregation results, and Sort is run to sort the aggregated values. Finally, WindowAgg is run to narrow down the results to the top three sum(lo_revenue) for each c_region , c_nation , and c_city . The processing time for GPU-PreAgg is 9.352 seconds, so we can see that roughly 4 seconds of the latter half of the process was spent on the CPU. On the other hand, the following execution plan disables CPU-Fallback by setting pg_strom.cpu_fallback=off (i.e. enables GPU-Sort). =# set pg_strom.cpu_fallback = off; SET =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=5595978.47..5602228.47 rows=125000 width=84) (actual time=9596.930..9598.194 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Result (cost=5595978.47..5599415.97 rows=125000 width=76) (actual time=9596.918..9597.292 rows=750 loops=1) -> Custom Scan (GpuPreAgg) on lineorder (cost=5595978.47..5597540.97 rows=125000 width=76) (actual time=9596.912..9597.061 rows=750 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 GPU-Sort keys: c_region, c_nation, c_city, pgstrom.fsum_numeric((pgstrom.psum(lo_revenue))) Window-Rank Filter: rank() over(PARTITION BY c_region, c_nation, c_city ORDER BY pgstrom.fsum_numeric((pgstrom.psum(lo_revenue)))) < 4 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.006..475.006 rows=3000000 loops=1) Planning Time: 0.381 ms Execution Time: 9710.616 ms (16 rows) The HashAggregate and Sort that were in the original query plan have gone, and instead, the lines GPU-Sort keys and Window-Rank Filter have appeared as options for GpuPreAgg . This indicates that GpuPreAgg creates the complete aggregation on the GPU, then sorts and outputs it. In addition, in this query, filtering is performed using the window function rank() . By pushing down this condition to the lower node GpuPreAgg , rows that are known to be filtered out in advance are removed from the result set, reducing the amount of data transferred from the GPU to the CPU and the number of rows that the CPU needs to copy. These processes are processed in parallel on the GPU memory, so they are generally faster than CPU processing. Row reductions by GPU-Sort There are several conditions that must be met for GPU-Sort to be triggered. As explained in the previous section, the first prerequisite is that CPU-Fallback is disabled. The optimizer will insert GPU-Sort in the following cases: - Upper nodes require sorted results, such as in an ORDER BY clause. - Window functions are used, and upper nodes require sorted results. For these workloads, PG-Strom will generate GPU-PreAgg/Join/Scan execution plans with GPU-Sort. In addition, in the following cases, by completing the sorting process on the GPU, it is possible to secondarily reduce the number of rows to be written back to CPU from the GPU. - When a LIMIT clause is added to ORDER BY, and the maximum number of rows that should be output is known in advance. - When the window function rank() or the like is used, the top number of rows that should be output from each partition is known in advance. The following execution plan joins several tables, performs aggregation operations, sorts the results by d_year and revenue , and outputs the top 10. =# set pg_strom.cpu_fallback = off; SET =# explain select c_nation, s_nation, d_year, sum(lo_revenue) as revenue from customer, lineorder, supplier, date1 where lo_custkey = c_custkey and lo_suppkey = s_suppkey and lo_orderdate = d_datekey and c_region = 'ASIA' and s_region = 'ASIA' and d_year >= 1992 and d_year <= 1997 group by c_nation, s_nation, d_year order by d_year, revenue limit 10; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------- Gather (cost=19266896.24..19266897.46 rows=10 width=68) (actual time=44024.638..44056.484 rows=10 loops=1) Workers Planned: 2 Workers Launched: 2 -> Result (cost=19265896.24..19265896.46 rows=10 width=68) (actual time=44016.179..44016.185 rows=3 loops=3) -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=19265896.24..19265896.31 rows=10 width=68) (actual time=44016.177..44016.181 rows=3 loops=3) GPU Projection: pgstrom.psum(lo_revenue), c_nation, s_nation, d_year GPU Join Quals [1]: (s_suppkey = lo_suppkey) [plan: 2500011000 -> 494752100, exec: 0 -> 0 GPU Outer Hash [1]: lo_suppkey GPU Inner Hash [1]: s_suppkey GPU Join Quals [2]: (c_custkey = lo_custkey) [plan: 494752100 -> 97977410, exec: 0 -> 0 GPU Outer Hash [2]: lo_custkey GPU Inner Hash [2]: c_custkey GPU Join Quals [3]: (d_datekey = lo_orderdate) [plan: 97977410 -> 84024450, exec: 0 -> 0 GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: c_nation, s_nation, d_year Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=114826068, ntuples=0 GPU-Sort keys: d_year, pgstrom.fsum_numeric((pgstrom.psum(lo_revenue))) GPU-Sort Limit: 10 -> Parallel Custom Scan (GpuScan) on supplier (cost=100.00..78805.87 rows=824560 width=22) (actual time=16.090..104.527 rows=666540 loops=3) GPU Projection: s_nation, s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 9999718 -> 824560, exec: 10000000 -> 1999620] Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=168663, ntuples=10000000 -> Parallel Custom Scan (GpuScan) on customer (cost=100.00..79517.76 rows=2475728 width=22) (actual time=14.848..226.354 rows=2000770 loops=3) GPU Projection: c_nation, c_custkey GPU Scan Quals: (c_region = 'ASIA'::bpchar) [plan: 30003780 -> 2475728, exec: 30000000 -> 6002311] Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=519628, ntuples=30000000 -> Parallel Seq Scan on date1 (cost=0.00..69.55 rows=1289 width=8) (actual time=0.009..0.132 rows=731 loops=3) Filter: ((d_year >= 1992) AND (d_year <= 1997)) Rows Removed by Filter: 121 Planning Time: 0.786 ms Execution Time: 44093.114 ms (32 rows) Without GPU-Sort, GPU-PreAgg will generate 4375 partial aggregation results, integrates them with HashAggregate, sorts them with Sort, and outputs only the top 10 with Limit. Depending on the scale of the problem, a strategy may be adopted in which partial sort results are generated for each PostgreSQL parallel worker and merge-sorted. In any case, sorting processing, which places a heavy load on the CPU, becomes a troublesome problem as the number of items increases. However, if it is known in advance that only the top 10 sorted results will be output, there is no need to return 4365 pieces of data to the CPU in the first place. This query execution plan does not include CPU Sort or HashAggregate. Instead, as a post-processing step of GPU-PreAgg, the results are sorted based on the key values \u200b\u200bdisplayed in the GPU-Sort keys line, and only the 10 rows displayed in the GPU-Sort Limit line are returned. The execution plan is to immediately return the GPU processing results to the client without executing Sort or Limit on the CPU side. In addition to the LIMIT clause, window functions can be used as hints to reduce the number of rows. Currently, the following conditional expressions can be used as hints: rank() OVER(...) < CONST rank() OVER(...) <= CONST dense_rank() OVER(...) < CONST dense_rank() OVER(...) <= CONST row_number() OVER(...) < CONST row_number() OVER(...) <= CONST If you want to disable CPU-Fallback but not GPU-Sort, you can enable/disable only GPU-Sort by pg_strom.enable_gpusort = [on|off] .","title":"GPU-Sort"},{"location":"gpusort/#gpu-sort","text":"This chapter introduces the GPU-Sort and related features.","title":"GPU-Sort"},{"location":"gpusort/#gpu-task-execution","text":"To explain the conditions for using GPU-Sort, we first need to know how PG-Strom processes SQL workloads. When GPU-Scan and GPU-Join read a table to be processed, they divide the data into chunks of about 64MB and read it from storage to GPU, evaluate WHERE clauses and JOIN conditions on GPU, and write the processing results back to CPU. At this time, the GPU memory used for Scan/Join processing is released and reused to process the next 64MB chunk. Normally, data reading from storage, Scan/Join processing on GPU, and writing back to CPU are executed concurrently, but GPU memory is still released and reused one after another, so PG-Strom does not use much GPU memory. On the other hand, GPU-PreAgg is different. Due to the nature of the workload, the results of the aggregation process are stored on the GPU (Dam execution), and then written back to the CPU in one go. Therefore, when GPU-PreAgg is completed, almost all of the data is in the GPU memory. Due to the nature of the sorting process, all of the data to be sorted must be loaded onto the GPU in order to execute GPU-Sort. In other words, in the case of GPU-Sort, which does not involve aggregation operations and has only GPU-Scan and GPU-Join as its lower nodes, the execution results must be stored in the GPU memory as in GPU-PreAgg. This is the same execution method used in Inner Pinned Buffer in GpuJoin , and when the results of GPU-Scan/Join are used in the next step, they are kept in the GPU memory instead of being returned to the CPU.","title":"GPU Task Execution"},{"location":"gpusort/#enables-gpu-sort","text":"Due to the nature of sorting workloads, all the target data must be stored in GPU memory. Therefore, it is a basic premise that the data to be sorted must fix in the GPU memory capacity, but there is one more thing to consider. Various GPU processes in PG-Strom have a mechanism called CPU-Fallback, which re-executes the processing of operators and SQL functions on the CPU if they cannot be completed on the GPU. Typically, this is the case when variable-length data does not fit into the PostgreSQL block size (8kB) and is stored in an external TOAST table. CPU-Fallback is a function to ensure continuity of processing even for extremely exceptional data, but rows that are subject to CPU-Fallback are stored in the CPU and do not exist in GPU memory, which causes an obstacle when sorting. Therefore, the GPU-Sort function works only when the CPU-Fallback function is disabled, i.e., when pg_strom.cpu_fallback=off is set. If CPU-Fallback is disabled, the complete results of GPU-Scan/Join/PreAgg are guaranteed to be in GPU memory, so PG-Strom can perform parallel sorting based on the Bitonic-Sorting algorithm and return the sorted results to the CPU. When used with a window function that limits the number of rows, such as the LIMIT clause or rank() < 4 , it will reduce the number of data returned to the CPU based on these optimization hints. This should contribute to speedup by reducing the number of data to be processed by the CPU. The following execution plan shows the result of narrowing down the number of rows using a window function ( rank() < 4 ) without enabling GPU-Sort. =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=32013352.01..33893039.51 rows=75187500 width=84) (actual time=13158.987..13335.106 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Sort (cost=32013352.01..32201320.76 rows=75187500 width=76) (actual time=13158.976..13238.136 rows=601500 loops=1) Sort Key: customer.c_region, customer.c_nation, customer.c_city, (pgstrom.sum_numeric((pgstrom.psum(lineorder.lo_revenue)))) Sort Method: quicksort Memory: 76268kB -> HashAggregate (cost=15987574.35..18836475.71 rows=75187500 width=76) (actual time=9990.801..10271.543 rows=601500 loops=1) Group Key: customer.c_region, customer.c_nation, customer.c_city, lineorder.lo_orderdate Planned Partitions: 8 Batches: 1 Memory Usage: 516113kB -> Custom Scan (GpuPreAgg) on lineorder (cost=4967906.38..5907750.13 rows=75187500 width=76) (actual time=9175.476..9352.529 rows=1203000 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.008..519.064 rows=3000000 loops=1) Planning Time: 1.395 ms Execution Time: 13494.808 ms (19 rows) After GPU-PreAgg, HashAggregate is run to aggregate the partial aggregation results, and Sort is run to sort the aggregated values. Finally, WindowAgg is run to narrow down the results to the top three sum(lo_revenue) for each c_region , c_nation , and c_city . The processing time for GPU-PreAgg is 9.352 seconds, so we can see that roughly 4 seconds of the latter half of the process was spent on the CPU. On the other hand, the following execution plan disables CPU-Fallback by setting pg_strom.cpu_fallback=off (i.e. enables GPU-Sort). =# set pg_strom.cpu_fallback = off; SET =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=5595978.47..5602228.47 rows=125000 width=84) (actual time=9596.930..9598.194 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Result (cost=5595978.47..5599415.97 rows=125000 width=76) (actual time=9596.918..9597.292 rows=750 loops=1) -> Custom Scan (GpuPreAgg) on lineorder (cost=5595978.47..5597540.97 rows=125000 width=76) (actual time=9596.912..9597.061 rows=750 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 GPU-Sort keys: c_region, c_nation, c_city, pgstrom.fsum_numeric((pgstrom.psum(lo_revenue))) Window-Rank Filter: rank() over(PARTITION BY c_region, c_nation, c_city ORDER BY pgstrom.fsum_numeric((pgstrom.psum(lo_revenue)))) < 4 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.006..475.006 rows=3000000 loops=1) Planning Time: 0.381 ms Execution Time: 9710.616 ms (16 rows) The HashAggregate and Sort that were in the original query plan have gone, and instead, the lines GPU-Sort keys and Window-Rank Filter have appeared as options for GpuPreAgg . This indicates that GpuPreAgg creates the complete aggregation on the GPU, then sorts and outputs it. In addition, in this query, filtering is performed using the window function rank() . By pushing down this condition to the lower node GpuPreAgg , rows that are known to be filtered out in advance are removed from the result set, reducing the amount of data transferred from the GPU to the CPU and the number of rows that the CPU needs to copy. These processes are processed in parallel on the GPU memory, so they are generally faster than CPU processing.","title":"Enables GPU-Sort"},{"location":"gpusort/#row-reductions-by-gpu-sort","text":"There are several conditions that must be met for GPU-Sort to be triggered. As explained in the previous section, the first prerequisite is that CPU-Fallback is disabled. The optimizer will insert GPU-Sort in the following cases: - Upper nodes require sorted results, such as in an ORDER BY clause. - Window functions are used, and upper nodes require sorted results. For these workloads, PG-Strom will generate GPU-PreAgg/Join/Scan execution plans with GPU-Sort. In addition, in the following cases, by completing the sorting process on the GPU, it is possible to secondarily reduce the number of rows to be written back to CPU from the GPU. - When a LIMIT clause is added to ORDER BY, and the maximum number of rows that should be output is known in advance. - When the window function rank() or the like is used, the top number of rows that should be output from each partition is known in advance. The following execution plan joins several tables, performs aggregation operations, sorts the results by d_year and revenue , and outputs the top 10. =# set pg_strom.cpu_fallback = off; SET =# explain select c_nation, s_nation, d_year, sum(lo_revenue) as revenue from customer, lineorder, supplier, date1 where lo_custkey = c_custkey and lo_suppkey = s_suppkey and lo_orderdate = d_datekey and c_region = 'ASIA' and s_region = 'ASIA' and d_year >= 1992 and d_year <= 1997 group by c_nation, s_nation, d_year order by d_year, revenue limit 10; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------- Gather (cost=19266896.24..19266897.46 rows=10 width=68) (actual time=44024.638..44056.484 rows=10 loops=1) Workers Planned: 2 Workers Launched: 2 -> Result (cost=19265896.24..19265896.46 rows=10 width=68) (actual time=44016.179..44016.185 rows=3 loops=3) -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=19265896.24..19265896.31 rows=10 width=68) (actual time=44016.177..44016.181 rows=3 loops=3) GPU Projection: pgstrom.psum(lo_revenue), c_nation, s_nation, d_year GPU Join Quals [1]: (s_suppkey = lo_suppkey) [plan: 2500011000 -> 494752100, exec: 0 -> 0 GPU Outer Hash [1]: lo_suppkey GPU Inner Hash [1]: s_suppkey GPU Join Quals [2]: (c_custkey = lo_custkey) [plan: 494752100 -> 97977410, exec: 0 -> 0 GPU Outer Hash [2]: lo_custkey GPU Inner Hash [2]: c_custkey GPU Join Quals [3]: (d_datekey = lo_orderdate) [plan: 97977410 -> 84024450, exec: 0 -> 0 GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: c_nation, s_nation, d_year Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=114826068, ntuples=0 GPU-Sort keys: d_year, pgstrom.fsum_numeric((pgstrom.psum(lo_revenue))) GPU-Sort Limit: 10 -> Parallel Custom Scan (GpuScan) on supplier (cost=100.00..78805.87 rows=824560 width=22) (actual time=16.090..104.527 rows=666540 loops=3) GPU Projection: s_nation, s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 9999718 -> 824560, exec: 10000000 -> 1999620] Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=168663, ntuples=10000000 -> Parallel Custom Scan (GpuScan) on customer (cost=100.00..79517.76 rows=2475728 width=22) (actual time=14.848..226.354 rows=2000770 loops=3) GPU Projection: c_nation, c_custkey GPU Scan Quals: (c_region = 'ASIA'::bpchar) [plan: 30003780 -> 2475728, exec: 30000000 -> 6002311] Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=519628, ntuples=30000000 -> Parallel Seq Scan on date1 (cost=0.00..69.55 rows=1289 width=8) (actual time=0.009..0.132 rows=731 loops=3) Filter: ((d_year >= 1992) AND (d_year <= 1997)) Rows Removed by Filter: 121 Planning Time: 0.786 ms Execution Time: 44093.114 ms (32 rows) Without GPU-Sort, GPU-PreAgg will generate 4375 partial aggregation results, integrates them with HashAggregate, sorts them with Sort, and outputs only the top 10 with Limit. Depending on the scale of the problem, a strategy may be adopted in which partial sort results are generated for each PostgreSQL parallel worker and merge-sorted. In any case, sorting processing, which places a heavy load on the CPU, becomes a troublesome problem as the number of items increases. However, if it is known in advance that only the top 10 sorted results will be output, there is no need to return 4365 pieces of data to the CPU in the first place. This query execution plan does not include CPU Sort or HashAggregate. Instead, as a post-processing step of GPU-PreAgg, the results are sorted based on the key values \u200b\u200bdisplayed in the GPU-Sort keys line, and only the 10 rows displayed in the GPU-Sort Limit line are returned. The execution plan is to immediately return the GPU processing results to the client without executing Sort or Limit on the CPU side. In addition to the LIMIT clause, window functions can be used as hints to reduce the number of rows. Currently, the following conditional expressions can be used as hints: rank() OVER(...) < CONST rank() OVER(...) <= CONST dense_rank() OVER(...) < CONST dense_rank() OVER(...) <= CONST row_number() OVER(...) < CONST row_number() OVER(...) <= CONST If you want to disable CPU-Fallback but not GPU-Sort, you can enable/disable only GPU-Sort by pg_strom.enable_gpusort = [on|off] .","title":"Row reductions by GPU-Sort"},{"location":"install/","text":"Installation This chapter introduces the steps to install PG-Strom. Checklist Server Hardware It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices. note002:HW Validation List may help you to choose the hardware. GPU Direct SQL Execution needs NVME-SSD devices, or fast network card with RoCE support, and to be installed under the same PCIe Root Complex where GPU is located on. GPU Device PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later; Please check at 002: HW Validation List - List of supported GPU models for GPU selection. Operating System PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or Rocky Linux version 9.x or 8.x series. GPU Direct SQL (with cuFile driver) needs the nvidia-fs driver distributed with CUDA Toolkit, and Mellanox OFED (OpenFabrics Enterprise Distribution) driver. PostgreSQL PG-Strom v5.0 requires PostgreSQL v15 or later. Some of PostgreSQL APIs used by PG-Strom internally are not included in the former versions. CUDA Toolkit PG-Strom requires CUDA Toolkit version 12.2update1 or later. Some of CUDA Driver APIs used by PG-Strom internally are not included in the former versions. Steps to Install The overall steps to install are below: Hardware Configuration OS Installation MOFED Driver installation CUDA Toolkit installation HeteroDB Extra Module installation PostgreSQL installation PG-Strom installation PostgreSQL Extensions installation PostGIS contrib/cube OS Installation Choose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution. NVIDIA DEVELOPER ZONE introduces the list of Linux distributions which are supported by CUDA Toolkit. In case of Red Hat Enterprise Linux 8.x series (including Rocky Linux 8.x series), choose \"Minimal installation\" as base environment, and also check the \"Development Tools\" add-ons for the software selection Next to the OS installation on the server, go on the package repository configuration to install the third-party packages. If you didn't check the \"Development Tools\" at the installer, we can additionally install the software using the command below after the operating system installation. # dnf groupinstall 'Development Tools' Tip If GPU devices installed on the server are too new, it may cause system crash during system boot. In this case, you may avoid the problem by adding nouveau.modeset=0 onto the kernel boot option, to disable the inbox graphic driver. Disables nouveau driver When the nouveau driver, that is an open source compatible driver for NVIDIA GPUs, is loaded, it prevent to load the nvidia driver. In this case, reboot the operating system after a configuration to disable the nouveau driver. To disable the nouveau driver, put the following configuration onto /etc/modprobe.d/disable-nouveau.conf , and run dracut command to apply them on the boot image of Linux kernel. Then, restart the system once. # cat > /etc/modprobe.d/disable-nouveau.conf <<EOF blacklist nouveau options nouveau modeset=0 EOF # dracut -f # shutdown -r now Disables IOMMU GPU-Direct SQL uses GPUDirect Storage (cuFile) API of CUDA. Prior to using GPUDirect Storage, it needs to disable the IOMMU configuration on the OS side. Configure the kernel boot option according to the NVIDIA GPUDirect Storage Installation and Troubleshooting Guide description. To disable IOMMU, add amd_iommu=off (for AMD CPU) or intel_iommu=off (for Intel CPU) to the kernel boot options. Configuration at RHEL9 The command below adds the kernel boot option. # grubby --update-kernel=ALL --args=\"amd_iommu=off\" Configuration at RHEL8 Open /etc/default/grub with an editor and add the above option to the GRUB_CMDLINE_LINUX_DEFAULT= line. For example, the settings should look like this: : GRUB_CMDLINE_LINUX=\"rhgb quiet amd_iommu=off\" : Run the following commands to apply the configuration to the kernel bool options. -- for BIOS based system # grub2-mkconfig -o /boot/grub2/grub.cfg # shutdown -r now -- for UEFI based system # grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg # shutdown -r now Enables extra repositories EPEL(Extra Packages for Enterprise Linux) Several software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux). You need to add a repository definition of EPEL packages for yum system to obtain these software. One of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver and related. Linux kernel module must be rebuilt according to version-up of Linux kernel, so we don't recommend to operate the system without DKMS. epel-release package provides the repository definition of EPEL. You can obtain the package from the Fedora Project website. -- For RHEL9 # dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm -- For RHEL8 # dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm -- For Rocky8/Rocky9 # dnf install epel-release Red Hat CodeReady Linux Builder Installation of MOFED (Mellanox OpenFabrics Enterprise Distribution) driver requires the Red Hat CodeReady Linux Builder repository which is disabled in the default configuration of Red Hat Enterprise Linux 8.x installation. In Rocky Linux, it is called PowerTools To enable this repository, run the command below: -- For RHEL9 # subscription-manager repos --enable codeready-builder-for-rhel-9-x86_64-rpms -- For Rocky9 # dnf config-manager --set-enabled crb -- For RHEL8 # subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms -- For Rocky8 # dnf config-manager --set-enabled powertools MOFED Driver Installation You can download the latest MOFED driver from here . This section introduces the example of installation from the tgz archive of MOFED driver version 23.10. The installer software of MOFED driver requires the createrepo and perl packages at least. After that, extract the tgz archive, then kick mlnxofedinstall script. Please don't forget the options to enable GPUDirect Storage features. # dnf install -y perl createrepo # tar zxvf MLNX_OFED_LINUX-23.10-2.1.3.1-rhel9.3-x86_64.tgz # cd MLNX_OFED_LINUX-23.10-2.1.3.1-rhel9.3-x86_64 # ./mlnxofedinstall --with-nvmf --with-nfsrdma --add-kernel-support # dracut -f During the build and installation of MOFED drivers, the installer may require additional packages. In this case, error message shall guide you the missing packages. So, please install them using dnf command. Error: One or more required packages for installing OFED-internal are missing. Please install the missing packages using your Linux distribution Package Management tool. Run: yum install kernel-rpm-macros Failed to build MLNX_OFED_LINUX for 5.14.0-362.8.1.el9_3.x86_64 Once MOFED drivers got installed, it should replace several INBOX drivers like nvme driver. For example, the command below shows the /lib/modules/<KERNEL_VERSION>/extra/mlnx-nvme/host/nvme-rdma.ko that is additionally installed, instead of the INBOX nvme-rdma ( /lib/modules/<KERNEL_VERSION>/kernel/drivers/nvme/host/nvme-rdma.ko.xz ). $ modinfo nvme-rdma filename: /lib/modules/5.14.0-427.18.1.el9_4.x86_64/extra/mlnx-nvme/host/nvme-rdma.ko license: GPL v2 rhelversion: 9.4 srcversion: 16C0049F26768D6EA12771B depends: nvme-core,rdma_cm,ib_core,nvme-fabrics,mlx_compat retpoline: Y name: nvme_rdma vermagic: 5.14.0-427.18.1.el9_4.x86_64 SMP preempt mod_unload modversions parm: register_always:Use memory registration even for contiguous memory regions (bool) Then, shutdown the system and restart, to replace the kernel modules already loaded (like nvme ). Please don't forget to run dracut -f after completion of the mlnxofedinstall script. Tips Linux kernel version up and MOFED driver MODED drivers do not use DKMS (Dynamic Kernel Module Support) in RHEL series distributions. Therefore, when the Linux kernel is upgraded, you will need to perform the above steps again to reinstall the MOFED driver that is compatible with the new Linux kernel. The Linux kernel may be updated together when another package is updated, such as when installing the CUDA Toolkit described below, but the same applies in that case. heterodb-swdc Installation PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . You need to add a repository definition of HeteroDB-SWDC for you system to obtain these software. heterodb-swdc package provides the repository definition of HeteroDB-SWDC. Access to the HeteroDB Software Distribution Center using Web browser, download the heterodb-swdc-1.3-1.el9.noarch.rpm on top of the file list, then install this package. (Use heterodb-swdc-1.3-1.el8.noarch.rpm for RHEL8) Once heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository. Install the heterodb-swdc package as follows. # dnf install https://heterodb.github.io/swdc/yum/rhel8-noarch/heterodb-swdc-1.2-1.el8.noarch.rpm CUDA Toolkit Installation This section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can check whether your installation is identical with the configuration described in this section. NVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (runfile), and the other is by RPM packages. We recommend the RPM installation for PG-Strom setup. You can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose \"rpm(network)\" edition. Once you choose the \"rpm(network)\" option, it shows a few step-by-step shell commands to register the CUDA repository and install the packages. Run the installation according to the guidance. # dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo # dnf clean all # dnf install cuda-toolkit-12-5 Next to the installation of the CUDA Toolkit, two types of commands are introduced to install the nvidia driver. Please use the open source version of nvidia-driver here. Only the open source version supports the GPUDirect Storage feature, and PG-Strom's GPU-Direct SQL utilizes this feature. Tips Use of Volta or former GPUs The open source edition of the nvidia driver does not support Volta generation GPUs or former. Therefore, if you want to use PG-Strom with Volta or Pascal generation GPUs, you need to use CUDA 12.2 Update 1, whose proprietary driver supports GPUDirect Storage. The CUDA 12.2 Update 1 package can be obtained here . Next, install the driver module nvidia-gds for the GPU-Direct Storage (GDS). Please specify the same version name as the CUDA Toolkit version after the package name. # dnf module install nvidia-driver:open-dkms # dnf install nvidia-gds-12-5 Once installation completed successfully, CUDA Toolkit is deployed at /usr/local/cuda . $ ls /usr/local/cuda/ bin/ gds/ nsightee_plugins/ targets/ compute-sanitizer/ include@ nvml/ tools/ CUDA_Toolkit_Release_Notes.txt lib64@ nvvm/ version.json DOCS libnvvp/ README EULA.txt LICENSE share/ extras/ man/ src/ Once installation gets completed, ensure the system recognizes the GPU devices correctly. nvidia-smi command shows GPU information installed on your system, as follows. $ nvidia-smi Mon Jun 3 09:56:41 2024 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA A100-PCIE-40GB Off | 00000000:41:00.0 Off | 0 | | N/A 58C P0 66W / 250W | 1MiB / 40960MiB | 0% Default | | | | Disabled | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ Tips Additional configurations for systems with NVSwitch For systems with multiple GPUs that use NVSwitch for interconnect them, the nvidia-fabricmanager module must be installed. If this package is not installed, cuInit() , which initializes CUDA, will fail with the CUDA_ERROR_SYSTEM_NOT_READY error, and PG-Strom will not start. Run the following commands to install the nvidia-fabricmanager package. (source) # dnf install nvidia-fabricmanager # systemctl enable nvidia-fabricmanager.service # systemctl start nvidia-fabricmanager.service Check GPUDirect Storage status After the installation of CUDA Toolkit according to the steps above, your system will become ready for the GPUDirect Storage. Run gdscheck tool to confirm the configuration for each storage devices, as follows. (Thie example loads not only nvme , but nvme-rdma and rpcrdma kernel modules also, therefore, it reports the related features as Supported ) # /usr/local/cuda/gds/tools/gdscheck -p GDS release version: 1.10.0.4 nvidia_fs version: 2.20 libcufile version: 2.12 Platform: x86_64 ============ ENVIRONMENT: ============ ===================== DRIVER CONFIGURATION: ===================== NVMe : Supported NVMeOF : Supported SCSI : Unsupported ScaleFlux CSD : Unsupported NVMesh : Unsupported DDN EXAScaler : Unsupported IBM Spectrum Scale : Unsupported NFS : Supported BeeGFS : Unsupported WekaFS : Unsupported Userspace RDMA : Unsupported --Mellanox PeerDirect : Disabled --rdma library : Not Loaded (libcufile_rdma.so) --rdma devices : Not configured --rdma_device_status : Up: 0 Down: 0 ===================== CUFILE CONFIGURATION: ===================== properties.use_compat_mode : true properties.force_compat_mode : false properties.gds_rdma_write_support : true properties.use_poll_mode : false properties.poll_mode_max_size_kb : 4 properties.max_batch_io_size : 128 properties.max_batch_io_timeout_msecs : 5 properties.max_direct_io_size_kb : 16384 properties.max_device_cache_size_kb : 131072 properties.max_device_pinned_mem_size_kb : 33554432 properties.posix_pool_slab_size_kb : 4 1024 16384 properties.posix_pool_slab_count : 128 64 32 properties.rdma_peer_affinity_policy : RoundRobin properties.rdma_dynamic_routing : 0 fs.generic.posix_unaligned_writes : false fs.lustre.posix_gds_min_kb: 0 fs.beegfs.posix_gds_min_kb: 0 fs.weka.rdma_write_support: false fs.gpfs.gds_write_support: false profile.nvtx : false profile.cufile_stats : 0 miscellaneous.api_check_aggressive : false execution.max_io_threads : 4 execution.max_io_queue_depth : 128 execution.parallel_io : true execution.min_io_threshold_size_kb : 8192 execution.max_request_parallelism : 4 properties.force_odirect_mode : false properties.prefer_iouring : false ========= GPU INFO: ========= GPU index 0 NVIDIA A100-PCIE-40GB bar:1 bar size (MiB):65536 supports GDS, IOMMU State: Disabled ============== PLATFORM INFO: ============== IOMMU: disabled Nvidia Driver Info Status: Supported(Nvidia Open Driver Installed) Cuda Driver Version Installed: 12050 Platform: AS -2014CS-TR, Arch: x86_64(Linux 5.14.0-427.18.1.el9_4.x86_64) Platform verification succeeded Tips Additional configuration for RAID volume For data reading from software RAID (md-raid0) volumes by GPUDirect Storage, the following line must be added to the /lib/udev/rules.d/63-md-raid-arrays.rules configuration file. IMPORT{\u200bprogram}=\"/usr/sbin/mdadm --detail --export $devnode\" Then reboot the system to ensure the new configuration. See NVIDIA GPUDirect Storage Installation and Troubleshooting Guide for the details. PCI Bar1 Memory Configuration GPU-Direct SQL maps GPU device memory to the PCI BAR1 region (physical address space) on the host system, and sends P2P-RDMA requests to NVME devices with that as the destination for the shortest data transfer. To perform P2P-RDMA with sufficient multiplicity, the GPU must have enough PCI BAR1 space to map the device buffer. The size of the PCI BAR1 area is fixed for most GPUs, and PG-Strom recommends products whose size exceeds the GPU device memory size. However, some GPU products allow to change the size of the PCI BAR1 area by switching the operation mode. If your GPU is either of the following, refer to the NVIDIA Display Mode Selector Tool and switch to the mode that maximizes the PCI BAR1 area size. NVIDIA L40S NVIDIA L40 NVIDIA A40 NVIDIA RTX 6000 Ada NVIDIA RTX A6000 NVIDIA RTX A5500 NVIDIA RTX A5000 To check the GPU memory size and PCI BAR1 size installed in the system, use the nvidia-smi -q command. Memory-related status is displayed as shown below. $ nvidia-smi -q : FB Memory Usage Total : 46068 MiB Reserved : 685 MiB Used : 4 MiB Free : 45377 MiB BAR1 Memory Usage Total : 65536 MiB Used : 1 MiB Free : 65535 MiB : HeteroDB extra modules heterodb-extra module enhances PG-Strom the following features. multi-GPUs support GPUDirect SQL GiST index support on GPU License management If you don't use the above features, only open source modules, you don't need to install the heterodb-extra module here. Please skip this section. Install the heterodb-extra package, downloaded from the SWDC, as follows. # dnf install heterodb-extra License activation License activation is needed to use all the features of heterodb-extra , provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted. Multiple GPUs support Striping of NVME-SSD drives (md-raid0) on GPUDirect SQL Support of NVME-oF device on GPUDirect SQL Support of GiST index on GPU-version of PostGIS workloads You can obtain a license file, like as a plain text below, from HeteroDB,Inc. IAgIVdKxhe+BSer3Y67jQW0+uTzYh00K6WOSH7xQ26Qcw8aeUNYqJB9YcKJTJb+QQhjmUeQpUnboNxVwLCd3HFuLXeBWMKp11/BgG0FSrkUWu/ZCtDtw0F1hEIUY7m767zAGV8y+i7BuNXGJFvRlAkxdVO3/K47ocIgoVkuzBfLvN/h9LffOydUnHPzrFHfLc0r3nNNgtyTrfvoZiXegkGM9GBTAKyq8uWu/OGonh9ybzVKOgofhDLk0rVbLohOXDhMlwDl2oMGIr83tIpCWG+BGE+TDwsJ4n71Sv6n4bi/ZBXBS498qShNHDGrbz6cNcDVBa+EuZc6HzZoF6UrljEcl= ---- VERSION:2 SERIAL_NR:HDB-TRIAL ISSUED_AT:2019-05-09 EXPIRED_AT:2019-06-08 GPU_UUID:GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f Copy the license file to /etc/heterodb.license , then restart PostgreSQL. The startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done. : LOG: HeteroDB Extra module loaded [api_version=20231105,cufile=on,nvme_strom=off,githash=9ca2fe4d2fbb795ad2d741dcfcb9f2fe499a5bdf] LOG: HeteroDB License: { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"2022-11-19\", \"expired_at\" : \"2099-12-31\", \"nr_gpus\" : 1, \"gpus\" : [ { \"uuid\" : \"GPU-13943bfd-5b30-38f5-0473-78979c134606\" } ]} LOG: PG-Strom version 5.0.1 built for PostgreSQL 15 (githash: 972441dbafed6679af86af40bc8613be2d73c4fd) : PostgreSQL Installation This section introduces PostgreSQL installation with RPM. We don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the ./configure script. PostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community. PostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages. Like the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software. Here is the list of yum repository definition: http://yum.postgresql.org/repopackages.php . Repository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v15 or later. You can install PostgreSQL as following steps: Installation of yum repository definition. Disables the distribution's default PostgreSQL module Installation of PostgreSQL packages. # dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm # dnf -y module disable postgresql # dnf install -y postgresql16-devel postgresql16-server Note On the Red Hat Enterprise Linux, the package name postgresql conflicts to the default one at the distribution, thus, unable to install the packages from PGDG. So, disable the postgresql module by the distribution, using dnf -y module disable postgresql . libarrow/libparquet Installation PG-Strom v6.1 and later requires libarrow and libparquet for building and installation. Since the packages provided by the Linux distribution may be outdated, please follow the developer community guidance and install the arrow-devel and parquet-devel packages. The steps required for a minimal installation, excluding any overlaps with the installation steps above, are as follows. $ sudo dnf install -y https://packages.apache.org/artifactory/arrow/almalinux/$(cut -d: -f5 /etc/system-release-cpe | cut -d. -f1)/apache-arrow-release-latest.rpm $ sudo dnf install -y arrow-devel $ sudo dnf install -y parquet-devel PG-Strom Installation RPM Installation PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . If you repository definition has been added, not many tasks are needed. We provide individual RPM packages of PG-Strom for each PostgreSQL major version. pg_strom-PG15 package is built for PostgreSQL v15, and pg_strom-PG16 is also built for PostgreSQL v16. It is a restriction due to binary compatibility of extension modules for PostgreSQL. # dnf install -y pg_strom-PG16 That's all for package installation. Installation from the source For developers, we also introduces the steps to build and install PG-Strom from the source code. Getting the source code Like RPM packages, you can download tarball of the source code from HeteroDB Software Distribution Center . On the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of PG-Strom on GitHub to use the latest development branch. $ git clone https://github.com/heterodb/pg-strom.git Cloning into 'pg-strom'... remote: Counting objects: 13797, done. remote: Compressing objects: 100% (215/215), done. remote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400 Receiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done. Resolving deltas: 100% (10504/10504), done. Building the PG-Strom Configuration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular strcut has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out. Thus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using pg_config command. If PATH environment variable is set to the pg_config command of the target PostgreSQL, run make and make install . Elsewhere, give PG_CONFIG=... parameter on make command to tell the full path of the pg_config command. $ cd pg-strom/src $ make PG_CONFIG=/usr/pgsql-16/bin/pg_config $ sudo make install PG_CONFIG=/usr/pgsql-16/bin/pg_config Post Installation Setup Creation of database cluster Database cluster is not constructed yet, run initdb command to set up initial database of PostgreSQL. The default path of the database cluster on RPM installation is /var/lib/pgsql/<version number>/data . If you install postgresql-alternatives package, this default path can be referenced by /var/lib/pgdata regardless of the PostgreSQL version. # su - postgres $ /usr/pgsql-16/bin/initdb -D /var/lib/pgdata/ The files belonging to this database system will be owned by user \"postgres\". This user must also own the server process. The database cluster will be initialized with locale \"en_US.UTF-8\". The default database encoding has accordingly been set to \"UTF8\". The default text search configuration will be set to \"english\". Data page checksums are disabled. fixing permissions on existing directory /var/lib/pgdata ... ok creating subdirectories ... ok selecting dynamic shared memory implementation ... posix selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting default time zone ... Asia/Tokyo creating configuration files ... ok running bootstrap script ... ok performing post-bootstrap initialization ... ok syncing data to disk ... ok initdb: warning: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /var/lib/pgdata/ -l logfile start Setup postgresql.conf Next, edit postgresql.conf which is a configuration file of PostgreSQL. The parameters below should be edited at least to work PG-Strom. Investigate other parameters according to usage of the system and expected workloads. shared_preload_libraries PG-Strom module must be loaded on startup of the postmaster process by the shared_preload_libraries . Unable to load it on demand. Therefore, you must add the configuration below. shared_preload_libraries = '$libdir/pg_strom' max_worker_processes PG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin. max_worker_processes = 100 shared_buffers Although it depends on the workloads, the initial configuration of shared_buffers is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently. So, we recommend to expand the variable for a certain margin. shared_buffers = 10GB Please consider to apply SSD-to-GPU Direct SQL Execution to process larger than system's physical RAM size. work_mem Although it depends on the workloads, the initial configuration of work_mem is too small to choose the optimal query execution plan on analytic queries. An typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting. So, we recommend to expand the variable for a certain margin. work_mem = 1GB Expand OS resource limits GPU Direct SQL especially tries to open many files simultaneously, so resource limit for number of file descriptors per process should be expanded. Also, we recommend not to limit core file size to generate core dump of PostgreSQL certainly on system crash. If PostgreSQL service is launched by systemd, you can put the configurations of resource limit at /etc/systemd/system/postgresql-XX.service.d/pg_strom.conf . RPM installation setups the configuration below by the default. It comments out configuration to the environment variable CUDA_ENABLE_COREDUMP_ON_EXCEPTION . This is a developer option that enables to generate GPU's core dump on any CUDA/GPU level errors, if enabled. See CUDA-GDB:GPU core dump support for more details. [Service] LimitNOFILE=65536 LimitCORE=infinity #Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 Start PostgreSQL Start PostgreSQL service. If PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices. The example below recognized two NVIDIA A100 (PCIE; 40GB), and displays the closest GPU identifier foe each NVME-SSD drive. # systemctl start postgresql-16 # journalctl -u postgresql-16 Jun 02 17:28:45 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:45.989 JST [20242] LOG: HeteroDB Extra module loaded [api_version=20240418,cufile=off,nvme_strom=off,githash=3ffc65428c07bb3c9d0e5c75a2973389f91dfcd4] Jun 02 17:28:45 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:45.989 JST [20242] LOG: HeteroDB License: { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"2024-06-02\", \"expired_at\" : \"2099-12-31\", \"nr_gpus\" : 1, \"gpus\" : [ { \"uuid\" : \"GPU-13943bfd-5b30-38f5-0473-78979c134606\" } ]} Jun 02 17:28:45 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:45.989 JST [20242] LOG: PG-Strom version 5.12.el9 built for PostgreSQL 16 (githash: ) Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.114 JST [20242] LOG: PG-Strom binary built for CUDA 12.4 (CUDA runtime 12.5) Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.114 JST [20242] WARNING: The CUDA version where this PG-Strom module binary was built for (12.4) is newer than the CUDA runtime version on this platform (12.5). It may lead unexpected behavior, and upgrade of CUDA toolkit is recommended. Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.114 JST [20242] LOG: PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.50GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:41:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78979c134606) Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:81:00:0] nvme6 (NGD-IN2500-080T4-C) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:82:00:0] nvme3 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c2:00:0] nvme1 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c6:00:0] nvme4 (Corsair MP600 CORE) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c3:00:0] nvme5 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c4:00:0] nvme2 (NGD-IN2500-080T4-C) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.217 JST [20242] LOG: redirecting log output to logging collector process Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.217 JST [20242] HINT: Future log output will appear in directory \"log\". Jun 02 17:28:48 buri.heterodb.com systemd[1]: Started PostgreSQL 16 database server. Creation of PG-Strom Extension At the last, create database objects related to PG-Strom, like SQL functions. This steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is CREATE EXTENSION on the SQL command line. Please note that this step is needed for each new database. If you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the template1 database, its configuration will be copied to the new database on CREATE DATABASE command. $ psql -U postgres psql (16.3) Type \"help\" for help. postgres=# CREATE EXTENSION pg_strom ; CREATE EXTENSION That's all for the installation. PostGIS Installation PG-Strom supports execution of a part of PostGIS functions on GPU devices. This section introduces the steps to install PostGIS module. Skip it on your demand. PostGIS module can be installed from the yum repository by PostgreSQL Global Development Group, like PostgreSQL itself. The example below shows the command to install PostGIS v3.4 built for PostgreSQL v16. # dnf install postgis34_16 Start PostgreSQL server after the initial setup of database cluster, then run CREATE EXTENSION command from SQL client to define geometry data type and SQL functions for geoanalytics. postgres=# CREATE EXTENSION postgis; CREATE EXTENSION Installation on Ubuntu Linux Although PG-Strom packages are not available for Ubuntu Linux right now, you can build and run PG-Strom from the source code. After the installation of Ubuntu Linux, install the MOFED driver, CUDA Toolkit, and PostgreSQL for Ubuntu Linux, respectively. Next, install the heterodb-extra package. A .deb package for Ubuntu Linux is provided, so please obtain the latest version from the SWDC . $ wget https://heterodb.github.io/swdc/deb/heterodb-extra_5.4-1_amd64.deb $ sudo dpkg -i heterodb-extra_5.4-1_amd64.deb Checkout the source code of PG-Strom, build and install as follows. At this time, do not forget to specify the target PostgreSQL by pg_config . Post-installation configuration is almost same as for Red Hat Enterprise Linux or Rocky Linux. $ git clone https://github.com/heterodb/pg-strom.git $ cd pg-strom/src $ make PG_CONFIG=/path/to/pgsql/bin/pg_config -j 8 $ sudo make PG_CONFIG=/path/to/pgsql/bin/pg_config install However, if you use a packaged PostgreSQL and start it by systemctl command, the PATH environment variable will be cleared (probably for security reasons). As a result, the script launched to build the GPU binary on the first startup will not work properly. To avoid this, if you are using Ubuntu Linux, add the following line to /etc/postgresql/main/PGVERSION/environment . /etc/postgresql/PGVERSION/main/environment: PATH='/usr/local/cuda/bin:/usr/bin:/bin'","title":"Install"},{"location":"install/#installation","text":"This chapter introduces the steps to install PG-Strom.","title":"Installation"},{"location":"install/#checklist","text":"Server Hardware It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices. note002:HW Validation List may help you to choose the hardware. GPU Direct SQL Execution needs NVME-SSD devices, or fast network card with RoCE support, and to be installed under the same PCIe Root Complex where GPU is located on. GPU Device PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 6.0 (Pascal generation) or later; Please check at 002: HW Validation List - List of supported GPU models for GPU selection. Operating System PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or Rocky Linux version 9.x or 8.x series. GPU Direct SQL (with cuFile driver) needs the nvidia-fs driver distributed with CUDA Toolkit, and Mellanox OFED (OpenFabrics Enterprise Distribution) driver. PostgreSQL PG-Strom v5.0 requires PostgreSQL v15 or later. Some of PostgreSQL APIs used by PG-Strom internally are not included in the former versions. CUDA Toolkit PG-Strom requires CUDA Toolkit version 12.2update1 or later. Some of CUDA Driver APIs used by PG-Strom internally are not included in the former versions.","title":"Checklist"},{"location":"install/#steps-to-install","text":"The overall steps to install are below: Hardware Configuration OS Installation MOFED Driver installation CUDA Toolkit installation HeteroDB Extra Module installation PostgreSQL installation PG-Strom installation PostgreSQL Extensions installation PostGIS contrib/cube","title":"Steps to Install"},{"location":"install/#os-installation","text":"Choose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution. NVIDIA DEVELOPER ZONE introduces the list of Linux distributions which are supported by CUDA Toolkit. In case of Red Hat Enterprise Linux 8.x series (including Rocky Linux 8.x series), choose \"Minimal installation\" as base environment, and also check the \"Development Tools\" add-ons for the software selection Next to the OS installation on the server, go on the package repository configuration to install the third-party packages. If you didn't check the \"Development Tools\" at the installer, we can additionally install the software using the command below after the operating system installation. # dnf groupinstall 'Development Tools' Tip If GPU devices installed on the server are too new, it may cause system crash during system boot. In this case, you may avoid the problem by adding nouveau.modeset=0 onto the kernel boot option, to disable the inbox graphic driver.","title":"OS Installation"},{"location":"install/#disables-nouveau-driver","text":"When the nouveau driver, that is an open source compatible driver for NVIDIA GPUs, is loaded, it prevent to load the nvidia driver. In this case, reboot the operating system after a configuration to disable the nouveau driver. To disable the nouveau driver, put the following configuration onto /etc/modprobe.d/disable-nouveau.conf , and run dracut command to apply them on the boot image of Linux kernel. Then, restart the system once. # cat > /etc/modprobe.d/disable-nouveau.conf <<EOF blacklist nouveau options nouveau modeset=0 EOF # dracut -f # shutdown -r now","title":"Disables nouveau driver"},{"location":"install/#disables-iommu","text":"GPU-Direct SQL uses GPUDirect Storage (cuFile) API of CUDA. Prior to using GPUDirect Storage, it needs to disable the IOMMU configuration on the OS side. Configure the kernel boot option according to the NVIDIA GPUDirect Storage Installation and Troubleshooting Guide description. To disable IOMMU, add amd_iommu=off (for AMD CPU) or intel_iommu=off (for Intel CPU) to the kernel boot options.","title":"Disables IOMMU"},{"location":"install/#configuration-at-rhel9","text":"The command below adds the kernel boot option. # grubby --update-kernel=ALL --args=\"amd_iommu=off\"","title":"Configuration at RHEL9"},{"location":"install/#configuration-at-rhel8","text":"Open /etc/default/grub with an editor and add the above option to the GRUB_CMDLINE_LINUX_DEFAULT= line. For example, the settings should look like this: : GRUB_CMDLINE_LINUX=\"rhgb quiet amd_iommu=off\" : Run the following commands to apply the configuration to the kernel bool options. -- for BIOS based system # grub2-mkconfig -o /boot/grub2/grub.cfg # shutdown -r now -- for UEFI based system # grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg # shutdown -r now","title":"Configuration at RHEL8"},{"location":"install/#enables-extra-repositories","text":"","title":"Enables extra repositories"},{"location":"install/#epelextra-packages-for-enterprise-linux","text":"Several software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux). You need to add a repository definition of EPEL packages for yum system to obtain these software. One of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver and related. Linux kernel module must be rebuilt according to version-up of Linux kernel, so we don't recommend to operate the system without DKMS. epel-release package provides the repository definition of EPEL. You can obtain the package from the Fedora Project website. -- For RHEL9 # dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm -- For RHEL8 # dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm -- For Rocky8/Rocky9 # dnf install epel-release","title":"EPEL(Extra Packages for Enterprise Linux)"},{"location":"install/#red-hat-codeready-linux-builder","text":"Installation of MOFED (Mellanox OpenFabrics Enterprise Distribution) driver requires the Red Hat CodeReady Linux Builder repository which is disabled in the default configuration of Red Hat Enterprise Linux 8.x installation. In Rocky Linux, it is called PowerTools To enable this repository, run the command below: -- For RHEL9 # subscription-manager repos --enable codeready-builder-for-rhel-9-x86_64-rpms -- For Rocky9 # dnf config-manager --set-enabled crb -- For RHEL8 # subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms -- For Rocky8 # dnf config-manager --set-enabled powertools","title":"Red Hat CodeReady Linux Builder"},{"location":"install/#mofed-driver-installation","text":"You can download the latest MOFED driver from here . This section introduces the example of installation from the tgz archive of MOFED driver version 23.10. The installer software of MOFED driver requires the createrepo and perl packages at least. After that, extract the tgz archive, then kick mlnxofedinstall script. Please don't forget the options to enable GPUDirect Storage features. # dnf install -y perl createrepo # tar zxvf MLNX_OFED_LINUX-23.10-2.1.3.1-rhel9.3-x86_64.tgz # cd MLNX_OFED_LINUX-23.10-2.1.3.1-rhel9.3-x86_64 # ./mlnxofedinstall --with-nvmf --with-nfsrdma --add-kernel-support # dracut -f During the build and installation of MOFED drivers, the installer may require additional packages. In this case, error message shall guide you the missing packages. So, please install them using dnf command. Error: One or more required packages for installing OFED-internal are missing. Please install the missing packages using your Linux distribution Package Management tool. Run: yum install kernel-rpm-macros Failed to build MLNX_OFED_LINUX for 5.14.0-362.8.1.el9_3.x86_64 Once MOFED drivers got installed, it should replace several INBOX drivers like nvme driver. For example, the command below shows the /lib/modules/<KERNEL_VERSION>/extra/mlnx-nvme/host/nvme-rdma.ko that is additionally installed, instead of the INBOX nvme-rdma ( /lib/modules/<KERNEL_VERSION>/kernel/drivers/nvme/host/nvme-rdma.ko.xz ). $ modinfo nvme-rdma filename: /lib/modules/5.14.0-427.18.1.el9_4.x86_64/extra/mlnx-nvme/host/nvme-rdma.ko license: GPL v2 rhelversion: 9.4 srcversion: 16C0049F26768D6EA12771B depends: nvme-core,rdma_cm,ib_core,nvme-fabrics,mlx_compat retpoline: Y name: nvme_rdma vermagic: 5.14.0-427.18.1.el9_4.x86_64 SMP preempt mod_unload modversions parm: register_always:Use memory registration even for contiguous memory regions (bool) Then, shutdown the system and restart, to replace the kernel modules already loaded (like nvme ). Please don't forget to run dracut -f after completion of the mlnxofedinstall script. Tips Linux kernel version up and MOFED driver MODED drivers do not use DKMS (Dynamic Kernel Module Support) in RHEL series distributions. Therefore, when the Linux kernel is upgraded, you will need to perform the above steps again to reinstall the MOFED driver that is compatible with the new Linux kernel. The Linux kernel may be updated together when another package is updated, such as when installing the CUDA Toolkit described below, but the same applies in that case.","title":"MOFED Driver Installation"},{"location":"install/#heterodb-swdc-installation","text":"PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . You need to add a repository definition of HeteroDB-SWDC for you system to obtain these software. heterodb-swdc package provides the repository definition of HeteroDB-SWDC. Access to the HeteroDB Software Distribution Center using Web browser, download the heterodb-swdc-1.3-1.el9.noarch.rpm on top of the file list, then install this package. (Use heterodb-swdc-1.3-1.el8.noarch.rpm for RHEL8) Once heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository. Install the heterodb-swdc package as follows. # dnf install https://heterodb.github.io/swdc/yum/rhel8-noarch/heterodb-swdc-1.2-1.el8.noarch.rpm","title":"heterodb-swdc Installation"},{"location":"install/#cuda-toolkit-installation","text":"This section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can check whether your installation is identical with the configuration described in this section. NVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (runfile), and the other is by RPM packages. We recommend the RPM installation for PG-Strom setup. You can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose \"rpm(network)\" edition. Once you choose the \"rpm(network)\" option, it shows a few step-by-step shell commands to register the CUDA repository and install the packages. Run the installation according to the guidance. # dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo # dnf clean all # dnf install cuda-toolkit-12-5 Next to the installation of the CUDA Toolkit, two types of commands are introduced to install the nvidia driver. Please use the open source version of nvidia-driver here. Only the open source version supports the GPUDirect Storage feature, and PG-Strom's GPU-Direct SQL utilizes this feature. Tips Use of Volta or former GPUs The open source edition of the nvidia driver does not support Volta generation GPUs or former. Therefore, if you want to use PG-Strom with Volta or Pascal generation GPUs, you need to use CUDA 12.2 Update 1, whose proprietary driver supports GPUDirect Storage. The CUDA 12.2 Update 1 package can be obtained here . Next, install the driver module nvidia-gds for the GPU-Direct Storage (GDS). Please specify the same version name as the CUDA Toolkit version after the package name. # dnf module install nvidia-driver:open-dkms # dnf install nvidia-gds-12-5 Once installation completed successfully, CUDA Toolkit is deployed at /usr/local/cuda . $ ls /usr/local/cuda/ bin/ gds/ nsightee_plugins/ targets/ compute-sanitizer/ include@ nvml/ tools/ CUDA_Toolkit_Release_Notes.txt lib64@ nvvm/ version.json DOCS libnvvp/ README EULA.txt LICENSE share/ extras/ man/ src/ Once installation gets completed, ensure the system recognizes the GPU devices correctly. nvidia-smi command shows GPU information installed on your system, as follows. $ nvidia-smi Mon Jun 3 09:56:41 2024 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA A100-PCIE-40GB Off | 00000000:41:00.0 Off | 0 | | N/A 58C P0 66W / 250W | 1MiB / 40960MiB | 0% Default | | | | Disabled | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ Tips Additional configurations for systems with NVSwitch For systems with multiple GPUs that use NVSwitch for interconnect them, the nvidia-fabricmanager module must be installed. If this package is not installed, cuInit() , which initializes CUDA, will fail with the CUDA_ERROR_SYSTEM_NOT_READY error, and PG-Strom will not start. Run the following commands to install the nvidia-fabricmanager package. (source) # dnf install nvidia-fabricmanager # systemctl enable nvidia-fabricmanager.service # systemctl start nvidia-fabricmanager.service","title":"CUDA Toolkit Installation"},{"location":"install/#check-gpudirect-storage-status","text":"After the installation of CUDA Toolkit according to the steps above, your system will become ready for the GPUDirect Storage. Run gdscheck tool to confirm the configuration for each storage devices, as follows. (Thie example loads not only nvme , but nvme-rdma and rpcrdma kernel modules also, therefore, it reports the related features as Supported ) # /usr/local/cuda/gds/tools/gdscheck -p GDS release version: 1.10.0.4 nvidia_fs version: 2.20 libcufile version: 2.12 Platform: x86_64 ============ ENVIRONMENT: ============ ===================== DRIVER CONFIGURATION: ===================== NVMe : Supported NVMeOF : Supported SCSI : Unsupported ScaleFlux CSD : Unsupported NVMesh : Unsupported DDN EXAScaler : Unsupported IBM Spectrum Scale : Unsupported NFS : Supported BeeGFS : Unsupported WekaFS : Unsupported Userspace RDMA : Unsupported --Mellanox PeerDirect : Disabled --rdma library : Not Loaded (libcufile_rdma.so) --rdma devices : Not configured --rdma_device_status : Up: 0 Down: 0 ===================== CUFILE CONFIGURATION: ===================== properties.use_compat_mode : true properties.force_compat_mode : false properties.gds_rdma_write_support : true properties.use_poll_mode : false properties.poll_mode_max_size_kb : 4 properties.max_batch_io_size : 128 properties.max_batch_io_timeout_msecs : 5 properties.max_direct_io_size_kb : 16384 properties.max_device_cache_size_kb : 131072 properties.max_device_pinned_mem_size_kb : 33554432 properties.posix_pool_slab_size_kb : 4 1024 16384 properties.posix_pool_slab_count : 128 64 32 properties.rdma_peer_affinity_policy : RoundRobin properties.rdma_dynamic_routing : 0 fs.generic.posix_unaligned_writes : false fs.lustre.posix_gds_min_kb: 0 fs.beegfs.posix_gds_min_kb: 0 fs.weka.rdma_write_support: false fs.gpfs.gds_write_support: false profile.nvtx : false profile.cufile_stats : 0 miscellaneous.api_check_aggressive : false execution.max_io_threads : 4 execution.max_io_queue_depth : 128 execution.parallel_io : true execution.min_io_threshold_size_kb : 8192 execution.max_request_parallelism : 4 properties.force_odirect_mode : false properties.prefer_iouring : false ========= GPU INFO: ========= GPU index 0 NVIDIA A100-PCIE-40GB bar:1 bar size (MiB):65536 supports GDS, IOMMU State: Disabled ============== PLATFORM INFO: ============== IOMMU: disabled Nvidia Driver Info Status: Supported(Nvidia Open Driver Installed) Cuda Driver Version Installed: 12050 Platform: AS -2014CS-TR, Arch: x86_64(Linux 5.14.0-427.18.1.el9_4.x86_64) Platform verification succeeded Tips Additional configuration for RAID volume For data reading from software RAID (md-raid0) volumes by GPUDirect Storage, the following line must be added to the /lib/udev/rules.d/63-md-raid-arrays.rules configuration file. IMPORT{\u200bprogram}=\"/usr/sbin/mdadm --detail --export $devnode\" Then reboot the system to ensure the new configuration. See NVIDIA GPUDirect Storage Installation and Troubleshooting Guide for the details.","title":"Check GPUDirect Storage status"},{"location":"install/#pci-bar1-memory-configuration","text":"GPU-Direct SQL maps GPU device memory to the PCI BAR1 region (physical address space) on the host system, and sends P2P-RDMA requests to NVME devices with that as the destination for the shortest data transfer. To perform P2P-RDMA with sufficient multiplicity, the GPU must have enough PCI BAR1 space to map the device buffer. The size of the PCI BAR1 area is fixed for most GPUs, and PG-Strom recommends products whose size exceeds the GPU device memory size. However, some GPU products allow to change the size of the PCI BAR1 area by switching the operation mode. If your GPU is either of the following, refer to the NVIDIA Display Mode Selector Tool and switch to the mode that maximizes the PCI BAR1 area size. NVIDIA L40S NVIDIA L40 NVIDIA A40 NVIDIA RTX 6000 Ada NVIDIA RTX A6000 NVIDIA RTX A5500 NVIDIA RTX A5000 To check the GPU memory size and PCI BAR1 size installed in the system, use the nvidia-smi -q command. Memory-related status is displayed as shown below. $ nvidia-smi -q : FB Memory Usage Total : 46068 MiB Reserved : 685 MiB Used : 4 MiB Free : 45377 MiB BAR1 Memory Usage Total : 65536 MiB Used : 1 MiB Free : 65535 MiB :","title":"PCI Bar1 Memory Configuration"},{"location":"install/#heterodb-extra-modules","text":"heterodb-extra module enhances PG-Strom the following features. multi-GPUs support GPUDirect SQL GiST index support on GPU License management If you don't use the above features, only open source modules, you don't need to install the heterodb-extra module here. Please skip this section. Install the heterodb-extra package, downloaded from the SWDC, as follows. # dnf install heterodb-extra","title":"HeteroDB extra modules"},{"location":"install/#license-activation","text":"License activation is needed to use all the features of heterodb-extra , provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted. Multiple GPUs support Striping of NVME-SSD drives (md-raid0) on GPUDirect SQL Support of NVME-oF device on GPUDirect SQL Support of GiST index on GPU-version of PostGIS workloads You can obtain a license file, like as a plain text below, from HeteroDB,Inc. IAgIVdKxhe+BSer3Y67jQW0+uTzYh00K6WOSH7xQ26Qcw8aeUNYqJB9YcKJTJb+QQhjmUeQpUnboNxVwLCd3HFuLXeBWMKp11/BgG0FSrkUWu/ZCtDtw0F1hEIUY7m767zAGV8y+i7BuNXGJFvRlAkxdVO3/K47ocIgoVkuzBfLvN/h9LffOydUnHPzrFHfLc0r3nNNgtyTrfvoZiXegkGM9GBTAKyq8uWu/OGonh9ybzVKOgofhDLk0rVbLohOXDhMlwDl2oMGIr83tIpCWG+BGE+TDwsJ4n71Sv6n4bi/ZBXBS498qShNHDGrbz6cNcDVBa+EuZc6HzZoF6UrljEcl= ---- VERSION:2 SERIAL_NR:HDB-TRIAL ISSUED_AT:2019-05-09 EXPIRED_AT:2019-06-08 GPU_UUID:GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f Copy the license file to /etc/heterodb.license , then restart PostgreSQL. The startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done. : LOG: HeteroDB Extra module loaded [api_version=20231105,cufile=on,nvme_strom=off,githash=9ca2fe4d2fbb795ad2d741dcfcb9f2fe499a5bdf] LOG: HeteroDB License: { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"2022-11-19\", \"expired_at\" : \"2099-12-31\", \"nr_gpus\" : 1, \"gpus\" : [ { \"uuid\" : \"GPU-13943bfd-5b30-38f5-0473-78979c134606\" } ]} LOG: PG-Strom version 5.0.1 built for PostgreSQL 15 (githash: 972441dbafed6679af86af40bc8613be2d73c4fd) :","title":"License activation"},{"location":"install/#postgresql-installation","text":"This section introduces PostgreSQL installation with RPM. We don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the ./configure script. PostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community. PostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages. Like the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software. Here is the list of yum repository definition: http://yum.postgresql.org/repopackages.php . Repository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v15 or later. You can install PostgreSQL as following steps: Installation of yum repository definition. Disables the distribution's default PostgreSQL module Installation of PostgreSQL packages. # dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm # dnf -y module disable postgresql # dnf install -y postgresql16-devel postgresql16-server Note On the Red Hat Enterprise Linux, the package name postgresql conflicts to the default one at the distribution, thus, unable to install the packages from PGDG. So, disable the postgresql module by the distribution, using dnf -y module disable postgresql .","title":"PostgreSQL Installation"},{"location":"install/#libarrowlibparquet-installation","text":"PG-Strom v6.1 and later requires libarrow and libparquet for building and installation. Since the packages provided by the Linux distribution may be outdated, please follow the developer community guidance and install the arrow-devel and parquet-devel packages. The steps required for a minimal installation, excluding any overlaps with the installation steps above, are as follows. $ sudo dnf install -y https://packages.apache.org/artifactory/arrow/almalinux/$(cut -d: -f5 /etc/system-release-cpe | cut -d. -f1)/apache-arrow-release-latest.rpm $ sudo dnf install -y arrow-devel $ sudo dnf install -y parquet-devel","title":"libarrow/libparquet Installation"},{"location":"install/#pg-strom-installation","text":"","title":"PG-Strom Installation"},{"location":"install/#rpm-installation","text":"PG-Strom and related packages are distributed from HeteroDB Software Distribution Center . If you repository definition has been added, not many tasks are needed. We provide individual RPM packages of PG-Strom for each PostgreSQL major version. pg_strom-PG15 package is built for PostgreSQL v15, and pg_strom-PG16 is also built for PostgreSQL v16. It is a restriction due to binary compatibility of extension modules for PostgreSQL. # dnf install -y pg_strom-PG16 That's all for package installation.","title":"RPM Installation"},{"location":"install/#installation-from-the-source","text":"For developers, we also introduces the steps to build and install PG-Strom from the source code.","title":"Installation from the source"},{"location":"install/#getting-the-source-code","text":"Like RPM packages, you can download tarball of the source code from HeteroDB Software Distribution Center . On the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of PG-Strom on GitHub to use the latest development branch. $ git clone https://github.com/heterodb/pg-strom.git Cloning into 'pg-strom'... remote: Counting objects: 13797, done. remote: Compressing objects: 100% (215/215), done. remote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400 Receiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done. Resolving deltas: 100% (10504/10504), done.","title":"Getting the source code"},{"location":"install/#building-the-pg-strom","text":"Configuration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular strcut has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out. Thus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using pg_config command. If PATH environment variable is set to the pg_config command of the target PostgreSQL, run make and make install . Elsewhere, give PG_CONFIG=... parameter on make command to tell the full path of the pg_config command. $ cd pg-strom/src $ make PG_CONFIG=/usr/pgsql-16/bin/pg_config $ sudo make install PG_CONFIG=/usr/pgsql-16/bin/pg_config","title":"Building the PG-Strom"},{"location":"install/#post-installation-setup","text":"","title":"Post Installation Setup"},{"location":"install/#creation-of-database-cluster","text":"Database cluster is not constructed yet, run initdb command to set up initial database of PostgreSQL. The default path of the database cluster on RPM installation is /var/lib/pgsql/<version number>/data . If you install postgresql-alternatives package, this default path can be referenced by /var/lib/pgdata regardless of the PostgreSQL version. # su - postgres $ /usr/pgsql-16/bin/initdb -D /var/lib/pgdata/ The files belonging to this database system will be owned by user \"postgres\". This user must also own the server process. The database cluster will be initialized with locale \"en_US.UTF-8\". The default database encoding has accordingly been set to \"UTF8\". The default text search configuration will be set to \"english\". Data page checksums are disabled. fixing permissions on existing directory /var/lib/pgdata ... ok creating subdirectories ... ok selecting dynamic shared memory implementation ... posix selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting default time zone ... Asia/Tokyo creating configuration files ... ok running bootstrap script ... ok performing post-bootstrap initialization ... ok syncing data to disk ... ok initdb: warning: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /var/lib/pgdata/ -l logfile start","title":"Creation of database cluster"},{"location":"install/#setup-postgresqlconf","text":"Next, edit postgresql.conf which is a configuration file of PostgreSQL. The parameters below should be edited at least to work PG-Strom. Investigate other parameters according to usage of the system and expected workloads. shared_preload_libraries PG-Strom module must be loaded on startup of the postmaster process by the shared_preload_libraries . Unable to load it on demand. Therefore, you must add the configuration below. shared_preload_libraries = '$libdir/pg_strom' max_worker_processes PG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin. max_worker_processes = 100 shared_buffers Although it depends on the workloads, the initial configuration of shared_buffers is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently. So, we recommend to expand the variable for a certain margin. shared_buffers = 10GB Please consider to apply SSD-to-GPU Direct SQL Execution to process larger than system's physical RAM size. work_mem Although it depends on the workloads, the initial configuration of work_mem is too small to choose the optimal query execution plan on analytic queries. An typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting. So, we recommend to expand the variable for a certain margin. work_mem = 1GB","title":"Setup postgresql.conf"},{"location":"install/#expand-os-resource-limits","text":"GPU Direct SQL especially tries to open many files simultaneously, so resource limit for number of file descriptors per process should be expanded. Also, we recommend not to limit core file size to generate core dump of PostgreSQL certainly on system crash. If PostgreSQL service is launched by systemd, you can put the configurations of resource limit at /etc/systemd/system/postgresql-XX.service.d/pg_strom.conf . RPM installation setups the configuration below by the default. It comments out configuration to the environment variable CUDA_ENABLE_COREDUMP_ON_EXCEPTION . This is a developer option that enables to generate GPU's core dump on any CUDA/GPU level errors, if enabled. See CUDA-GDB:GPU core dump support for more details. [Service] LimitNOFILE=65536 LimitCORE=infinity #Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1","title":"Expand OS resource limits"},{"location":"install/#start-postgresql","text":"Start PostgreSQL service. If PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices. The example below recognized two NVIDIA A100 (PCIE; 40GB), and displays the closest GPU identifier foe each NVME-SSD drive. # systemctl start postgresql-16 # journalctl -u postgresql-16 Jun 02 17:28:45 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:45.989 JST [20242] LOG: HeteroDB Extra module loaded [api_version=20240418,cufile=off,nvme_strom=off,githash=3ffc65428c07bb3c9d0e5c75a2973389f91dfcd4] Jun 02 17:28:45 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:45.989 JST [20242] LOG: HeteroDB License: { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"2024-06-02\", \"expired_at\" : \"2099-12-31\", \"nr_gpus\" : 1, \"gpus\" : [ { \"uuid\" : \"GPU-13943bfd-5b30-38f5-0473-78979c134606\" } ]} Jun 02 17:28:45 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:45.989 JST [20242] LOG: PG-Strom version 5.12.el9 built for PostgreSQL 16 (githash: ) Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.114 JST [20242] LOG: PG-Strom binary built for CUDA 12.4 (CUDA runtime 12.5) Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.114 JST [20242] WARNING: The CUDA version where this PG-Strom module binary was built for (12.4) is newer than the CUDA runtime version on this platform (12.5). It may lead unexpected behavior, and upgrade of CUDA toolkit is recommended. Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.114 JST [20242] LOG: PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.50GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:41:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78979c134606) Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:81:00:0] nvme6 (NGD-IN2500-080T4-C) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:82:00:0] nvme3 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c2:00:0] nvme1 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c6:00:0] nvme4 (Corsair MP600 CORE) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c3:00:0] nvme5 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.117 JST [20242] LOG: [0000:c4:00:0] nvme2 (NGD-IN2500-080T4-C) --> GPU0 [dist=9] Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.217 JST [20242] LOG: redirecting log output to logging collector process Jun 02 17:28:48 buri.heterodb.com postgres[20242]: 2024-06-02 17:28:48.217 JST [20242] HINT: Future log output will appear in directory \"log\". Jun 02 17:28:48 buri.heterodb.com systemd[1]: Started PostgreSQL 16 database server.","title":"Start PostgreSQL"},{"location":"install/#creation-of-pg-strom-extension","text":"At the last, create database objects related to PG-Strom, like SQL functions. This steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is CREATE EXTENSION on the SQL command line. Please note that this step is needed for each new database. If you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the template1 database, its configuration will be copied to the new database on CREATE DATABASE command. $ psql -U postgres psql (16.3) Type \"help\" for help. postgres=# CREATE EXTENSION pg_strom ; CREATE EXTENSION That's all for the installation.","title":"Creation of PG-Strom Extension"},{"location":"install/#postgis-installation","text":"PG-Strom supports execution of a part of PostGIS functions on GPU devices. This section introduces the steps to install PostGIS module. Skip it on your demand. PostGIS module can be installed from the yum repository by PostgreSQL Global Development Group, like PostgreSQL itself. The example below shows the command to install PostGIS v3.4 built for PostgreSQL v16. # dnf install postgis34_16 Start PostgreSQL server after the initial setup of database cluster, then run CREATE EXTENSION command from SQL client to define geometry data type and SQL functions for geoanalytics. postgres=# CREATE EXTENSION postgis; CREATE EXTENSION","title":"PostGIS Installation"},{"location":"install/#installation-on-ubuntu-linux","text":"Although PG-Strom packages are not available for Ubuntu Linux right now, you can build and run PG-Strom from the source code. After the installation of Ubuntu Linux, install the MOFED driver, CUDA Toolkit, and PostgreSQL for Ubuntu Linux, respectively. Next, install the heterodb-extra package. A .deb package for Ubuntu Linux is provided, so please obtain the latest version from the SWDC . $ wget https://heterodb.github.io/swdc/deb/heterodb-extra_5.4-1_amd64.deb $ sudo dpkg -i heterodb-extra_5.4-1_amd64.deb Checkout the source code of PG-Strom, build and install as follows. At this time, do not forget to specify the target PostgreSQL by pg_config . Post-installation configuration is almost same as for Red Hat Enterprise Linux or Rocky Linux. $ git clone https://github.com/heterodb/pg-strom.git $ cd pg-strom/src $ make PG_CONFIG=/path/to/pgsql/bin/pg_config -j 8 $ sudo make PG_CONFIG=/path/to/pgsql/bin/pg_config install However, if you use a packaged PostgreSQL and start it by systemctl command, the PATH environment variable will be cleared (probably for security reasons). As a result, the script launched to build the GPU binary on the first startup will not work properly. To avoid this, if you are using Ubuntu Linux, add the following line to /etc/postgresql/main/PGVERSION/environment . /etc/postgresql/PGVERSION/main/environment: PATH='/usr/local/cuda/bin:/usr/bin:/bin'","title":"Installation on Ubuntu Linux"},{"location":"operations/","text":"Basic operations Confirmation of GPU off-loading You can use EXPLAIN command to check whether query is executed on GPU device or not. A query is internally split into multiple elements and executed, and PG-Strom is capable to run SCAN, JOIN and GROUP BY in parallel on GPU device. If you can find out GpuScan, GpuJoin or GpuPreAgg was displayed instead of the standard operations by PostgreSQL, it means the query is partially executed on GPU device. Below is an example of EXPLAIN command output. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------- HashAggregate (cost=2924539.01..2924612.42 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Custom Scan (GpuPreAgg) on lineorder (cost=2924421.55..2924494.96 rows=5873 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (lo_partkey = p_partkey) [plan: 600046000 -> 783060 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (lo_suppkey = s_suppkey) [plan: 783060 -> 157695 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (lo_orderdate = d_datekey) [plan: 157695 -> 157695 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Seq Scan on part (cost=0.00..41481.00 rows=1827 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Custom Scan (GpuScan) on supplier (cost=100.00..19001.67 rows=203767 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 1000000 -> 203767] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (22 rows) You can notice some unusual query execution plans. GpuJoin and GpuPreAgg are implemented on the CustomScan mechanism. In this example, GpuJoin runs JOIN operation on lineorder , date1 , part and supplier , then GpuPreAgg which receives the result of GpuJoin runs GROUP BY operation by the d_year and p_brand1 column on GPU device. PG-Strom interacts with the query optimizer during PostgreSQL is building a query execution plan, and it offers alternative query execution plan with estimated cost for PostgreSQL's optimizer, if any of SCAN, JOIN, or GROUP BY are executable on GPU device. This estimated cost is better than other query execution plans that run on CPU, it chooses the alternative execution plan that shall run on GPU device. For GPU execution, it requires operators, functions and data types in use must be supported by PG-Strom. It supports numeric types like int or float , date and time types like date or timestamp , variable length string like text and so on. It also supports arithmetic operations, comparison operators and many built-in operators. See References for the detailed list. Enables/Disables PG-Strom PG-Strom analyzes SQL given by the user, and if it can be executed on the GPU, it generates opcodes corresponding to WHERE clauses and JOIN search conditions and executes them transparently on the GPU. These processes are done automatically, but you can explicitly disable PG-Strom with the following command and make it work the same as original PostgreSQL. =# set pg_strom.enabled = off; SET In addition, you can enable/disable individual functions using the following parameters. pg_strom.enable_gpuscan pg_strom.enable_gpujoin pg_strom.enable_gpuhashjoin pg_strom.enable_gpugistindex pg_strom.enable_gpupreagg pg_strom.enable_gpusort pg_strom.enable_brin pg_strom.enable_partitionwise_gpujoin pg_strom.enable_partitionwise_gpupreagg CPU+GPU Hybrid Parallel PG-Strom also supports PostgreSQL's CPU parallel execution. PostgreSQL's CPU parallel execution is implemented by a Gather node starting several background worker processes, and later combining the results of queries that each background worker \"partially\" executed. PG-Strom processes such as GpuJoin and GpuPreAgg can be executed on the background worker side, and each process uses the GPU to perform processing. Normally, the processing speed of each CPU core to set up a buffer to supply data to the GPU is much slower than the processing speed of SQL workloads on the GPU, so a hybrid of CPU parallelism and GPU parallelism can be expected to improve processing speed. In the CPU parallel execution mode, Gather node launches several background worker processes, then it gathers the result of \"partial\" execution by individual background workers. CustomScan execution plan provided by PG-Strom, like GpuJoin or GpuPreAgg, support execution at the background workers. They process their partial task using GPU individually. A CPU core usually needs much more time to set up buffer to supply data for GPU than execution of SQL workloads on GPU, so hybrid usage of CPU and GPU parallel can expect higher performance. On the other hands, each PostgreSQL process needs to connect the PG-Strom GPU service, running in the background, and initialize the per-session stete, so a high degree of CPU parallelism is not always better. Look at the execution plan below. The execution plan below Gather can be executed by background workers. The lineorder table, which holds 600 million rows, is scanned by two background worker processes and the coordinator process, so approximately 200 million rows per process are processed by GpuPreAgg, and the results are joined by Gather and HashAggregate nodes. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------------------- HashAggregate (cost=1265644.05..1265717.46 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Gather (cost=1264982.11..1265600.00 rows=5873 width=46) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=1263982.11..1264012.70 rows=5873 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (lo_partkey = p_partkey) [plan: 250019100 -> 326275 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (lo_suppkey = s_suppkey) [plan: 326275 -> 65706 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (lo_orderdate = d_datekey) [plan: 65706 -> 65706 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Seq Scan on part (cost=0.00..29231.00 rows=761 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Parallel Custom Scan (GpuScan) on supplier (cost=100.00..8002.40 rows=84903 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 1000000 -> 84903] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Seq Scan on date1 (cost=0.00..62.04 rows=1504 width=8) (24 rows) Configuration of parallelism Parallelism in PostgreSQL is the number of processes when multiple worker processes are used in parallel to execute a query. This is the number of processes that the Gather node starts in the execution plan, and can be controlled mainly by the max_parallel_workers_per_gather parameter. The existence of parallel worker processes is also important in PG-Strom. Even though most data reading from storage is done by the GPU-Direct SQL mechanism and the CPU load is not large, it is the CPU's job to check the visibility of blocks to be read and to copy the contents of dirty buffers. In addition, there is another point in PG-Strom that should be considered when considering the parallelism of processing. That is the number of worker threads in the GPU-Service. The diagram above shows a schematic of the PG-Strom architecture. When a client connects to PostgreSQL, the postmaster process, which manages all processes, starts a PostgreSQL Backend process for each connection. This process receives SQL from the client and executes the query based on the execution plan, possibly with the help of a Parallel Worker process. When using PG-Strom to execute queries, these processes open a connection to the resident PG-Strom GPU Service process via a UNIX domain socket. Then, they send requests one after another, pairing the instruction code to be executed with the storage information to be read (approximately 64MB chunks). PG-Strom GPU Service is multi-threaded, and each worker thread executes these requests one after another when it receives them. A typical request is processed as follows: read from storage, start GPU Kernel, collect the processing result, and send a response request. Since these processes can be easily multiplexed, it is necessary to launch a sufficient number of threads to avoid idling resources, for example, while thread A is waiting to read from storage, thread B can execute GPU Kernel. To change the number of worker threads, use the pg_strom.max_async_tasks parameter. For each GPU, the number of threads specified by this parameter will be launched and wait for requests from the PostgreSQL backend/worker process. =# SET pg_strom.max_async_tasks = 24; SET The parameter setting takes effect immediately. For example, if you increase it from the default 16 to 24 , 8 additional worker threads will be launched for each GPU. After a few seconds, you will see the following log output: LOG: GPU0 workers - 8 startup, 0 terminate LOG: GPU1 workers - 8 startup, 0 terminate Consolidation of sub-plans PG-Strom can execute SCAN, JOIN, GROUP BY and SORT processes on the GPU. However, if you simply replace the corresponding standard PostgreSQL processes with GPU processes, you will run into problems. After SCAN, the data is written back to the host buffer, then copied back to the GPU for JOIN, and then written back to the host buffer again before GROUP BY is executed, resulting in data ping-ponging between the CPU and GPU. Compared to exchanging data (rows) in CPU memory, the CPU and GPU are connected by a PCI-E bus, so data transfer inevitably incurs a large cost. To avoid this, if a series of GPU-compatible tasks such as SCAN, JOIN, GROUP BY and SORT can be performed consecutively, data should be exchanged as much as possible in GPU memory, and writing data back to the CPU should be minimised. The following execution plan is for a mixed workload of SCAN, JOIN, and GROUP BY executed on PostgreSQL. You can see that the lineorder table, which is the largest, is used as the axis to join the part , supplier , and date1 tables using HashJoin, and finally an Aggregate is used to perform the aggregation process. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------- Finalize HashAggregate (cost=14892768.98..14892842.39 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Gather (cost=14891403.50..14892651.52 rows=11746 width=46) Workers Planned: 2 -> Partial HashAggregate (cost=14890403.50..14890476.92 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Hash Join (cost=52477.64..14889910.71 rows=65706 width=20) Hash Cond: (lineorder.lo_orderdate = date1.d_datekey) -> Parallel Hash Join (cost=52373.13..14888902.74 rows=65706 width=20) Hash Cond: (lineorder.lo_suppkey = supplier.s_suppkey) -> Parallel Hash Join (cost=29240.51..14864272.81 rows=326275 width=26) Hash Cond: (lineorder.lo_partkey = part.p_partkey) -> Parallel Seq Scan on lineorder (cost=0.00..13896101.47 rows=250019147 width=20) -> Parallel Hash (cost=29231.00..29231.00 rows=761 width=14) -> Parallel Seq Scan on part (cost=0.00..29231.00 rows=761 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Parallel Hash (cost=22071.33..22071.33 rows=84903 width=6) -> Parallel Seq Scan on supplier (cost=0.00..22071.33 rows=84903 width=6) Filter: (s_region = 'ASIA'::bpchar) -> Hash (cost=72.56..72.56 rows=2556 width=8) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (21 rows) Conversely, when utilizing PG-Strom, the scenario is notably distinct. With the exception of the Result node, which is responsible for displaying the results, all processing is executed by Custom Scan (GpuPreAgg). (Note: In this execution plan, CPU parallelism and CPU-Fallback are disabled to keep the results as simple as possible.) However, despite being designated as GPU-PreAgg, this processing does not exclusively perform GROUP BY. According to the EXPLAIN output, this GPU-PreAgg scans the lineorder table, which is the largest, while reading the part, supplier, and date1 tables at the lower nodes and performing JOIN processing with these. It then groups by d_year and p_brand1 , sorts by the same keys, and returns the processing results to the CPU. In PostgreSQL, complex queries are often broken down into many elements, and an execution plan containing many processing steps is generated. On the other hand, in PG-Strom, these elements are also executed without any omissions, but it is generally more efficient to integrate them into a single plan as much as possible. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1 order by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------- Result (cost=3111326.30..3111451.10 rows=5873 width=46) -> Custom Scan (GpuPreAgg) on lineorder (cost=3111326.30..3111363.01 rows=5873 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (lo_partkey = p_partkey) [plan: 600046000 -> 783060 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (lo_suppkey = s_suppkey) [plan: 783060 -> 157695 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (lo_orderdate = d_datekey) [plan: 157695 -> 157695 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> GPU-Sort keys: d_year, p_brand1 -> Seq Scan on part (cost=0.00..41481.00 rows=1827 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Custom Scan (GpuScan) on supplier (cost=100.00..19156.92 rows=203767 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 1000000 -> 203767] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (22 rows) Special Logging For example, when PG-Strom does not behave as expected, log output is important to find out why it is not working as expected. In this section, we will explain how to output information specific to PG-Strom. When PG-Strom checks a given query and evaluates whether it can be executed on the GPU and what additional features are available, please increase the log output level to DEBUG2 . If the condition expression contains an operator that is determined to be unable to be executed on the GPU and the data type is included, PG-Strom will output this information in the log. See the example below. Currently, PG-Strom does not support GPU execution of to_hex() function, so it has given up on generating CustomScan(GpuScan) because the WHERE clause contains this function. =# SET client_min_messages = DEBUG2; SET =# explain select count(*), lo_shipmode from lineorder where to_hex(lo_orderdate) like '%34' group by lo_shipmode; DEBUG: (__codegen_func_expression:1858) function to_hex(integer) is not supported on the target device DETAIL: problematic expression: {OPEXPR :opno 1209 :opfuncid 850 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 100 :args ({FUNCEXPR :funcid 2089 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 0 :args ({VAR :varno 1 :varattno 6 :vartype 23 :vartypmod -1 :varcollid 0 :varnullingrels (b) :varlevelsup 0 :varnosyn 1 :varattnosyn 6 :location 65}) :location 58} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 84 :constvalue 7 [ 28 0 0 0 37 51 52 ]}) :location 79} DEBUG: (__codegen_func_expression:1858) function to_hex(integer) is not supported on the target device DETAIL: problematic expression: {OPEXPR :opno 1209 :opfuncid 850 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 100 :args ({FUNCEXPR :funcid 2089 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 0 :args ({VAR :varno 1 :varattno 6 :vartype 23 :vartypmod -1 :varcollid 0 :varnullingrels (b) :varlevelsup 0 :varnosyn 1 :varattnosyn 6 :location 65}) :location 58} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 84 :constvalue 7 [ 28 0 0 0 37 51 52 ]}) :location 79} QUERY PLAN --------------------------------------------------------------------------------------------------------- Finalize GroupAggregate (cost=15197201.22..15197203.00 rows=7 width=19) Group Key: lo_shipmode -> Gather Merge (cost=15197201.22..15197202.86 rows=14 width=19) Workers Planned: 2 -> Sort (cost=15196201.20..15196201.22 rows=7 width=19) Sort Key: lo_shipmode -> Partial HashAggregate (cost=15196201.03..15196201.10 rows=7 width=19) Group Key: lo_shipmode -> Parallel Seq Scan on lineorder (cost=0.00..15146197.20 rows=10000766 width=11) Filter: (to_hex(lo_orderdate) ~~ '%34'::text) (10 rows) The output is intended for developers, so it's not necessarily easy to understand, but it shows that the to_hex(integer) function is not supported. Using this as a reference, you can rewrite the condition clause to achieve the same effect (of course, there may be cases where rewriting is not possible). postgres=# explain select count(*), lo_shipmode from lineorder where lo_orderdate % 256 = 34 group by lo_shipmode; DEBUG: gpusort: disabled by pg_strom.cpu_fallback DEBUG: gpusort: disabled by pg_strom.cpu_fallback DEBUG: gpucache: table 'lineorder' is not configured - check row/statement triggers with pgstrom.gpucache_sync_trigger() QUERY PLAN ---------------------------------------------------------------------------------------------------------- HashAggregate (cost=1221119.96..1221120.03 rows=7 width=19) Group Key: lo_shipmode -> Gather (cost=1221119.19..1221119.93 rows=7 width=19) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=1220119.19..1220119.23 rows=7 width=19) GPU Projection: pgstrom.nrows(), lo_shipmode GPU Scan Quals: ((lo_orderdate % 256) = 34) [plan: 600046000 -> 1250096] GPU Group Key: lo_shipmode Scan-Engine: GPU-Direct with 2 GPUs <0,1> (9 rows) In this way, an execution plan for GPU-PreAgg with a filter based on the WHERE clause was generated. It also shows that although the use of GPU-Sort and GPU-Cache was considered, they were not available ( pg_strom.cpu_fallback is enabled) and were not configured ( lineorder has no GPU-Cache setting). Consequently, an execution plan for GPU-PreAgg with a filter based on the WHERE clause was generated. It also shows that although the use of GPU-Sort and GPU-Cache was considered, they were not available ( pg_strom.cpu_fallback is enabled) and were not configured ( lineorder has no GPU-Cache setting). When PG-Strom uses GPU-Direct SQL, it uses the heterodb-extra extension module. To control the output of logs from the heterodb-extra extension module, use the pg_strom.extra_ereport_level parameter. The setting value ranges from 0 to 2, and is roughly classified as follows: 0 ... Only output clear errors 1 ... Output logs related to internal conditional branching 2 ... Output detailed messages for debugging See the example below. =# import foreign schema f_customer from server arrow_fdw into public options (file '/tmp/f_customer.arrow'); IMPORT FOREIGN SCHEMA =# set pg_strom.extra_ereport_level = 1; SET =# explain select count(*), c_name from f_customer group by c_name; QUERY PLAN --------------------------------------------------------------------------------------------------------- HashAggregate (cost=38597.46..38599.46 rows=200 width=40) Group Key: c_name -> Gather (cost=38575.42..38596.46 rows=200 width=40) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on f_customer (cost=37575.42..37576.46 rows=200 width=40) GPU Projection: pgstrom.nrows(), c_name GPU Group Key: c_name referenced: c_name file0: /tmp/f_customer.arrow (read: 629.43MB, size: 3404.59MB) Scan-Engine: VFS with 2 GPUs <0,1> (10 rows) As you can see, aggregation queries that reference /tmp/f_customer.arrow cannot use GPU-Direct SQL. If you check the logs to see why, you will see the following message: LOG: heterodb-extra: [info] path='/tmp/f_customer.arrow' on 'sdb3 (8,19)' optimal_gpus=00000000 numa_gpus=00000000 system_gpus=00000003 license-validation='-' policy='optimal' (pcie.c:1738) LOG: [info] foreign-table='f_customer' arrow-file='/tmp/f_customer.arrow' has no schedulable GPUs (arrow_fdw.c:2829) It turns out that /dev/sdb3 where /tmp/f_customer.arrow is located is not an NVME-SSD, and therefore there is no GPU that can be scheduled, so GPU-Direct SQL cannot be invoked. So, copy /tmp/f_customer.arrow to a partition on the NVME-SSD and run it again. =# import foreign schema f_customer from server arrow_fdw into public options (file '/opt/arrow/f_customer.arrow'); IMPORT FOREIGN SCHEMA =# explain select count(*), c_name from f_customer group by c_name; QUERY PLAN --------------------------------------------------------------------------------------------------------- HashAggregate (cost=38597.46..38599.46 rows=200 width=40) Group Key: c_name -> Gather (cost=38575.42..38596.46 rows=200 width=40) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on f_customer (cost=37575.42..37576.46 rows=200 width=40) GPU Projection: pgstrom.nrows(), c_name GPU Group Key: c_name referenced: c_name file0: /opt/arrow/f_customer.arrow (read: 629.43MB, size: 3404.59MB) Scan-Engine: GPU-Direct with 2 GPUs <0,1> (10 rows) In this way, we were able to successfully enable GPU-Direct SQL and run GpuPreAgg. the log outputs optimal_gpus=00000003 numa_gpus=00000003 when referring to /opt/arrow/mytest.arrow as follows, i.e. it is possible to schedule to GPU0 and GPU1. LOG: heterodb-extra: [info] path='/opt/arrow/mytest.arrow' on 'md127p1 (259,9)' optimal_gpus=00000003 numa_gpus=00000003 system_gpus=00000003 license-validation='Y' policy='optimal' (pcie.c:1738) Knowledge base We publish several articles, just called \"notes\", on the project wiki-site of PG-Strom. https://github.com/heterodb/pg-strom/wiki","title":"Basic Operations"},{"location":"operations/#basic-operations","text":"","title":"Basic operations"},{"location":"operations/#confirmation-of-gpu-off-loading","text":"You can use EXPLAIN command to check whether query is executed on GPU device or not. A query is internally split into multiple elements and executed, and PG-Strom is capable to run SCAN, JOIN and GROUP BY in parallel on GPU device. If you can find out GpuScan, GpuJoin or GpuPreAgg was displayed instead of the standard operations by PostgreSQL, it means the query is partially executed on GPU device. Below is an example of EXPLAIN command output. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------- HashAggregate (cost=2924539.01..2924612.42 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Custom Scan (GpuPreAgg) on lineorder (cost=2924421.55..2924494.96 rows=5873 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (lo_partkey = p_partkey) [plan: 600046000 -> 783060 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (lo_suppkey = s_suppkey) [plan: 783060 -> 157695 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (lo_orderdate = d_datekey) [plan: 157695 -> 157695 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Seq Scan on part (cost=0.00..41481.00 rows=1827 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Custom Scan (GpuScan) on supplier (cost=100.00..19001.67 rows=203767 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 1000000 -> 203767] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (22 rows) You can notice some unusual query execution plans. GpuJoin and GpuPreAgg are implemented on the CustomScan mechanism. In this example, GpuJoin runs JOIN operation on lineorder , date1 , part and supplier , then GpuPreAgg which receives the result of GpuJoin runs GROUP BY operation by the d_year and p_brand1 column on GPU device. PG-Strom interacts with the query optimizer during PostgreSQL is building a query execution plan, and it offers alternative query execution plan with estimated cost for PostgreSQL's optimizer, if any of SCAN, JOIN, or GROUP BY are executable on GPU device. This estimated cost is better than other query execution plans that run on CPU, it chooses the alternative execution plan that shall run on GPU device. For GPU execution, it requires operators, functions and data types in use must be supported by PG-Strom. It supports numeric types like int or float , date and time types like date or timestamp , variable length string like text and so on. It also supports arithmetic operations, comparison operators and many built-in operators. See References for the detailed list.","title":"Confirmation of GPU off-loading"},{"location":"operations/#enablesdisables-pg-strom","text":"PG-Strom analyzes SQL given by the user, and if it can be executed on the GPU, it generates opcodes corresponding to WHERE clauses and JOIN search conditions and executes them transparently on the GPU. These processes are done automatically, but you can explicitly disable PG-Strom with the following command and make it work the same as original PostgreSQL. =# set pg_strom.enabled = off; SET In addition, you can enable/disable individual functions using the following parameters. pg_strom.enable_gpuscan pg_strom.enable_gpujoin pg_strom.enable_gpuhashjoin pg_strom.enable_gpugistindex pg_strom.enable_gpupreagg pg_strom.enable_gpusort pg_strom.enable_brin pg_strom.enable_partitionwise_gpujoin pg_strom.enable_partitionwise_gpupreagg","title":"Enables/Disables PG-Strom"},{"location":"operations/#cpugpu-hybrid-parallel","text":"PG-Strom also supports PostgreSQL's CPU parallel execution. PostgreSQL's CPU parallel execution is implemented by a Gather node starting several background worker processes, and later combining the results of queries that each background worker \"partially\" executed. PG-Strom processes such as GpuJoin and GpuPreAgg can be executed on the background worker side, and each process uses the GPU to perform processing. Normally, the processing speed of each CPU core to set up a buffer to supply data to the GPU is much slower than the processing speed of SQL workloads on the GPU, so a hybrid of CPU parallelism and GPU parallelism can be expected to improve processing speed. In the CPU parallel execution mode, Gather node launches several background worker processes, then it gathers the result of \"partial\" execution by individual background workers. CustomScan execution plan provided by PG-Strom, like GpuJoin or GpuPreAgg, support execution at the background workers. They process their partial task using GPU individually. A CPU core usually needs much more time to set up buffer to supply data for GPU than execution of SQL workloads on GPU, so hybrid usage of CPU and GPU parallel can expect higher performance. On the other hands, each PostgreSQL process needs to connect the PG-Strom GPU service, running in the background, and initialize the per-session stete, so a high degree of CPU parallelism is not always better. Look at the execution plan below. The execution plan below Gather can be executed by background workers. The lineorder table, which holds 600 million rows, is scanned by two background worker processes and the coordinator process, so approximately 200 million rows per process are processed by GpuPreAgg, and the results are joined by Gather and HashAggregate nodes. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------------------- HashAggregate (cost=1265644.05..1265717.46 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Gather (cost=1264982.11..1265600.00 rows=5873 width=46) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=1263982.11..1264012.70 rows=5873 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (lo_partkey = p_partkey) [plan: 250019100 -> 326275 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (lo_suppkey = s_suppkey) [plan: 326275 -> 65706 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (lo_orderdate = d_datekey) [plan: 65706 -> 65706 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Seq Scan on part (cost=0.00..29231.00 rows=761 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Parallel Custom Scan (GpuScan) on supplier (cost=100.00..8002.40 rows=84903 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 1000000 -> 84903] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Seq Scan on date1 (cost=0.00..62.04 rows=1504 width=8) (24 rows)","title":"CPU+GPU Hybrid Parallel"},{"location":"operations/#configuration-of-parallelism","text":"Parallelism in PostgreSQL is the number of processes when multiple worker processes are used in parallel to execute a query. This is the number of processes that the Gather node starts in the execution plan, and can be controlled mainly by the max_parallel_workers_per_gather parameter. The existence of parallel worker processes is also important in PG-Strom. Even though most data reading from storage is done by the GPU-Direct SQL mechanism and the CPU load is not large, it is the CPU's job to check the visibility of blocks to be read and to copy the contents of dirty buffers. In addition, there is another point in PG-Strom that should be considered when considering the parallelism of processing. That is the number of worker threads in the GPU-Service. The diagram above shows a schematic of the PG-Strom architecture. When a client connects to PostgreSQL, the postmaster process, which manages all processes, starts a PostgreSQL Backend process for each connection. This process receives SQL from the client and executes the query based on the execution plan, possibly with the help of a Parallel Worker process. When using PG-Strom to execute queries, these processes open a connection to the resident PG-Strom GPU Service process via a UNIX domain socket. Then, they send requests one after another, pairing the instruction code to be executed with the storage information to be read (approximately 64MB chunks). PG-Strom GPU Service is multi-threaded, and each worker thread executes these requests one after another when it receives them. A typical request is processed as follows: read from storage, start GPU Kernel, collect the processing result, and send a response request. Since these processes can be easily multiplexed, it is necessary to launch a sufficient number of threads to avoid idling resources, for example, while thread A is waiting to read from storage, thread B can execute GPU Kernel. To change the number of worker threads, use the pg_strom.max_async_tasks parameter. For each GPU, the number of threads specified by this parameter will be launched and wait for requests from the PostgreSQL backend/worker process. =# SET pg_strom.max_async_tasks = 24; SET The parameter setting takes effect immediately. For example, if you increase it from the default 16 to 24 , 8 additional worker threads will be launched for each GPU. After a few seconds, you will see the following log output: LOG: GPU0 workers - 8 startup, 0 terminate LOG: GPU1 workers - 8 startup, 0 terminate","title":"Configuration of parallelism"},{"location":"operations/#consolidation-of-sub-plans","text":"PG-Strom can execute SCAN, JOIN, GROUP BY and SORT processes on the GPU. However, if you simply replace the corresponding standard PostgreSQL processes with GPU processes, you will run into problems. After SCAN, the data is written back to the host buffer, then copied back to the GPU for JOIN, and then written back to the host buffer again before GROUP BY is executed, resulting in data ping-ponging between the CPU and GPU. Compared to exchanging data (rows) in CPU memory, the CPU and GPU are connected by a PCI-E bus, so data transfer inevitably incurs a large cost. To avoid this, if a series of GPU-compatible tasks such as SCAN, JOIN, GROUP BY and SORT can be performed consecutively, data should be exchanged as much as possible in GPU memory, and writing data back to the CPU should be minimised. The following execution plan is for a mixed workload of SCAN, JOIN, and GROUP BY executed on PostgreSQL. You can see that the lineorder table, which is the largest, is used as the axis to join the part , supplier , and date1 tables using HashJoin, and finally an Aggregate is used to perform the aggregation process. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------- Finalize HashAggregate (cost=14892768.98..14892842.39 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Gather (cost=14891403.50..14892651.52 rows=11746 width=46) Workers Planned: 2 -> Partial HashAggregate (cost=14890403.50..14890476.92 rows=5873 width=46) Group Key: date1.d_year, part.p_brand1 -> Hash Join (cost=52477.64..14889910.71 rows=65706 width=20) Hash Cond: (lineorder.lo_orderdate = date1.d_datekey) -> Parallel Hash Join (cost=52373.13..14888902.74 rows=65706 width=20) Hash Cond: (lineorder.lo_suppkey = supplier.s_suppkey) -> Parallel Hash Join (cost=29240.51..14864272.81 rows=326275 width=26) Hash Cond: (lineorder.lo_partkey = part.p_partkey) -> Parallel Seq Scan on lineorder (cost=0.00..13896101.47 rows=250019147 width=20) -> Parallel Hash (cost=29231.00..29231.00 rows=761 width=14) -> Parallel Seq Scan on part (cost=0.00..29231.00 rows=761 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Parallel Hash (cost=22071.33..22071.33 rows=84903 width=6) -> Parallel Seq Scan on supplier (cost=0.00..22071.33 rows=84903 width=6) Filter: (s_region = 'ASIA'::bpchar) -> Hash (cost=72.56..72.56 rows=2556 width=8) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (21 rows) Conversely, when utilizing PG-Strom, the scenario is notably distinct. With the exception of the Result node, which is responsible for displaying the results, all processing is executed by Custom Scan (GpuPreAgg). (Note: In this execution plan, CPU parallelism and CPU-Fallback are disabled to keep the results as simple as possible.) However, despite being designated as GPU-PreAgg, this processing does not exclusively perform GROUP BY. According to the EXPLAIN output, this GPU-PreAgg scans the lineorder table, which is the largest, while reading the part, supplier, and date1 tables at the lower nodes and performing JOIN processing with these. It then groups by d_year and p_brand1 , sorts by the same keys, and returns the processing results to the CPU. In PostgreSQL, complex queries are often broken down into many elements, and an execution plan containing many processing steps is generated. On the other hand, in PG-Strom, these elements are also executed without any omissions, but it is generally more efficient to integrate them into a single plan as much as possible. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1 order by d_year, p_brand1; QUERY PLAN ------------------------------------------------------------------------------------------------- Result (cost=3111326.30..3111451.10 rows=5873 width=46) -> Custom Scan (GpuPreAgg) on lineorder (cost=3111326.30..3111363.01 rows=5873 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (lo_partkey = p_partkey) [plan: 600046000 -> 783060 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (lo_suppkey = s_suppkey) [plan: 783060 -> 157695 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (lo_orderdate = d_datekey) [plan: 157695 -> 157695 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> GPU-Sort keys: d_year, p_brand1 -> Seq Scan on part (cost=0.00..41481.00 rows=1827 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Custom Scan (GpuScan) on supplier (cost=100.00..19156.92 rows=203767 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'ASIA'::bpchar) [plan: 1000000 -> 203767] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (22 rows)","title":"Consolidation of sub-plans"},{"location":"operations/#special-logging","text":"For example, when PG-Strom does not behave as expected, log output is important to find out why it is not working as expected. In this section, we will explain how to output information specific to PG-Strom. When PG-Strom checks a given query and evaluates whether it can be executed on the GPU and what additional features are available, please increase the log output level to DEBUG2 . If the condition expression contains an operator that is determined to be unable to be executed on the GPU and the data type is included, PG-Strom will output this information in the log. See the example below. Currently, PG-Strom does not support GPU execution of to_hex() function, so it has given up on generating CustomScan(GpuScan) because the WHERE clause contains this function. =# SET client_min_messages = DEBUG2; SET =# explain select count(*), lo_shipmode from lineorder where to_hex(lo_orderdate) like '%34' group by lo_shipmode; DEBUG: (__codegen_func_expression:1858) function to_hex(integer) is not supported on the target device DETAIL: problematic expression: {OPEXPR :opno 1209 :opfuncid 850 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 100 :args ({FUNCEXPR :funcid 2089 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 0 :args ({VAR :varno 1 :varattno 6 :vartype 23 :vartypmod -1 :varcollid 0 :varnullingrels (b) :varlevelsup 0 :varnosyn 1 :varattnosyn 6 :location 65}) :location 58} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 84 :constvalue 7 [ 28 0 0 0 37 51 52 ]}) :location 79} DEBUG: (__codegen_func_expression:1858) function to_hex(integer) is not supported on the target device DETAIL: problematic expression: {OPEXPR :opno 1209 :opfuncid 850 :opresulttype 16 :opretset false :opcollid 0 :inputcollid 100 :args ({FUNCEXPR :funcid 2089 :funcresulttype 25 :funcretset false :funcvariadic false :funcformat 0 :funccollid 100 :inputcollid 0 :args ({VAR :varno 1 :varattno 6 :vartype 23 :vartypmod -1 :varcollid 0 :varnullingrels (b) :varlevelsup 0 :varnosyn 1 :varattnosyn 6 :location 65}) :location 58} {CONST :consttype 25 :consttypmod -1 :constcollid 100 :constlen -1 :constbyval false :constisnull false :location 84 :constvalue 7 [ 28 0 0 0 37 51 52 ]}) :location 79} QUERY PLAN --------------------------------------------------------------------------------------------------------- Finalize GroupAggregate (cost=15197201.22..15197203.00 rows=7 width=19) Group Key: lo_shipmode -> Gather Merge (cost=15197201.22..15197202.86 rows=14 width=19) Workers Planned: 2 -> Sort (cost=15196201.20..15196201.22 rows=7 width=19) Sort Key: lo_shipmode -> Partial HashAggregate (cost=15196201.03..15196201.10 rows=7 width=19) Group Key: lo_shipmode -> Parallel Seq Scan on lineorder (cost=0.00..15146197.20 rows=10000766 width=11) Filter: (to_hex(lo_orderdate) ~~ '%34'::text) (10 rows) The output is intended for developers, so it's not necessarily easy to understand, but it shows that the to_hex(integer) function is not supported. Using this as a reference, you can rewrite the condition clause to achieve the same effect (of course, there may be cases where rewriting is not possible). postgres=# explain select count(*), lo_shipmode from lineorder where lo_orderdate % 256 = 34 group by lo_shipmode; DEBUG: gpusort: disabled by pg_strom.cpu_fallback DEBUG: gpusort: disabled by pg_strom.cpu_fallback DEBUG: gpucache: table 'lineorder' is not configured - check row/statement triggers with pgstrom.gpucache_sync_trigger() QUERY PLAN ---------------------------------------------------------------------------------------------------------- HashAggregate (cost=1221119.96..1221120.03 rows=7 width=19) Group Key: lo_shipmode -> Gather (cost=1221119.19..1221119.93 rows=7 width=19) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=1220119.19..1220119.23 rows=7 width=19) GPU Projection: pgstrom.nrows(), lo_shipmode GPU Scan Quals: ((lo_orderdate % 256) = 34) [plan: 600046000 -> 1250096] GPU Group Key: lo_shipmode Scan-Engine: GPU-Direct with 2 GPUs <0,1> (9 rows) In this way, an execution plan for GPU-PreAgg with a filter based on the WHERE clause was generated. It also shows that although the use of GPU-Sort and GPU-Cache was considered, they were not available ( pg_strom.cpu_fallback is enabled) and were not configured ( lineorder has no GPU-Cache setting). Consequently, an execution plan for GPU-PreAgg with a filter based on the WHERE clause was generated. It also shows that although the use of GPU-Sort and GPU-Cache was considered, they were not available ( pg_strom.cpu_fallback is enabled) and were not configured ( lineorder has no GPU-Cache setting). When PG-Strom uses GPU-Direct SQL, it uses the heterodb-extra extension module. To control the output of logs from the heterodb-extra extension module, use the pg_strom.extra_ereport_level parameter. The setting value ranges from 0 to 2, and is roughly classified as follows: 0 ... Only output clear errors 1 ... Output logs related to internal conditional branching 2 ... Output detailed messages for debugging See the example below. =# import foreign schema f_customer from server arrow_fdw into public options (file '/tmp/f_customer.arrow'); IMPORT FOREIGN SCHEMA =# set pg_strom.extra_ereport_level = 1; SET =# explain select count(*), c_name from f_customer group by c_name; QUERY PLAN --------------------------------------------------------------------------------------------------------- HashAggregate (cost=38597.46..38599.46 rows=200 width=40) Group Key: c_name -> Gather (cost=38575.42..38596.46 rows=200 width=40) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on f_customer (cost=37575.42..37576.46 rows=200 width=40) GPU Projection: pgstrom.nrows(), c_name GPU Group Key: c_name referenced: c_name file0: /tmp/f_customer.arrow (read: 629.43MB, size: 3404.59MB) Scan-Engine: VFS with 2 GPUs <0,1> (10 rows) As you can see, aggregation queries that reference /tmp/f_customer.arrow cannot use GPU-Direct SQL. If you check the logs to see why, you will see the following message: LOG: heterodb-extra: [info] path='/tmp/f_customer.arrow' on 'sdb3 (8,19)' optimal_gpus=00000000 numa_gpus=00000000 system_gpus=00000003 license-validation='-' policy='optimal' (pcie.c:1738) LOG: [info] foreign-table='f_customer' arrow-file='/tmp/f_customer.arrow' has no schedulable GPUs (arrow_fdw.c:2829) It turns out that /dev/sdb3 where /tmp/f_customer.arrow is located is not an NVME-SSD, and therefore there is no GPU that can be scheduled, so GPU-Direct SQL cannot be invoked. So, copy /tmp/f_customer.arrow to a partition on the NVME-SSD and run it again. =# import foreign schema f_customer from server arrow_fdw into public options (file '/opt/arrow/f_customer.arrow'); IMPORT FOREIGN SCHEMA =# explain select count(*), c_name from f_customer group by c_name; QUERY PLAN --------------------------------------------------------------------------------------------------------- HashAggregate (cost=38597.46..38599.46 rows=200 width=40) Group Key: c_name -> Gather (cost=38575.42..38596.46 rows=200 width=40) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on f_customer (cost=37575.42..37576.46 rows=200 width=40) GPU Projection: pgstrom.nrows(), c_name GPU Group Key: c_name referenced: c_name file0: /opt/arrow/f_customer.arrow (read: 629.43MB, size: 3404.59MB) Scan-Engine: GPU-Direct with 2 GPUs <0,1> (10 rows) In this way, we were able to successfully enable GPU-Direct SQL and run GpuPreAgg. the log outputs optimal_gpus=00000003 numa_gpus=00000003 when referring to /opt/arrow/mytest.arrow as follows, i.e. it is possible to schedule to GPU0 and GPU1. LOG: heterodb-extra: [info] path='/opt/arrow/mytest.arrow' on 'md127p1 (259,9)' optimal_gpus=00000003 numa_gpus=00000003 system_gpus=00000003 license-validation='Y' policy='optimal' (pcie.c:1738)","title":"Special Logging"},{"location":"operations/#knowledge-base","text":"We publish several articles, just called \"notes\", on the project wiki-site of PG-Strom. https://github.com/heterodb/pg-strom/wiki","title":"Knowledge base"},{"location":"partition/","text":"Partitioning This chapter introduces the way to use PG-Strom and the partitioning feature of PostgreSQL. Note that this chapter is only valid when PG-Strom works on PostgreSQL v11 or later . Also see PostgreSQL Document: Table Partitioning for more details of the partitioning feature of PostgreSQL. Overview PostgreSQL v10 newly support table partitioning. This mechanism splits one logically large table into physically small pieces. It is valuable because it can skip partitioned child tables which is obviously unnecessary to scan from the search qualification, and it can offer broader I/O bandwidth by physically distributed storage and so on. PostgreSQL v10 supports two kinds of them: range-partitioning and list-partitioning. Then, PostgreSQL v11 newly supports hash-partitioning and partition-wise JOINs. The diagram below shows a range-partitioning configuration with date -type key values. A record which has 2018-05-30 as key is distributed to the partition child table tbl_2018 , in the same way, a record which has 2014-03-21 is distributed to the partition child table tbl_2014 , and so on. In case when scan qualifier WHERE ymd > '2016-07-01'::date is added on scan of the partitioned table for example, it is obvious that tbl_2014 and tbl_2015 contains no records to match, therefore, PostgreSQL' optimizer constructs query execution plan which runs on only tbl_2016 , tbl_2017 and tbl_2018 then merges their results by Append node. It shall perform as if records are read from one logical table. When PG-Strom is used with table partitioning of PostgreSQL together, its optimizer may choose GpuScan to scan the individual partition child tables to be scanned, in the result of cost estimation. In this case, Append node merges the results of GpuScan . On the other hands, if query runs JOIN or GROUP BY, which can be accelerated by PG-Strom, next to the scan on partitioned table, it needs consideration from the standpoint of performance optimization. For example, in case when query scans non-partitioned table then runs JOIN with other tables and GROUP BY, under some conditions, it can handle step-step data exchange on GPU device memory. It is an optimal workload for PG-Strom due to minimized data exchange between GPU and CPU. In case when query runs corresponding workload on the partitioned table, it is problematic that Append node is injected into between the child tables scan and JOIN/GROUP BY. Under the query execution plan, the result of GpuScan must be written back to the host system, then Append merges them and send back the data to GPU to run the following GpuJoin and GpuPreAgg. It is never efficient query execution. The example below shows a query execution plan to the query which includes JOIN and GROUP BY towards the partitioned table pt by the key field ymd of date type; per year distribution. Due to the scan qualification, it omits scan on the partition child tables for 2016 or prior, in addition, a combined JOIN and GROUP BY on the pt_2017 , pt_2018 and pt_2019 shall be executed prior to the Append . # EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- HashAggregate (cost=196410.07..196412.57 rows=200 width=48) Group Key: pt_2017.cat -> Gather (cost=66085.69..196389.07 rows=1200 width=72) Workers Planned: 2 -> Parallel Append (cost=65085.69..195269.07 rows=600 width=72) -> Parallel Custom Scan (GpuPreAgg) (cost=65085.69..65089.69 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2017 (cost=32296.64..74474.20 rows=1050772 width=40) Outer Scan: pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050772...2521854) HashKeys: pt_2017.aid JoinQuals: (pt_2017.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65078.35..65082.35 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2018 (cost=32296.65..74465.75 rows=1050649 width=40) Outer Scan: pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050649...2521557) HashKeys: pt_2018.aid JoinQuals: (pt_2018.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65093.03..65097.03 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2019 (cost=32296.65..74482.64 rows=1050896 width=40) Outer Scan: pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050896...2522151) HashKeys: pt_2019.aid JoinQuals: (pt_2019.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (38 rows) Configuration and Operation By the GUC parameters below, PG-Strom enables/disables the push-down of JOIN/GROUP BY under the partition child tables. Parameter Type Default Description pg_strom.enable_partitionwise_gpujoin bool on Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.enable_partitionwise_gpupreagg bool on Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later. Default of the parameters are on . Once set to off , push-down is disabled. The query execution plan is changed as follows, by EXPLAIN command for the query above section. It uses GpuScan to scan the partition child tables, however, their results are once written back to the host system, then merged by Append and moved to GPU again to process GpuJoin . postgres=# set pg_strom.enable_partitionwise_gpujoin = off; SET postgres=# set pg_strom.enable_partitionwise_gpupreagg = off; SET postgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date group by cat; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- Finalize GroupAggregate (cost=341392.92..341399.42 rows=200 width=48) Group Key: pt.cat -> Sort (cost=341392.92..341393.92 rows=400 width=72) Sort Key: pt.cat -> Gather (cost=341333.63..341375.63 rows=400 width=72) Workers Planned: 2 -> Partial HashAggregate (cost=340333.63..340335.63 rows=200 width=72) Group Key: pt.cat -> Parallel Custom Scan (GpuJoin) (cost=283591.92..283591.92 rows=7565562 width=40) Depth 1: GpuHashJoin (nrows 3152318...7565562) HashKeys: pt.aid JoinQuals: (pt.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Append (cost=28540.80..200673.34 rows=3152318 width=36) -> Parallel Custom Scan (GpuScan) on pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (21 rows) Consideration for SSD/GPU location Limitations Experimental Feature It is an experimental feature to push down GpuJoin and GpuPreAgg to the partitioned child tables, so it may lead unexpected behavior or system crash. In such case, disable the feature using pg_strom.enable_partitionwise_gpujoin or pg_strom.enable_partitionwise_gpupreagg . And report your case to PG-Strom Issues .","title":"Partitioning"},{"location":"partition/#partitioning","text":"This chapter introduces the way to use PG-Strom and the partitioning feature of PostgreSQL. Note that this chapter is only valid when PG-Strom works on PostgreSQL v11 or later . Also see PostgreSQL Document: Table Partitioning for more details of the partitioning feature of PostgreSQL.","title":"Partitioning"},{"location":"partition/#overview","text":"PostgreSQL v10 newly support table partitioning. This mechanism splits one logically large table into physically small pieces. It is valuable because it can skip partitioned child tables which is obviously unnecessary to scan from the search qualification, and it can offer broader I/O bandwidth by physically distributed storage and so on. PostgreSQL v10 supports two kinds of them: range-partitioning and list-partitioning. Then, PostgreSQL v11 newly supports hash-partitioning and partition-wise JOINs. The diagram below shows a range-partitioning configuration with date -type key values. A record which has 2018-05-30 as key is distributed to the partition child table tbl_2018 , in the same way, a record which has 2014-03-21 is distributed to the partition child table tbl_2014 , and so on. In case when scan qualifier WHERE ymd > '2016-07-01'::date is added on scan of the partitioned table for example, it is obvious that tbl_2014 and tbl_2015 contains no records to match, therefore, PostgreSQL' optimizer constructs query execution plan which runs on only tbl_2016 , tbl_2017 and tbl_2018 then merges their results by Append node. It shall perform as if records are read from one logical table. When PG-Strom is used with table partitioning of PostgreSQL together, its optimizer may choose GpuScan to scan the individual partition child tables to be scanned, in the result of cost estimation. In this case, Append node merges the results of GpuScan . On the other hands, if query runs JOIN or GROUP BY, which can be accelerated by PG-Strom, next to the scan on partitioned table, it needs consideration from the standpoint of performance optimization. For example, in case when query scans non-partitioned table then runs JOIN with other tables and GROUP BY, under some conditions, it can handle step-step data exchange on GPU device memory. It is an optimal workload for PG-Strom due to minimized data exchange between GPU and CPU. In case when query runs corresponding workload on the partitioned table, it is problematic that Append node is injected into between the child tables scan and JOIN/GROUP BY. Under the query execution plan, the result of GpuScan must be written back to the host system, then Append merges them and send back the data to GPU to run the following GpuJoin and GpuPreAgg. It is never efficient query execution. The example below shows a query execution plan to the query which includes JOIN and GROUP BY towards the partitioned table pt by the key field ymd of date type; per year distribution. Due to the scan qualification, it omits scan on the partition child tables for 2016 or prior, in addition, a combined JOIN and GROUP BY on the pt_2017 , pt_2018 and pt_2019 shall be executed prior to the Append . # EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date GROUP BY cat; QUERY PLAN -------------------------------------------------------------------------------- HashAggregate (cost=196410.07..196412.57 rows=200 width=48) Group Key: pt_2017.cat -> Gather (cost=66085.69..196389.07 rows=1200 width=72) Workers Planned: 2 -> Parallel Append (cost=65085.69..195269.07 rows=600 width=72) -> Parallel Custom Scan (GpuPreAgg) (cost=65085.69..65089.69 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2017 (cost=32296.64..74474.20 rows=1050772 width=40) Outer Scan: pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050772...2521854) HashKeys: pt_2017.aid JoinQuals: (pt_2017.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65078.35..65082.35 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2018 (cost=32296.65..74465.75 rows=1050649 width=40) Outer Scan: pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050649...2521557) HashKeys: pt_2018.aid JoinQuals: (pt_2018.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) -> Parallel Custom Scan (GpuPreAgg) (cost=65093.03..65097.03 rows=200 width=72) Reduction: Local Combined GpuJoin: enabled -> Parallel Custom Scan (GpuJoin) on pt_2019 (cost=32296.65..74482.64 rows=1050896 width=40) Outer Scan: pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) Outer Scan Filter: (ymd > '2017-01-01'::date) Depth 1: GpuHashJoin (nrows 1050896...2522151) HashKeys: pt_2019.aid JoinQuals: (pt_2019.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (38 rows)","title":"Overview"},{"location":"partition/#configuration-and-operation","text":"By the GUC parameters below, PG-Strom enables/disables the push-down of JOIN/GROUP BY under the partition child tables. Parameter Type Default Description pg_strom.enable_partitionwise_gpujoin bool on Enables/disables whether GpuJoin is pushed down to the partition children. Available only PostgreSQL v10 or later. pg_strom.enable_partitionwise_gpupreagg bool on Enables/disables whether GpuPreAgg is pushed down to the partition children. Available only PostgreSQL v10 or later. Default of the parameters are on . Once set to off , push-down is disabled. The query execution plan is changed as follows, by EXPLAIN command for the query above section. It uses GpuScan to scan the partition child tables, however, their results are once written back to the host system, then merged by Append and moved to GPU again to process GpuJoin . postgres=# set pg_strom.enable_partitionwise_gpujoin = off; SET postgres=# set pg_strom.enable_partitionwise_gpupreagg = off; SET postgres=# EXPLAIN SELECT cat,count(*),avg(ax) FROM pt NATURAL JOIN t1 WHERE ymd > '2017-01-01'::date group by cat; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- Finalize GroupAggregate (cost=341392.92..341399.42 rows=200 width=48) Group Key: pt.cat -> Sort (cost=341392.92..341393.92 rows=400 width=72) Sort Key: pt.cat -> Gather (cost=341333.63..341375.63 rows=400 width=72) Workers Planned: 2 -> Partial HashAggregate (cost=340333.63..340335.63 rows=200 width=72) Group Key: pt.cat -> Parallel Custom Scan (GpuJoin) (cost=283591.92..283591.92 rows=7565562 width=40) Depth 1: GpuHashJoin (nrows 3152318...7565562) HashKeys: pt.aid JoinQuals: (pt.aid = t1.aid) KDS-Hash (size: 10.78MB) -> Append (cost=28540.80..200673.34 rows=3152318 width=36) -> Parallel Custom Scan (GpuScan) on pt_2017 (cost=28540.80..66891.11 rows=1050772 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2018 (cost=28540.81..66883.43 rows=1050649 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Parallel Custom Scan (GpuScan) on pt_2019 (cost=28540.80..66898.79 rows=1050896 width=36) GPU Filter: (ymd > '2017-01-01'::date) -> Seq Scan on t1 (cost=0.00..1935.00 rows=100000 width=12) (21 rows)","title":"Configuration and Operation"},{"location":"partition/#consideration-for-ssdgpu-location","text":"","title":"Consideration for SSD/GPU location"},{"location":"partition/#limitations","text":"Experimental Feature It is an experimental feature to push down GpuJoin and GpuPreAgg to the partitioned child tables, so it may lead unexpected behavior or system crash. In such case, disable the feature using pg_strom.enable_partitionwise_gpujoin or pg_strom.enable_partitionwise_gpupreagg . And report your case to PG-Strom Issues .","title":"Limitations"},{"location":"pinned_buffer/","text":"Pinned Inner Buffer This chapter introduces the Pinned Inner Buffer feature, a technology that improves efficiency of large tables join using GPU-Join. Overview Look at the EXPLAIN output below. When PG-Strom joins tables, it usually reads the largest table ( lineorder in this case; called the OUTER table) asynchronously, while performing join processing and aggregation processing with other tables. Let's proceed. Due to the constraints of the JOIN algorithm, it is necessary to read other tables ( date1 , part , supplier in this case; called the INNER tables) into memory in advance, and also calculate the hash value of the JOIN key. Although these tables are not as large as the OUTER table, preparing an INNER buffer that exceeds several GB is a heavy process. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN --------------------------------------------------------------------------------------------------------------- GroupAggregate (cost=31007186.70..31023043.21 rows=6482 width=46) Group Key: date1.d_year, part.p_brand1 -> Sort (cost=31007186.70..31011130.57 rows=1577548 width=20) Sort Key: date1.d_year, part.p_brand1 -> Custom Scan (GpuJoin) on lineorder (cost=275086.19..30844784.03 rows=1577548 width=20) GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue GPU Join Quals [1]: (part.p_partkey = lineorder.lo_partkey) ... [nrows: 5994236000 -> 7804495] GPU Outer Hash [1]: lineorder.lo_partkey GPU Inner Hash [1]: part.p_partkey GPU Join Quals [2]: (supplier.s_suppkey = lineorder.lo_suppkey) ... [nrows: 7804495 -> 1577548] GPU Outer Hash [2]: lineorder.lo_suppkey GPU Inner Hash [2]: supplier.s_suppkey GPU Join Quals [3]: (date1.d_datekey = lineorder.lo_orderdate) ... [nrows: 1577548 -> 1577548] GPU Outer Hash [3]: lineorder.lo_orderdate GPU Inner Hash [3]: date1.d_datekey GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on part (cost=0.00..59258.00 rows=2604 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Custom Scan (GpuScan) on supplier (cost=100.00..190348.83 rows=2019384 width=6) GPU Projection: s_suppkey GPU Pinned Buffer: enabled GPU Scan Quals: (s_region = 'ASIA'::bpchar) [rows: 9990357 -> 2019384] GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (24 rows) GpuJoin usually reads the INNER table through the PostgreSQL API row-by-row, calculates its hash value, and writes them to the INNER buffer on the host shared memory. The GPU-Service process transfers this INNER buffer onto the GPU device memory, then we can start reading the OUTER table and processing the JOIN with inner tables. If the INNER table is relatively large and contains search conditions that are executable on the GPU, GpuScan may exists under GpuJoin, as in the EXPLAIN output below. In this case, the INNER table is once processed on the GPU by GpuScan, the execution results are returned to the CPU, and then written to the INNER buffer before it is loaded onto the GPU again. It looks like there is quite a bit of wasted data flow. In this way, if data ping-pong occurs between the CPU and GPU when reading the INNER table or building the INNER buffer, you can configure GPUJoin to use Pinned Inner Buffer . It is possible to shorten the execution start lead time and reduce memory usage. In the above EXPLAIN output, reading of the supplier table will be performed by GpuScan, and according to the statistical information, it is estimated that about 2 million rows will be read from the table. Meanwhile, notice the output of GPU Pinned Buffer: enabled . This is a function that if the estimated size of the INNER table exceeds the configuration value of pg_strom.pinned_inner_buffer_threshold , the processing result of GpuScan is retained in the GPU memory and used as part of the INNER buffer at the next GpuJoin. (If necessary, hash value calculation is also performed on the GPU). Therefore, after the contents of the supplier table are read from storage to the GPU using GPU-Direct SQL, they can be used in the next GPUJoin without being returned to the CPU or loaded to the GPU again. However, there are some points to be aware of. To use Pinned Inner Buffer, CPU-Fallback must be disabled. CPU-Fallback is a function that writes back to the CPU data that could not be processed by the GPU and re-executes it. For example, a conditional expression that references TOASTed variable-length data cannot be executed by the GPU in principle, so this function is used to write it back to the CPU and re-execute it. However, if a CPU-Fallback occurs while executing GpuScan, it cannot be guaranteed that the result buffer in the GPU memory (which is used as the INNER buffer for GpuJoin) is a complete result set. In addition, if a CPU-Fallback occurs when executing GpuJoin that uses Pinned Inner Buffer, the CPU cannot execute the fallback process because it does not have the INNER buffer required for JOIN in the first place. Therefore, to use Pinned Inner Buffer, it is necessary to disable CPU-Fallback by specifying SET pg_strom.cpu_fallback = off . This is the same reason why GPU-Sort also requires disabling CPU-Fallback processing. in case multi-GPUs In many systems, the capacity of GPU RAM is limited compared to the host system RAM, and there are also constraints on the size of the hash table. This limitation can be alleviated by splitting the hash table across multiple GPUs, but if an INNER row located on one GPU is referenced while a JOIN is being executed on another GPU, a phenomenon known as GPU memory thrashing occurs, resulting in a significant slowdown in speed. Therefore, a mechanism is needed to ensure locality of memory access while GPU-Join is being executed. In a multi-GPU system, the Pinned Inner Buffer works as follows: If the INNER table scan process is executed on multiple GPUs prior to GPU-Join and the results of that process are stored in GPU memory to build a hash table, it is completely random which rows are on each GPU. If the rows read from the OUTER side in the next step, the Hash-Join process, are first joined with the INNER row on GPU1, then with the INNER row on GPU2, and finally with the INNER row on GPU0, extreme thrashing will occur, causing a severe drop in performance. For this reason, when using the Pinned-Inner-Buffer in a multi-GPU system, a reconstruction process is inserted and the hash table is reallocated to the appropriate GPU. For example, in a system equipped with three GPUs, if the size of the hash table fits roughly into the RAM of the three GPUs, after the GPU-Scan of the INNER table is completed, the hash value of the join key to be used in the next GPU-Join is calculated, and if the remainder when dividing this by 3 is 0, it is reallocated to GPU0, if it is 1 then it is reallocated to GPU1, and if it is 2 then it is reallocated to GPU2. By inserting this process, it is possible to create a state in which when GPU-Join is executed on GPU0, the hash table will only contain INNER rows whose remainder when the hash value is divided by 3 is 0, and similarly, GPU1 will only contain INNER rows whose remainder when the hash value is divided by 3 is 1. Next, when GPU-Join is executed using this divided hash table, when the GPU that first loaded data from the OUTER table (let's call it GPU2 here) references the hash table, if the remainder when dividing the hash value calculated from the OUTER row by 3 is other than 2, then there will obviously be no matching INNER row on that GPU. Therefore, GPU2 will generate a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 2. Next, this OUTER data is transferred to GPU1 by GPU-to-GPU Copy, which generates a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 1. By repeating this process, \"partial Hash-Join results\" are generated on each GPU, but the combination of these is equal to the complete Hash-Join result, and as a result, it is now possible to execute GPU-Join even if the INNER hash table is larger in size than the GPU's on-board RAM. In relation to this feature, the pg_strom.pinned_inner_buffer_partition_size parameter has been added. This specifies the threshold size for dividing the Pinned-Inner-Buffer among multiple GPUs. The initial value is set to about 80-90% of the GPU's installed memory, so administrators usually do not need to change this.","title":"Pinned Inner Buffer"},{"location":"pinned_buffer/#pinned-inner-buffer","text":"This chapter introduces the Pinned Inner Buffer feature, a technology that improves efficiency of large tables join using GPU-Join.","title":"Pinned Inner Buffer"},{"location":"pinned_buffer/#overview","text":"Look at the EXPLAIN output below. When PG-Strom joins tables, it usually reads the largest table ( lineorder in this case; called the OUTER table) asynchronously, while performing join processing and aggregation processing with other tables. Let's proceed. Due to the constraints of the JOIN algorithm, it is necessary to read other tables ( date1 , part , supplier in this case; called the INNER tables) into memory in advance, and also calculate the hash value of the JOIN key. Although these tables are not as large as the OUTER table, preparing an INNER buffer that exceeds several GB is a heavy process. =# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228' and s_region = 'ASIA' group by d_year, p_brand1; QUERY PLAN --------------------------------------------------------------------------------------------------------------- GroupAggregate (cost=31007186.70..31023043.21 rows=6482 width=46) Group Key: date1.d_year, part.p_brand1 -> Sort (cost=31007186.70..31011130.57 rows=1577548 width=20) Sort Key: date1.d_year, part.p_brand1 -> Custom Scan (GpuJoin) on lineorder (cost=275086.19..30844784.03 rows=1577548 width=20) GPU Projection: date1.d_year, part.p_brand1, lineorder.lo_revenue GPU Join Quals [1]: (part.p_partkey = lineorder.lo_partkey) ... [nrows: 5994236000 -> 7804495] GPU Outer Hash [1]: lineorder.lo_partkey GPU Inner Hash [1]: part.p_partkey GPU Join Quals [2]: (supplier.s_suppkey = lineorder.lo_suppkey) ... [nrows: 7804495 -> 1577548] GPU Outer Hash [2]: lineorder.lo_suppkey GPU Inner Hash [2]: supplier.s_suppkey GPU Join Quals [3]: (date1.d_datekey = lineorder.lo_orderdate) ... [nrows: 1577548 -> 1577548] GPU Outer Hash [3]: lineorder.lo_orderdate GPU Inner Hash [3]: date1.d_datekey GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on part (cost=0.00..59258.00 rows=2604 width=14) Filter: ((p_brand1 >= 'MFGR#2221'::bpchar) AND (p_brand1 <= 'MFGR#2228'::bpchar)) -> Custom Scan (GpuScan) on supplier (cost=100.00..190348.83 rows=2019384 width=6) GPU Projection: s_suppkey GPU Pinned Buffer: enabled GPU Scan Quals: (s_region = 'ASIA'::bpchar) [rows: 9990357 -> 2019384] GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 (cost=0.00..72.56 rows=2556 width=8) (24 rows) GpuJoin usually reads the INNER table through the PostgreSQL API row-by-row, calculates its hash value, and writes them to the INNER buffer on the host shared memory. The GPU-Service process transfers this INNER buffer onto the GPU device memory, then we can start reading the OUTER table and processing the JOIN with inner tables. If the INNER table is relatively large and contains search conditions that are executable on the GPU, GpuScan may exists under GpuJoin, as in the EXPLAIN output below. In this case, the INNER table is once processed on the GPU by GpuScan, the execution results are returned to the CPU, and then written to the INNER buffer before it is loaded onto the GPU again. It looks like there is quite a bit of wasted data flow. In this way, if data ping-pong occurs between the CPU and GPU when reading the INNER table or building the INNER buffer, you can configure GPUJoin to use Pinned Inner Buffer . It is possible to shorten the execution start lead time and reduce memory usage. In the above EXPLAIN output, reading of the supplier table will be performed by GpuScan, and according to the statistical information, it is estimated that about 2 million rows will be read from the table. Meanwhile, notice the output of GPU Pinned Buffer: enabled . This is a function that if the estimated size of the INNER table exceeds the configuration value of pg_strom.pinned_inner_buffer_threshold , the processing result of GpuScan is retained in the GPU memory and used as part of the INNER buffer at the next GpuJoin. (If necessary, hash value calculation is also performed on the GPU). Therefore, after the contents of the supplier table are read from storage to the GPU using GPU-Direct SQL, they can be used in the next GPUJoin without being returned to the CPU or loaded to the GPU again. However, there are some points to be aware of. To use Pinned Inner Buffer, CPU-Fallback must be disabled. CPU-Fallback is a function that writes back to the CPU data that could not be processed by the GPU and re-executes it. For example, a conditional expression that references TOASTed variable-length data cannot be executed by the GPU in principle, so this function is used to write it back to the CPU and re-execute it. However, if a CPU-Fallback occurs while executing GpuScan, it cannot be guaranteed that the result buffer in the GPU memory (which is used as the INNER buffer for GpuJoin) is a complete result set. In addition, if a CPU-Fallback occurs when executing GpuJoin that uses Pinned Inner Buffer, the CPU cannot execute the fallback process because it does not have the INNER buffer required for JOIN in the first place. Therefore, to use Pinned Inner Buffer, it is necessary to disable CPU-Fallback by specifying SET pg_strom.cpu_fallback = off . This is the same reason why GPU-Sort also requires disabling CPU-Fallback processing.","title":"Overview"},{"location":"pinned_buffer/#in-case-multi-gpus","text":"In many systems, the capacity of GPU RAM is limited compared to the host system RAM, and there are also constraints on the size of the hash table. This limitation can be alleviated by splitting the hash table across multiple GPUs, but if an INNER row located on one GPU is referenced while a JOIN is being executed on another GPU, a phenomenon known as GPU memory thrashing occurs, resulting in a significant slowdown in speed. Therefore, a mechanism is needed to ensure locality of memory access while GPU-Join is being executed. In a multi-GPU system, the Pinned Inner Buffer works as follows: If the INNER table scan process is executed on multiple GPUs prior to GPU-Join and the results of that process are stored in GPU memory to build a hash table, it is completely random which rows are on each GPU. If the rows read from the OUTER side in the next step, the Hash-Join process, are first joined with the INNER row on GPU1, then with the INNER row on GPU2, and finally with the INNER row on GPU0, extreme thrashing will occur, causing a severe drop in performance. For this reason, when using the Pinned-Inner-Buffer in a multi-GPU system, a reconstruction process is inserted and the hash table is reallocated to the appropriate GPU. For example, in a system equipped with three GPUs, if the size of the hash table fits roughly into the RAM of the three GPUs, after the GPU-Scan of the INNER table is completed, the hash value of the join key to be used in the next GPU-Join is calculated, and if the remainder when dividing this by 3 is 0, it is reallocated to GPU0, if it is 1 then it is reallocated to GPU1, and if it is 2 then it is reallocated to GPU2. By inserting this process, it is possible to create a state in which when GPU-Join is executed on GPU0, the hash table will only contain INNER rows whose remainder when the hash value is divided by 3 is 0, and similarly, GPU1 will only contain INNER rows whose remainder when the hash value is divided by 3 is 1. Next, when GPU-Join is executed using this divided hash table, when the GPU that first loaded data from the OUTER table (let's call it GPU2 here) references the hash table, if the remainder when dividing the hash value calculated from the OUTER row by 3 is other than 2, then there will obviously be no matching INNER row on that GPU. Therefore, GPU2 will generate a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 2. Next, this OUTER data is transferred to GPU1 by GPU-to-GPU Copy, which generates a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 1. By repeating this process, \"partial Hash-Join results\" are generated on each GPU, but the combination of these is equal to the complete Hash-Join result, and as a result, it is now possible to execute GPU-Join even if the INNER hash table is larger in size than the GPU's on-board RAM. In relation to this feature, the pg_strom.pinned_inner_buffer_partition_size parameter has been added. This specifies the threshold size for dividing the Pinned-Inner-Buffer among multiple GPUs. The initial value is set to about 80-90% of the GPU's installed memory, so administrators usually do not need to change this.","title":"in case multi-GPUs"},{"location":"postgis/","text":"GPU-PostGIS This chapter describes GPU-PostGIS Overview PostGIS is an extension to PostgreSQL to utilize geographic information. PostGIS provides data type ( Geometry ) for handling geographic data such as points, lines, and polygons, as well as a large number of functions and operators for evaluating geographic data elements, such as distance calculation, inclusion, and intersection determination. In addition, some of the operators can search faster by the R-Tree using GiST(Generalized Search Tree) mechanism included in PostgreSQL. Since the first version was released in 2001, it has been enhanced and maintained by the developer community for over 20 years. These functions and operators provided by PostGIS are very large, over 500 in total. For this reason, PG-Strom has ported only a few relatively frequently used PostGIS functions to the GPU. For example: geometry st_point(float8 lon,float8 lat) returns a point with the given longitude and latitude as a Point of Geometry type. bool st_contains(geometry a,geometry b) determines if the geometry a contains the geometry b or not. bool st_crosses(geometry,geometry) determines if the geometries intersect each other. text st_relate(geometry,geometry) returns the relationship between geometries as a matrix representation of DE-9IM(Dimensionally Extended 9-Intersection Model) . PostGIS Usage You can use GPU-PostGIS without any configurations. PG-Strom will automatically determine if the PostGIS functions used in the query are executable on the GPU when PostGIS is installed from the package or the source code and the geometry data types and PostGIS functions are defined using the CREATE EXTENSION syntax. Please refer to the PostGIS documentaion for installation. For example, the following query uses the GPU-executable PostGIS funtion st_contains() and st_makepoint() to determine if a two-dimensional point read from the table is contained within the range of the geometry type constant 'polygon ((10 10,30 10,30 20,10 20,10 10))' . As you can see from the fact that these functions are listed as part of the \"GPU Filter:\", PG-Strom will automatically detect supported PostGIS functions and attempt to run them on the GPU as much as possible. =# explain select * from dpoints where st_contains('polygon ((10 10,30 10,30 20,10 20,10 10))', st_makepoint(x,y)); QUERY PLAN ------------------------------------------------------------------------------------------ Custom Scan (GpuScan) on dpoints (cost=1397205.10..12627630.76 rows=800 width=28) GPU Filter: st_contains('01030000000100000005000000000000000000244000000000000024400000000000003E4000000000000024400000000000003E4000000000000034400000000000002440000000000000344000000000000024400000000000002440'::geometry, st_makepoint(x, y)) GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) (3 rows) GiST Index Some of the PostGIS functions that evaluate relationships between geometries, such as st_contains() and st_crosses() , support the GiST index (R-Tree), which enables fast refinement of the search using only the CPU. GpuJoin in PG-Strom sometimes transfers not only the contents of the table but also GiST index (R-Tree) to filter the rows to be joined fast when the join condition between tables can be accelerated. This process is usually executed at a much higher parallelism level than the CPU, so a significant speedup can be expected. On the other hand, GpuScan does not use GiST index to scan a single table. This is because IndexScan filtering by CPU is often faster. The following is an example of a SQL statement to create a GiST index on city boundary data (\"geom\" column of \"giscity\" table). =# CREATE INDEX on giscity USING gist (geom); CREATE INDEX The following is an execution plan of SQL that joins municipal boundary data (\"giscity\" table) and latitude and longitude data (\"dpoints\" table) and outputs the number of latitude and longitude data (points) contained in the area expressed as polygons for each municipality. The optimizer selects GpuJoin, and GpuGiSTJoin to join \"giscity\" table with \"dpoints\" table. The \"IndexFilter:\" line shows that the filtering condition on the GiST index is (g.geom ~ st_makepoint(d.x, d.y)) and the index giscity_geom_idx will be used. The execution of PostGIS functions is a relatively \"heavy\" process even for GPU. By using GiST index, we can eliminate combinations that obviously do not match the condition and speed up the search process significantly. =# EXPLAIN SELECT pref, city, count(*) FROM giscity g, dpoints d WHERE pref = 'Tokyo' AND st_contains(g.geom,st_makepoint(d.x, d.y)) GROUP BY pref, city; QUERY PLAN ----------------------------------------------------------------------------------------------------------- GroupAggregate (cost=5700646.35..5700759.39 rows=5024 width=29) Group Key: g.n03_001, g.n03_004 -> Sort (cost=5700646.35..5700658.91 rows=5024 width=29) Sort Key: g.n03_004 -> Custom Scan (GpuPreAgg) (cost=5700274.71..5700337.51 rows=5024 width=29) Reduction: Local Combined GpuJoin: enabled GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on dpoints d (cost=638671.58..5668511.23 rows=50821573 width=21) Outer Scan: dpoints d (cost=0.00..141628.18 rows=7999618 width=16) Depth 1: GpuGiSTJoin(nrows 7999618...50821573) HeapSize: 3251.36KB IndexFilter: (g.geom ~ st_makepoint(d.x, d.y)) on giscity_geom_idx JoinQuals: st_contains(g.geom, st_makepoint(d.x, d.y)) GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) -> Seq Scan on giscity g (cost=0.00..8929.24 rows=6353 width=1883) Filter: ((pref)::text = 'Tokyo'::text) (17 rows)","title":"PostGIS"},{"location":"postgis/#gpu-postgis","text":"This chapter describes GPU-PostGIS","title":"GPU-PostGIS"},{"location":"postgis/#overview","text":"PostGIS is an extension to PostgreSQL to utilize geographic information. PostGIS provides data type ( Geometry ) for handling geographic data such as points, lines, and polygons, as well as a large number of functions and operators for evaluating geographic data elements, such as distance calculation, inclusion, and intersection determination. In addition, some of the operators can search faster by the R-Tree using GiST(Generalized Search Tree) mechanism included in PostgreSQL. Since the first version was released in 2001, it has been enhanced and maintained by the developer community for over 20 years. These functions and operators provided by PostGIS are very large, over 500 in total. For this reason, PG-Strom has ported only a few relatively frequently used PostGIS functions to the GPU. For example: geometry st_point(float8 lon,float8 lat) returns a point with the given longitude and latitude as a Point of Geometry type. bool st_contains(geometry a,geometry b) determines if the geometry a contains the geometry b or not. bool st_crosses(geometry,geometry) determines if the geometries intersect each other. text st_relate(geometry,geometry) returns the relationship between geometries as a matrix representation of DE-9IM(Dimensionally Extended 9-Intersection Model) .","title":"Overview"},{"location":"postgis/#postgis-usage","text":"You can use GPU-PostGIS without any configurations. PG-Strom will automatically determine if the PostGIS functions used in the query are executable on the GPU when PostGIS is installed from the package or the source code and the geometry data types and PostGIS functions are defined using the CREATE EXTENSION syntax. Please refer to the PostGIS documentaion for installation. For example, the following query uses the GPU-executable PostGIS funtion st_contains() and st_makepoint() to determine if a two-dimensional point read from the table is contained within the range of the geometry type constant 'polygon ((10 10,30 10,30 20,10 20,10 10))' . As you can see from the fact that these functions are listed as part of the \"GPU Filter:\", PG-Strom will automatically detect supported PostGIS functions and attempt to run them on the GPU as much as possible. =# explain select * from dpoints where st_contains('polygon ((10 10,30 10,30 20,10 20,10 10))', st_makepoint(x,y)); QUERY PLAN ------------------------------------------------------------------------------------------ Custom Scan (GpuScan) on dpoints (cost=1397205.10..12627630.76 rows=800 width=28) GPU Filter: st_contains('01030000000100000005000000000000000000244000000000000024400000000000003E4000000000000024400000000000003E4000000000000034400000000000002440000000000000344000000000000024400000000000002440'::geometry, st_makepoint(x, y)) GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) (3 rows)","title":"PostGIS Usage"},{"location":"postgis/#gist-index","text":"Some of the PostGIS functions that evaluate relationships between geometries, such as st_contains() and st_crosses() , support the GiST index (R-Tree), which enables fast refinement of the search using only the CPU. GpuJoin in PG-Strom sometimes transfers not only the contents of the table but also GiST index (R-Tree) to filter the rows to be joined fast when the join condition between tables can be accelerated. This process is usually executed at a much higher parallelism level than the CPU, so a significant speedup can be expected. On the other hand, GpuScan does not use GiST index to scan a single table. This is because IndexScan filtering by CPU is often faster. The following is an example of a SQL statement to create a GiST index on city boundary data (\"geom\" column of \"giscity\" table). =# CREATE INDEX on giscity USING gist (geom); CREATE INDEX The following is an execution plan of SQL that joins municipal boundary data (\"giscity\" table) and latitude and longitude data (\"dpoints\" table) and outputs the number of latitude and longitude data (points) contained in the area expressed as polygons for each municipality. The optimizer selects GpuJoin, and GpuGiSTJoin to join \"giscity\" table with \"dpoints\" table. The \"IndexFilter:\" line shows that the filtering condition on the GiST index is (g.geom ~ st_makepoint(d.x, d.y)) and the index giscity_geom_idx will be used. The execution of PostGIS functions is a relatively \"heavy\" process even for GPU. By using GiST index, we can eliminate combinations that obviously do not match the condition and speed up the search process significantly. =# EXPLAIN SELECT pref, city, count(*) FROM giscity g, dpoints d WHERE pref = 'Tokyo' AND st_contains(g.geom,st_makepoint(d.x, d.y)) GROUP BY pref, city; QUERY PLAN ----------------------------------------------------------------------------------------------------------- GroupAggregate (cost=5700646.35..5700759.39 rows=5024 width=29) Group Key: g.n03_001, g.n03_004 -> Sort (cost=5700646.35..5700658.91 rows=5024 width=29) Sort Key: g.n03_004 -> Custom Scan (GpuPreAgg) (cost=5700274.71..5700337.51 rows=5024 width=29) Reduction: Local Combined GpuJoin: enabled GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) -> Custom Scan (GpuJoin) on dpoints d (cost=638671.58..5668511.23 rows=50821573 width=21) Outer Scan: dpoints d (cost=0.00..141628.18 rows=7999618 width=16) Depth 1: GpuGiSTJoin(nrows 7999618...50821573) HeapSize: 3251.36KB IndexFilter: (g.geom ~ st_makepoint(d.x, d.y)) on giscity_geom_idx JoinQuals: st_contains(g.geom, st_makepoint(d.x, d.y)) GPU Preference: GPU0 (NVIDIA Tesla V100-PCIE-16GB) -> Seq Scan on giscity g (cost=0.00..8929.24 rows=6353 width=1883) Filter: ((pref)::text = 'Tokyo'::text) (17 rows)","title":"GiST Index"},{"location":"ref_devfuncs/","text":"Functions and operators This chapter introduces the functions and operators executable on GPU devices. Type cast bool <-- int4 int1 <-- int2 , int4 , int8 , float2 , float4 , float8 , numeric int2 <-- int1 , int4 , int8 , float2 , float4 , float8 , numeric int4 <-- bool , int1 , int2 , int8 , float2 , float4 , float8 , numeric int8 <-- int1 , int2 , int4 , float2 , float4 , float8 , numeric float2 <-- int1 , int2 , int4 , int8 , float4 , float8 , numeric float4 <-- int1 , int2 , int4 , int8 , float2 , float8 , numeric float8 <-- int1 , int2 , int4 , int8 , float2 , float4 , numeric numeric <-- int1 , int2 , int4 , int8 , float2 , float4 , float8 money <-- int4 , int8 , numeric date <-- timestamp , timestamptz time <-- timetz , timestamp , timestamptz timetz <-- time , timestamptz timestamp <-- date , timestamptz timestamptz <-- date , timestamp Numeric functions/operators bool COMP bool comparison operators of boolean type. COMP is any of =,<> } INT COMP INT comparison operators of integer types. INT is any of int1,int2,int4,int8 . It is acceptable if left side and right side have different interger types. COMP is any of =,<>,<,<=,>=,> } FP COMP FP comparison operators of floating-point types. FP is any of float2,float4,float8 . It is acceptable if left side and right side have different floating-point types. COMP is any of =,<>,<,<=,>=,> } numeric COMP numeric comparison operators of numeric type. COMP is any of =,<>,<,<=,>=,> } INT OP INT arithemetic operators of integer types. INT is any of int1,int2,int4,int8 . It is acceptable if left side and right side have different interger types. OP is any of +,-,*,/ } FP OP FP arithemetic operators of floating-point types. FP is any of float2,float4,float8 . It is acceptable if left side and right side have different floating-point types. COMP is any of +,-,*,/ } numeric OP numeric comparison operators of numeric type. OP is any of +,-,*,/ } INT % INT Reminer operator. INT is any of int1,int2,int4,int8 } INT & INT Bitwise AND operator. INT is any of int1,int2,int4,int8 } INT | INT Bitwise OR operator. INT is any of int1,int2,int4,int8 } INT # INT Bitwise XOR operator. INT is any of int1,int2,int4,int8 } ~ INT Bitwise NOT operator. INT is any of int1,int2,int4,int8 } INT >> int4 Right shift operator. INT is any of int1,int2,int4,int8 } INT << int4 Left shift operator. INT is any of int1,int2,int4,int8 } + TYPE Unary plus operator. TYPE is any of int1,int2,int4,int8,float2,float4,float8,numeric .} - TYPE Unary minus operator. TYPE is any of int1,int2,int4,int8,float2,float4,float8,numeric .} @ TYPE Absolute value. TYPE is any of int1,int2,int4,int8,float2,float4,float8,numeric .} Mathematical functions float8 cbrt(float8) float8 dcbrt(float8) cube root float8 ceil(float8) float8 ceiling(float8) nearest integer greater than or equal to argument float8 exp(float8) float8 dexp(float8) exponential float8 floor(float8) nearest integer less than or equal to argument float8 ln(float8) float8 dlog1(float8) natural logarithm float8 log(float8) float8 dlog10(float8) base 10 logarithm float8 pi() circumference ratio float8 power(float8,float8) float8 pow(float8,float8) float8 dpow(float8,float8) power float8 round(float8) float8 dround(float8) round to the nearest integer float8 sign(float8) sign of the argument float8 sqrt(float8) float8 dsqrt(float8) square root| float8 trunc(float8) float8 dtrunc(float8) truncate toward zero| Trigonometric functions float8 degrees(float8) radians to degrees} float8 radians(float8) degrees to radians} float8 acos(float8) inverse cosine} float8 asin(float8) inverse sine} float8 atan(float8) inverse tangent} float8 atan2(float8,float8) inverse tangent of arg1 / arg2 } float8 cos(float8) cosine} float8 cot(float8) cotangent} float8 sin(float8) sine} float8 tan(float8) tangent} Date and time operators date COMP date comparison operators for date type. COMP is any of =,<>,<,<=,>=,> .} date COMP timestamp timestamp COMP date comparison operators for date and timestamp type. COMP is any of =,<>,<,<=,>=,> .} date COMP timestamptz timestamptz COMP date comparison operators for date and timestamptz type. COMP is any of =,<>,<,<=,>=,> .} time COMP time comparison operators for time type. COMP is any of =,<>,<,<=,>=,> .} timetz COMP timetz comparison operators for timetz type. COMP is any of =,<>,<,<=,>=,> .} timestamp COMP timestamp comparison operators for timestamp type. COMP is any of =,<>,<,<=,>=,> .} timestamptz COMP timestamptz comparison operators for timestamptz type. COMP is any of =,<>,<,<=,>=,> .} timestamp COMP timestamptz timestamptz COMP timestamp comparison operators for timestamp and timestamptz type. COMP is any of =,<>,<,<=,>=,> .} interval COMP interval comparison operators for interval type. COMP is any of =,<>,<,<=,>=,> .} date + int4 int4 + date addition operator of date type} date - int4 subtraction operator of date type} date - date difference between date types} date + time time + date constructs a timestamp from date and time } date + timetz constructs a timestamptz from date and timetz } time - time difference between time types} timestamp - timestamp difference between timestamp types} timetz + interval timetz - interval addition or subtraction operator of timetz by interval .} timestamptz + interval timestamptz - interval addition or subtraction operator of timestamptz by interval .} overlaps(TYPE,TYPE,TYPE,TYPE) checks whether the 2 given time periods overlaps. TYPE is any of time,timetz,timestamp,timestamptz .} extract(text FROM TYPE) retrieves subfields such as day or hour from date/time values. TYPE is any of time,timetz,timestamp,timestamptz,interval .} now() current time of the transaction} - interval unary minus operator of interval type} interval + interval addition operator of interval type} interval - interval subtraction operator of interval type} Text functions/operators {text,bpchar} COMP {text,bpchar} comparison operators; COMP is any of =,<>,<,<=,>=,> Note that <,<=,>=,> operators are valid only when locale is UTF-8 or C (no locale).} substring(text,int4) substring(text,int4,int4) substr(text,int4) substr(text,int4,int4) extracts the substring} length({text,bpchar}) length of the string} {text,bpchar} [NOT] LIKE text pattern-matching according to the LIKE expression} {text,bpchar} [NOT] ILIKE text case-insensitive pattern-matching according to the LIKE expression. Note that ILIKE operator is valid only when locale is UTF-8 or C (no locale).} Network functions/operators macaddr COMP macaddr comparison operators; COMP is any of =,<>,<,<=,>=,> } macaddr & macaddr Bitwise AND operator} macaddr | macaddr Bitwise OR operator} ~ macaddr Bitwise NOT operator} trunc(macaddr) Set last 3 bytes to zero} inet COMP inet comparison operators; COMP is any of =,<>,<,<=,>=,> } inet << inet Left side is contained by right side} inet <<= inet Left side is contained by or equals to right side} inet >> inet Left side contains right side} inet >>= inet Left side contains or is equals to right side} inet && inet Left side contains or is contained by right side} ~ inet Bitwise NOT operator} inet & inet Bitwise AND operator} inet | inet Bitwise OR operator} inet + int8 addition operator} inet - int8 subtraction operator} inet - inet subtraction operator} broadcast(inet) returns the broadcast address of the given network address} family(inet) returns the family of the given network address; 4 for IPv4, and 6 for IPv6} hostmask(inet) extract host mask of the given network address} masklen(inet) extract netmask length of the given network address} netmask(inet) extract netmask of the given network address} network(inet) extract network part of the given network address} set_masklen(NETADDR,int) set netmask length of the given network address; NETADDR is either inet or cidr .} inet_merge(inet,inet) the smallest network which includes both of the given networks} Currency operators money COMP money comparison operators; COMP is any of =,<>,<,<=,>=,> } money OP money arthmetric operators; OP is any of +,-,/ } money * TYPE TYPE * money Multiply a currency with a numeric value; TYPE is any of int2,int4,float2,float4,float8 } money / TYPE Division of a currency by a numeric value; TYPE is any of int2,int4,float2,float4,float8 } money / money Division of currency values} UUID operators uuid COMP uuid comparison operator. COMP is any of =,<>,<,<=,>=,> } JSONB operators jsonb -> KEY Get a JSON object field specified by the KEY } jsonb -> NUM Get a JSON array element indexed by NUM } jsonb ->> KEY Get a JSON object field specified by the KEY , as text} jsonb ->> NUM Get a JSON array element indexed by NUM , as text} (jsonb ->> KEY)::TYPE If TYPE is any of int2,int4,int8,float4,float8,numeric , get a JSON object field specified by KEY , as numeric data type. See the note below.} (jsonb ->> NUM)::TYPE If TYPE is any of int2,int4,int8,float4,float8,numeric Get a JSON array element indexed by NUM , as numeric data type. See the note below.} jsonb ? KEY Check whether jsonb object contains the KEY } Note When we convert a jsonb element fetched by jsonb ->> KEY operator into numerical data types like float or numeric , PostgreSQL takes 2 steps operations; an internal numerical form is printed as text first, then it is converted into numerical data type. PG-Strom optimizes the GPU code using a special device function to fetch a numerical datum from jsonb object/array, if jsonb ->> KEY operator and text-to-numeric case are continuously used. PostGIS Functions geometry st_makepoint(float8,float8) geometry st_point(float8,float8) It makes 2-dimensional POINT geometry. geometry st_makepoint(float8,float8,float8) It makes 3-dimensional POINT geometry. geometry st_makepoint(float8,float8,float8,float8) It makes 4-dimensional POINT geometry. geometry st_setsrid(geometry,int4) It assigns SRID on the given geometry float8 st_distance(geometry,geometry) It returns the distance between geometries in float8 . bool st_dwithin(geometry,geometry,float8) It returns true if the distance between geometries is shorter than the specified threshold. It is often faster than the combination of st_distance and comparison operator. text st_relate(geometry,geometry) It checks intersection of geometries, then returns DE9-IM(Dimensionally Extended Nine-Intersection Matrix) format string. bool st_contains(geometry,geometry) It returns whether the geometry1 fully contains the geometry2. bool st_crosses(geometry,geometry) It returns whether the geometries are crossed. int4 st_linecrossingdirection(geometry,geometry) It checks how two LINESTRING geometries are crossing, or not crossing. CUBE Type Functions cube COMP cube comparison operators; COMP is any of =,<>,<,<=,>=,> } bool cube_contains(cube, cube) It returns whether the first cube fully contains the second cube. bool cube_contained(cube, cube) It returns whether the first cube is fully contained by the second cube. float8 cube_ll_coord(cube, int4) VCF Support Functions text vcf_variant_getattr(text, text) It regards the first argument as a string of tokens separated by ':' and returns the string after KEY= specified in the second argument. =# SELECT vcf_variant_getattr('ABC=123:NM=this is a pen:XYZ=3.1415', 'NM'); vcf_variant_getattr --------------------- this is a pen (1 row) text vcf_info_getattr(text, text) It regards the first argument as a string of tokens separated by ';' and returns the string after KEY= specified in the second argument. =# SELECT vcf_info_getattr('ABC=123;NM=this is a pen;XYZ=3.1415', 'XYZ'); vcf_info_getattr ------------------ 3.141 (1 row)","title":"Functions and Operators"},{"location":"ref_devfuncs/#functions-and-operators","text":"This chapter introduces the functions and operators executable on GPU devices.","title":"Functions and operators"},{"location":"ref_devfuncs/#type-cast","text":"bool <-- int4 int1 <-- int2 , int4 , int8 , float2 , float4 , float8 , numeric int2 <-- int1 , int4 , int8 , float2 , float4 , float8 , numeric int4 <-- bool , int1 , int2 , int8 , float2 , float4 , float8 , numeric int8 <-- int1 , int2 , int4 , float2 , float4 , float8 , numeric float2 <-- int1 , int2 , int4 , int8 , float4 , float8 , numeric float4 <-- int1 , int2 , int4 , int8 , float2 , float8 , numeric float8 <-- int1 , int2 , int4 , int8 , float2 , float4 , numeric numeric <-- int1 , int2 , int4 , int8 , float2 , float4 , float8 money <-- int4 , int8 , numeric date <-- timestamp , timestamptz time <-- timetz , timestamp , timestamptz timetz <-- time , timestamptz timestamp <-- date , timestamptz timestamptz <-- date , timestamp","title":"Type cast"},{"location":"ref_devfuncs/#numeric-functionsoperators","text":"bool COMP bool comparison operators of boolean type. COMP is any of =,<> } INT COMP INT comparison operators of integer types. INT is any of int1,int2,int4,int8 . It is acceptable if left side and right side have different interger types. COMP is any of =,<>,<,<=,>=,> } FP COMP FP comparison operators of floating-point types. FP is any of float2,float4,float8 . It is acceptable if left side and right side have different floating-point types. COMP is any of =,<>,<,<=,>=,> } numeric COMP numeric comparison operators of numeric type. COMP is any of =,<>,<,<=,>=,> } INT OP INT arithemetic operators of integer types. INT is any of int1,int2,int4,int8 . It is acceptable if left side and right side have different interger types. OP is any of +,-,*,/ } FP OP FP arithemetic operators of floating-point types. FP is any of float2,float4,float8 . It is acceptable if left side and right side have different floating-point types. COMP is any of +,-,*,/ } numeric OP numeric comparison operators of numeric type. OP is any of +,-,*,/ } INT % INT Reminer operator. INT is any of int1,int2,int4,int8 } INT & INT Bitwise AND operator. INT is any of int1,int2,int4,int8 } INT | INT Bitwise OR operator. INT is any of int1,int2,int4,int8 } INT # INT Bitwise XOR operator. INT is any of int1,int2,int4,int8 } ~ INT Bitwise NOT operator. INT is any of int1,int2,int4,int8 } INT >> int4 Right shift operator. INT is any of int1,int2,int4,int8 } INT << int4 Left shift operator. INT is any of int1,int2,int4,int8 } + TYPE Unary plus operator. TYPE is any of int1,int2,int4,int8,float2,float4,float8,numeric .} - TYPE Unary minus operator. TYPE is any of int1,int2,int4,int8,float2,float4,float8,numeric .} @ TYPE Absolute value. TYPE is any of int1,int2,int4,int8,float2,float4,float8,numeric .}","title":"Numeric functions/operators"},{"location":"ref_devfuncs/#mathematical-functions","text":"float8 cbrt(float8) float8 dcbrt(float8) cube root float8 ceil(float8) float8 ceiling(float8) nearest integer greater than or equal to argument float8 exp(float8) float8 dexp(float8) exponential float8 floor(float8) nearest integer less than or equal to argument float8 ln(float8) float8 dlog1(float8) natural logarithm float8 log(float8) float8 dlog10(float8) base 10 logarithm float8 pi() circumference ratio float8 power(float8,float8) float8 pow(float8,float8) float8 dpow(float8,float8) power float8 round(float8) float8 dround(float8) round to the nearest integer float8 sign(float8) sign of the argument float8 sqrt(float8) float8 dsqrt(float8) square root| float8 trunc(float8) float8 dtrunc(float8) truncate toward zero|","title":"Mathematical functions"},{"location":"ref_devfuncs/#trigonometric-functions","text":"float8 degrees(float8) radians to degrees} float8 radians(float8) degrees to radians} float8 acos(float8) inverse cosine} float8 asin(float8) inverse sine} float8 atan(float8) inverse tangent} float8 atan2(float8,float8) inverse tangent of arg1 / arg2 } float8 cos(float8) cosine} float8 cot(float8) cotangent} float8 sin(float8) sine} float8 tan(float8) tangent}","title":"Trigonometric functions"},{"location":"ref_devfuncs/#date-and-time-operators","text":"date COMP date comparison operators for date type. COMP is any of =,<>,<,<=,>=,> .} date COMP timestamp timestamp COMP date comparison operators for date and timestamp type. COMP is any of =,<>,<,<=,>=,> .} date COMP timestamptz timestamptz COMP date comparison operators for date and timestamptz type. COMP is any of =,<>,<,<=,>=,> .} time COMP time comparison operators for time type. COMP is any of =,<>,<,<=,>=,> .} timetz COMP timetz comparison operators for timetz type. COMP is any of =,<>,<,<=,>=,> .} timestamp COMP timestamp comparison operators for timestamp type. COMP is any of =,<>,<,<=,>=,> .} timestamptz COMP timestamptz comparison operators for timestamptz type. COMP is any of =,<>,<,<=,>=,> .} timestamp COMP timestamptz timestamptz COMP timestamp comparison operators for timestamp and timestamptz type. COMP is any of =,<>,<,<=,>=,> .} interval COMP interval comparison operators for interval type. COMP is any of =,<>,<,<=,>=,> .} date + int4 int4 + date addition operator of date type} date - int4 subtraction operator of date type} date - date difference between date types} date + time time + date constructs a timestamp from date and time } date + timetz constructs a timestamptz from date and timetz } time - time difference between time types} timestamp - timestamp difference between timestamp types} timetz + interval timetz - interval addition or subtraction operator of timetz by interval .} timestamptz + interval timestamptz - interval addition or subtraction operator of timestamptz by interval .} overlaps(TYPE,TYPE,TYPE,TYPE) checks whether the 2 given time periods overlaps. TYPE is any of time,timetz,timestamp,timestamptz .} extract(text FROM TYPE) retrieves subfields such as day or hour from date/time values. TYPE is any of time,timetz,timestamp,timestamptz,interval .} now() current time of the transaction} - interval unary minus operator of interval type} interval + interval addition operator of interval type} interval - interval subtraction operator of interval type}","title":"Date and time operators"},{"location":"ref_devfuncs/#text-functionsoperators","text":"{text,bpchar} COMP {text,bpchar} comparison operators; COMP is any of =,<>,<,<=,>=,> Note that <,<=,>=,> operators are valid only when locale is UTF-8 or C (no locale).} substring(text,int4) substring(text,int4,int4) substr(text,int4) substr(text,int4,int4) extracts the substring} length({text,bpchar}) length of the string} {text,bpchar} [NOT] LIKE text pattern-matching according to the LIKE expression} {text,bpchar} [NOT] ILIKE text case-insensitive pattern-matching according to the LIKE expression. Note that ILIKE operator is valid only when locale is UTF-8 or C (no locale).}","title":"Text functions/operators"},{"location":"ref_devfuncs/#network-functionsoperators","text":"macaddr COMP macaddr comparison operators; COMP is any of =,<>,<,<=,>=,> } macaddr & macaddr Bitwise AND operator} macaddr | macaddr Bitwise OR operator} ~ macaddr Bitwise NOT operator} trunc(macaddr) Set last 3 bytes to zero} inet COMP inet comparison operators; COMP is any of =,<>,<,<=,>=,> } inet << inet Left side is contained by right side} inet <<= inet Left side is contained by or equals to right side} inet >> inet Left side contains right side} inet >>= inet Left side contains or is equals to right side} inet && inet Left side contains or is contained by right side} ~ inet Bitwise NOT operator} inet & inet Bitwise AND operator} inet | inet Bitwise OR operator} inet + int8 addition operator} inet - int8 subtraction operator} inet - inet subtraction operator} broadcast(inet) returns the broadcast address of the given network address} family(inet) returns the family of the given network address; 4 for IPv4, and 6 for IPv6} hostmask(inet) extract host mask of the given network address} masklen(inet) extract netmask length of the given network address} netmask(inet) extract netmask of the given network address} network(inet) extract network part of the given network address} set_masklen(NETADDR,int) set netmask length of the given network address; NETADDR is either inet or cidr .} inet_merge(inet,inet) the smallest network which includes both of the given networks}","title":"Network functions/operators"},{"location":"ref_devfuncs/#currency-operators","text":"money COMP money comparison operators; COMP is any of =,<>,<,<=,>=,> } money OP money arthmetric operators; OP is any of +,-,/ } money * TYPE TYPE * money Multiply a currency with a numeric value; TYPE is any of int2,int4,float2,float4,float8 } money / TYPE Division of a currency by a numeric value; TYPE is any of int2,int4,float2,float4,float8 } money / money Division of currency values}","title":"Currency operators"},{"location":"ref_devfuncs/#uuid-operators","text":"uuid COMP uuid comparison operator. COMP is any of =,<>,<,<=,>=,> }","title":"UUID operators"},{"location":"ref_devfuncs/#jsonb-operators","text":"jsonb -> KEY Get a JSON object field specified by the KEY } jsonb -> NUM Get a JSON array element indexed by NUM } jsonb ->> KEY Get a JSON object field specified by the KEY , as text} jsonb ->> NUM Get a JSON array element indexed by NUM , as text} (jsonb ->> KEY)::TYPE If TYPE is any of int2,int4,int8,float4,float8,numeric , get a JSON object field specified by KEY , as numeric data type. See the note below.} (jsonb ->> NUM)::TYPE If TYPE is any of int2,int4,int8,float4,float8,numeric Get a JSON array element indexed by NUM , as numeric data type. See the note below.} jsonb ? KEY Check whether jsonb object contains the KEY } Note When we convert a jsonb element fetched by jsonb ->> KEY operator into numerical data types like float or numeric , PostgreSQL takes 2 steps operations; an internal numerical form is printed as text first, then it is converted into numerical data type. PG-Strom optimizes the GPU code using a special device function to fetch a numerical datum from jsonb object/array, if jsonb ->> KEY operator and text-to-numeric case are continuously used.","title":"JSONB operators"},{"location":"ref_devfuncs/#postgis-functions","text":"geometry st_makepoint(float8,float8) geometry st_point(float8,float8) It makes 2-dimensional POINT geometry. geometry st_makepoint(float8,float8,float8) It makes 3-dimensional POINT geometry. geometry st_makepoint(float8,float8,float8,float8) It makes 4-dimensional POINT geometry. geometry st_setsrid(geometry,int4) It assigns SRID on the given geometry float8 st_distance(geometry,geometry) It returns the distance between geometries in float8 . bool st_dwithin(geometry,geometry,float8) It returns true if the distance between geometries is shorter than the specified threshold. It is often faster than the combination of st_distance and comparison operator. text st_relate(geometry,geometry) It checks intersection of geometries, then returns DE9-IM(Dimensionally Extended Nine-Intersection Matrix) format string. bool st_contains(geometry,geometry) It returns whether the geometry1 fully contains the geometry2. bool st_crosses(geometry,geometry) It returns whether the geometries are crossed. int4 st_linecrossingdirection(geometry,geometry) It checks how two LINESTRING geometries are crossing, or not crossing.","title":"PostGIS Functions"},{"location":"ref_devfuncs/#cube-type-functions","text":"cube COMP cube comparison operators; COMP is any of =,<>,<,<=,>=,> } bool cube_contains(cube, cube) It returns whether the first cube fully contains the second cube. bool cube_contained(cube, cube) It returns whether the first cube is fully contained by the second cube. float8 cube_ll_coord(cube, int4)","title":"CUBE Type Functions"},{"location":"ref_devfuncs/#vcf-support-functions","text":"text vcf_variant_getattr(text, text) It regards the first argument as a string of tokens separated by ':' and returns the string after KEY= specified in the second argument. =# SELECT vcf_variant_getattr('ABC=123:NM=this is a pen:XYZ=3.1415', 'NM'); vcf_variant_getattr --------------------- this is a pen (1 row) text vcf_info_getattr(text, text) It regards the first argument as a string of tokens separated by ';' and returns the string after KEY= specified in the second argument. =# SELECT vcf_info_getattr('ABC=123;NM=this is a pen;XYZ=3.1415', 'XYZ'); vcf_info_getattr ------------------ 3.141 (1 row)","title":"VCF Support Functions"},{"location":"ref_params/","text":"GUC Parameters This session introduces PG-Strom's configuration parameters. Enables/disables a particular feature pg_strom.enabled [type: bool / default: on] Enables/disables entire PG-Strom features at once pg_strom.enable_gpuscan [type: bool / default: on] Enables/disables GpuScan pg_strom.enable_gpuhashjoin [type: bool / default: on] Enables/disables JOIN by GpuHashJoin pg_strom.enable_gpugistindex [type: bool / default: on] Enables/disables JOIN by GpuGiSTIndex pg_strom.enable_gpujoin [type: bool / default: on] Enables/disables entire GpuJoin features (including GpuHashJoin and GpuGiSTIndex) pg_strom.enable_gpupreagg [type: bool / default: on] Enables/disables GpuPreAgg pg_strom.enable_gpusort [type: bool / default: on] Enables/disables GPU-Sort Check (here)[gpusort.md] to know the detail of GPU-Sort. pg_strom.enable_numeric_aggfuncs [type: bool / default: on] Enables/disables support of aggregate function that takes numeric data type. Note that aggregated function at GPU mapps numeric data type to 128bit fixed-point variable, so it raises an error if you run the aggregate functions with extremely large or highly-precise values. You can turn off this configuration to enforce the aggregate functions being operated by CPU. pg_strom.enable_brin [type: bool / default: on] Enables/disables BRIN index support on tables scan pg_strom.cpu_fallback [type: enum / default: notice ] Controls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\" notice ... Runs CPU fallback operations with notice message on , true ... Runs CPU fallback operations with no message output off , false ... Disabled CPU fallback operations with an error pg_strom.regression_test_mode [type: bool / default: off] It disables some EXPLAIN command output that depends on software execution platform, like GPU model name. It avoid \"false-positive\" on the regression test, so use usually don't tough this configuration. pg_strom.explain_developer_mode [\u578b: bool / \u521d\u671f\u5024: off] Among the various information displayed by EXPLAIN VERBOSE, this option displays information that is useful for developers. Since this information is cumbersome for general users and DB administrators, we recommend that you usually leave it at the default value. Optimizer Configuration pg_strom.gpu_setup_cost [type: real / default: 100 * DEFAULT_SEQ_PAGE_COST ] Cost value for initialization of GPU device pg_strom.gpu_tuple_cost [type: real / default: DEFAULT_CPU_TUPLE_COST ] Cost value to send tuples to, or receive tuples from GPU for each. pg_strom.gpu_operator_cost [type: real / default: DEFAULT_CPU_OPERATOR_COST / 16 ] Cost value to process an expression formula on GPU. If larger value than cpu_operator_cost is configured, no chance to choose PG-Strom towards any size of tables pg_strom.enable_partitionwise_gpujoin [type: bool / default: on] Enables/disables whether GpuJoin is pushed down to the partition children. pg_strom.enable_partitionwise_gpupreagg [type: bool / default: on] Enables/disables whether GpuPreAgg is pushed down to the partition children. pg_strom.pinned_inner_buffer_threshold [type: int / \u521d\u671f\u5024: 0 ] If the INNER table of GpuJoin is either GpuScan or GpuJoin, and the estimated size of its processing result is larger than this configured value, the result is retained on the GPU device without being returned to the CPU, and then reused as a part of the INNER buffer of the subsequent GpuJoin. If the configured value is 0 , this function will be disabled. pg_strom.pinned_inner_buffer_partition_size [type: int / default: auto] When using Pinned Inner Buffer with GPU-Join, if the buffer size exceeds the threshold specified by this parameter, it will attempt to split the buffer into multiple pieces. This parameter is automatically set to about 70% to 80% of the GPU memory, and usually does not need to be specified by the user. Check (here)[http://buri.heterodb.in/operations/#inner-pinned-buffer-of-gpujoin] for more information. pg_strom.extra_ereport_level [type: int / default: auto] Specifies the error level reported by the heterodb-extra module from 0 to 2. The initial value is set by the value of the environment variable HETERODB_EXTRA_EREPORT_LEVEL , and if not set, it will be 0 . Executor Configuration pg_strom.max_async_tasks [type: int / default: 12 ] Max number of asynchronous taks PG-Strom can submit to the GPU execution queue, and is also the number of GPU Service worker threads. GPUDirect SQL Configuration pg_strom.gpudirect_driver [type: text ] It shows the driver software name of GPUDirect SQL (read-only). Either cufile , nvme-strom or vfs pg_strom.gpudirect_enabled [type: bool / default: on ] Enables/disables GPUDirect SQL feature. pg_strom.gpu_direct_seq_page_cost [type: real / default: DEFAULT_SEQ_PAGE_COST / 4 ] The cost of scanning a table using GPU-Direct SQL, instead of the seq_page_cost , when the optimizer calculates the cost of an execution plan. pg_strom.gpudirect_threshold [type: int / default: auto] Controls the table-size threshold to invoke GPUDirect SQL feature. The default is auto configuration; a threshold calculated by the system physical memory size and shared_buffers configuration. pg_strom.manual_optimal_gpus [type: text / default: none] It manually configures the closest GPU for the target storage volumn, like NVME device or NFS volume. Its format string is: {<nvmeX>|/path/to/tablespace}=gpuX[:gpuX...] . It describes relationship between the closest GPU and NVME device or tablespace directory path. It accepts multiple configurations separated by comma character. Example: pg_strom.manual_optimal_gpus = 'nvme1=gpu0,nvme2=gpu1,/mnt/nfsroot=gpu0' <gpuX> means a GPU with device identifier X. <nvmeX> means a local NVME-SSD or a remote NVME-oF device. /path/to/tablespace means full-path of the tablespace directory. Automatic configuration is often sufficient for local NVME-SSD drives, however, you should manually configure the closest GPU for NVME-oF or NFS-over-RDMA volumes. Arrow_Fdw Configuration arrow_fdw.enabled [type: bool / default: on ] By adjustment of estimated cost value, it turns on/off Arrow_Fdw. Note that only Foreign Scan (Arrow_Fdw) can scan on Arrow files, if GpuScan is not capable to run on. arrow_fdw.stats_hint_enabled [type: bool / default: on ] When Arrow file has min/max statistics, this parameter controls whether unnecessary record-batches shall be skipped, or not. arrow_fdw.metadata_cache_size [type: int / default: 512MB ] Size of shared memory to cache metadata of Arrow files. Once consumption of the shared memory exceeds this value, the older metadata shall be released based on LRU. GPU Cache configuration pg_strom.enable_gpucache [type: bool / default: on ] Controls whether search/analytic query tries to use GPU Cache. Note that GPU Cache trigger functions continue to update the REDO Log buffer, even if this parameter is turned off. pg_strom.gpucache_auto_preload [type: text / default: null ] It specifies the table names to be loaded onto GPU Cache just after PostgreSQL startup. Its format is DATABASE_NAME.SCHEMA_NAME.TABLE_NAME , and separated by comma if multiple tables are preloaded. Initial-loading of GPU Cache usually takes a lot of time. So, preloading enables to avoid delay of response time of search/analytic queries on the first time. If this parameter is '*', PG-Strom tries to load all the configured tables onto GPU Cache sequentially. GPU Device Configuration pg_strom.gpu_mempool_segment_sz [type: int / default: 1GB ] The segment size when GPU Service allocates GPU device memory for the memory pool. GPU device memory allocation is a relatively heavy process, so it is recommended to use memory pools to reuse memory. pg_strom.gpu_mempool_max_ratio [type: real / default: 50% ] It specifies the percentage of device memory that can be used for the GPU device memory memory pool. It works to suppress excessive GPU device memory consumption by the memory pool and ensure sufficient working memory. pg_strom.gpu_mempool_min_ratio [type: real / default: 5% ] It specify the percentage of GPU device memory that is preserved as the memory pool segment, and remained even after memory usage. By maintaining a minimum memory pool, the next query can be executed quickly. pg_strom.gpu_mempool_release_delay [type: int / default: 5000 ] GPU Service does not release a segment of a memory pool immediately, even if it becomes empty. When the time specified by this parameter (in milliseconds) has elapsed since the segment was last used, it is released and returned to the system. By inserting a certain delay, you can reduce the frequency of GPU device memory allocation/release. pg_strom.cuda_toolkit_basedir [type: text / default: /usr/local/cuda ] PG-Strom uses the CUDA Toolkit to build GPU code on its start-up. Specify the installation path of the CUDA Toolkit to be used at that time. Normally, the CUDA toolkit is installed under /usr/local/cuda , but if you want to use a different directory, you can change the setting with this parameter. pg_strom.cuda_stack_limit [type: int / default: 32 ] When PG-Strom executes SQL workloads on the GPU, it automatically configures the size of the stack space used by GPU threads depending on the complexity of the processing. For example, it allocates a relatively large stack for PostGIS functions or expressions that include recursive calls. This parameter specifies the upper limit in kB in that case. pg_strom.cuda_visible_devices [type: text / default: null ] List of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable CUDAVISIBLE_DEVICES","title":"GUC Parameters"},{"location":"ref_params/#guc-parameters","text":"This session introduces PG-Strom's configuration parameters.","title":"GUC Parameters"},{"location":"ref_params/#enablesdisables-a-particular-feature","text":"pg_strom.enabled [type: bool / default: on] Enables/disables entire PG-Strom features at once pg_strom.enable_gpuscan [type: bool / default: on] Enables/disables GpuScan pg_strom.enable_gpuhashjoin [type: bool / default: on] Enables/disables JOIN by GpuHashJoin pg_strom.enable_gpugistindex [type: bool / default: on] Enables/disables JOIN by GpuGiSTIndex pg_strom.enable_gpujoin [type: bool / default: on] Enables/disables entire GpuJoin features (including GpuHashJoin and GpuGiSTIndex) pg_strom.enable_gpupreagg [type: bool / default: on] Enables/disables GpuPreAgg pg_strom.enable_gpusort [type: bool / default: on] Enables/disables GPU-Sort Check (here)[gpusort.md] to know the detail of GPU-Sort. pg_strom.enable_numeric_aggfuncs [type: bool / default: on] Enables/disables support of aggregate function that takes numeric data type. Note that aggregated function at GPU mapps numeric data type to 128bit fixed-point variable, so it raises an error if you run the aggregate functions with extremely large or highly-precise values. You can turn off this configuration to enforce the aggregate functions being operated by CPU. pg_strom.enable_brin [type: bool / default: on] Enables/disables BRIN index support on tables scan pg_strom.cpu_fallback [type: enum / default: notice ] Controls whether it actually run CPU fallback operations, if GPU program returned \"CPU ReCheck Error\" notice ... Runs CPU fallback operations with notice message on , true ... Runs CPU fallback operations with no message output off , false ... Disabled CPU fallback operations with an error pg_strom.regression_test_mode [type: bool / default: off] It disables some EXPLAIN command output that depends on software execution platform, like GPU model name. It avoid \"false-positive\" on the regression test, so use usually don't tough this configuration. pg_strom.explain_developer_mode [\u578b: bool / \u521d\u671f\u5024: off] Among the various information displayed by EXPLAIN VERBOSE, this option displays information that is useful for developers. Since this information is cumbersome for general users and DB administrators, we recommend that you usually leave it at the default value.","title":"Enables/disables a particular feature"},{"location":"ref_params/#optimizer-configuration","text":"pg_strom.gpu_setup_cost [type: real / default: 100 * DEFAULT_SEQ_PAGE_COST ] Cost value for initialization of GPU device pg_strom.gpu_tuple_cost [type: real / default: DEFAULT_CPU_TUPLE_COST ] Cost value to send tuples to, or receive tuples from GPU for each. pg_strom.gpu_operator_cost [type: real / default: DEFAULT_CPU_OPERATOR_COST / 16 ] Cost value to process an expression formula on GPU. If larger value than cpu_operator_cost is configured, no chance to choose PG-Strom towards any size of tables pg_strom.enable_partitionwise_gpujoin [type: bool / default: on] Enables/disables whether GpuJoin is pushed down to the partition children. pg_strom.enable_partitionwise_gpupreagg [type: bool / default: on] Enables/disables whether GpuPreAgg is pushed down to the partition children. pg_strom.pinned_inner_buffer_threshold [type: int / \u521d\u671f\u5024: 0 ] If the INNER table of GpuJoin is either GpuScan or GpuJoin, and the estimated size of its processing result is larger than this configured value, the result is retained on the GPU device without being returned to the CPU, and then reused as a part of the INNER buffer of the subsequent GpuJoin. If the configured value is 0 , this function will be disabled. pg_strom.pinned_inner_buffer_partition_size [type: int / default: auto] When using Pinned Inner Buffer with GPU-Join, if the buffer size exceeds the threshold specified by this parameter, it will attempt to split the buffer into multiple pieces. This parameter is automatically set to about 70% to 80% of the GPU memory, and usually does not need to be specified by the user. Check (here)[http://buri.heterodb.in/operations/#inner-pinned-buffer-of-gpujoin] for more information. pg_strom.extra_ereport_level [type: int / default: auto] Specifies the error level reported by the heterodb-extra module from 0 to 2. The initial value is set by the value of the environment variable HETERODB_EXTRA_EREPORT_LEVEL , and if not set, it will be 0 .","title":"Optimizer Configuration"},{"location":"ref_params/#executor-configuration","text":"pg_strom.max_async_tasks [type: int / default: 12 ] Max number of asynchronous taks PG-Strom can submit to the GPU execution queue, and is also the number of GPU Service worker threads.","title":"Executor Configuration"},{"location":"ref_params/#gpudirect-sql-configuration","text":"pg_strom.gpudirect_driver [type: text ] It shows the driver software name of GPUDirect SQL (read-only). Either cufile , nvme-strom or vfs pg_strom.gpudirect_enabled [type: bool / default: on ] Enables/disables GPUDirect SQL feature. pg_strom.gpu_direct_seq_page_cost [type: real / default: DEFAULT_SEQ_PAGE_COST / 4 ] The cost of scanning a table using GPU-Direct SQL, instead of the seq_page_cost , when the optimizer calculates the cost of an execution plan. pg_strom.gpudirect_threshold [type: int / default: auto] Controls the table-size threshold to invoke GPUDirect SQL feature. The default is auto configuration; a threshold calculated by the system physical memory size and shared_buffers configuration. pg_strom.manual_optimal_gpus [type: text / default: none] It manually configures the closest GPU for the target storage volumn, like NVME device or NFS volume. Its format string is: {<nvmeX>|/path/to/tablespace}=gpuX[:gpuX...] . It describes relationship between the closest GPU and NVME device or tablespace directory path. It accepts multiple configurations separated by comma character. Example: pg_strom.manual_optimal_gpus = 'nvme1=gpu0,nvme2=gpu1,/mnt/nfsroot=gpu0' <gpuX> means a GPU with device identifier X. <nvmeX> means a local NVME-SSD or a remote NVME-oF device. /path/to/tablespace means full-path of the tablespace directory. Automatic configuration is often sufficient for local NVME-SSD drives, however, you should manually configure the closest GPU for NVME-oF or NFS-over-RDMA volumes.","title":"GPUDirect SQL Configuration"},{"location":"ref_params/#arrow_fdw-configuration","text":"arrow_fdw.enabled [type: bool / default: on ] By adjustment of estimated cost value, it turns on/off Arrow_Fdw. Note that only Foreign Scan (Arrow_Fdw) can scan on Arrow files, if GpuScan is not capable to run on. arrow_fdw.stats_hint_enabled [type: bool / default: on ] When Arrow file has min/max statistics, this parameter controls whether unnecessary record-batches shall be skipped, or not. arrow_fdw.metadata_cache_size [type: int / default: 512MB ] Size of shared memory to cache metadata of Arrow files. Once consumption of the shared memory exceeds this value, the older metadata shall be released based on LRU.","title":"Arrow_Fdw Configuration"},{"location":"ref_params/#gpu-cache-configuration","text":"pg_strom.enable_gpucache [type: bool / default: on ] Controls whether search/analytic query tries to use GPU Cache. Note that GPU Cache trigger functions continue to update the REDO Log buffer, even if this parameter is turned off. pg_strom.gpucache_auto_preload [type: text / default: null ] It specifies the table names to be loaded onto GPU Cache just after PostgreSQL startup. Its format is DATABASE_NAME.SCHEMA_NAME.TABLE_NAME , and separated by comma if multiple tables are preloaded. Initial-loading of GPU Cache usually takes a lot of time. So, preloading enables to avoid delay of response time of search/analytic queries on the first time. If this parameter is '*', PG-Strom tries to load all the configured tables onto GPU Cache sequentially.","title":"GPU Cache configuration"},{"location":"ref_params/#gpu-device-configuration","text":"pg_strom.gpu_mempool_segment_sz [type: int / default: 1GB ] The segment size when GPU Service allocates GPU device memory for the memory pool. GPU device memory allocation is a relatively heavy process, so it is recommended to use memory pools to reuse memory. pg_strom.gpu_mempool_max_ratio [type: real / default: 50% ] It specifies the percentage of device memory that can be used for the GPU device memory memory pool. It works to suppress excessive GPU device memory consumption by the memory pool and ensure sufficient working memory. pg_strom.gpu_mempool_min_ratio [type: real / default: 5% ] It specify the percentage of GPU device memory that is preserved as the memory pool segment, and remained even after memory usage. By maintaining a minimum memory pool, the next query can be executed quickly. pg_strom.gpu_mempool_release_delay [type: int / default: 5000 ] GPU Service does not release a segment of a memory pool immediately, even if it becomes empty. When the time specified by this parameter (in milliseconds) has elapsed since the segment was last used, it is released and returned to the system. By inserting a certain delay, you can reduce the frequency of GPU device memory allocation/release. pg_strom.cuda_toolkit_basedir [type: text / default: /usr/local/cuda ] PG-Strom uses the CUDA Toolkit to build GPU code on its start-up. Specify the installation path of the CUDA Toolkit to be used at that time. Normally, the CUDA toolkit is installed under /usr/local/cuda , but if you want to use a different directory, you can change the setting with this parameter. pg_strom.cuda_stack_limit [type: int / default: 32 ] When PG-Strom executes SQL workloads on the GPU, it automatically configures the size of the stack space used by GPU threads depending on the complexity of the processing. For example, it allocates a relatively large stack for PostGIS functions or expressions that include recursive calls. This parameter specifies the upper limit in kB in that case. pg_strom.cuda_visible_devices [type: text / default: null ] List of GPU device numbers in comma separated, if you want to recognize particular GPUs on PostgreSQL startup. It is equivalent to the environment variable CUDAVISIBLE_DEVICES","title":"GPU Device Configuration"},{"location":"ref_sqlfuncs/","text":"SQL Objects This chapter introduces SQL objects additionally provided by PG-Strom. System Information pgstrom.device_info System View It shows properties of GPU devices installed for PG-Strom. Below is schema definition of the view. name type description gpu_id int GPU device number att_name text Attribute name att_value text Attribute value att_desc text Attribute description There are various kind of GPU device properties, but depending on the CUDA driver version where system is running. So, pgstrom.device_info system view identifies the target property by GPU device identifier ( gpu_id ) and attribute name ( att_name ). Below is an example of pgstrom.device_info system view. postgres=# select * from pgstrom.gpu_device_info limit 10; gpu_id | att_name | att_value | att_desc --------+-----------------------+------------------------------------------+------------------------------------- 0 | DEV_NAME | NVIDIA A100-PCIE-40GB | GPU Device Name 0 | DEV_ID | 0 | GPU Device ID 0 | DEV_UUID | GPU-13943bfd-5b30-38f5-0473-78979c134606 | GPU Device UUID 0 | DEV_TOTAL_MEMSZ | 39.39GB | GPU Total RAM Size 0 | DEV_BAR1_MEMSZ | 64.00GB | GPU PCI Bar1 Size 0 | NUMA_NODE_ID | -1 | GPU NUMA Node Id 0 | MAX_THREADS_PER_BLOCK | 1024 | Maximum number of threads per block 0 | MAX_BLOCK_DIM_X | 1024 | Maximum block dimension X 0 | MAX_BLOCK_DIM_Y | 1024 | Maximum block dimension Y 0 | MAX_BLOCK_DIM_Z | 64 | Maximum block dimension Z (10 rows) Arrow_Fdw fdw_handler pgstrom.arrow_fdw_handler() FDW handler function of Arrow_Fdw. Usually, users don't need to invoke this function. void pgstrom.arrow_fdw_validator(text[], oid) FDW options validation function of Arrow_Fdw. Usually, users don't need to invoke this function. event_trigger pgstrom.arrow_fdw_precheck_schema() Event trigger function to validate schema definition of Arrow files. Usually, users don't need to invoke this function. text pgstrom.arrow_fdw_check_pattern(text, text) Checks whether the file name given in the first argument matches the pattern given in the second argument. This is useful for checking how wildcards work when using the pattern option of Arrow_Fdw. =# select pgstrom.arrow_fdw_check_pattern('data_202408_tokyo.data', 'data__${region}.data'); arrow_fdw_check_pattern ------------------------------------------- true {@ymd]=[202408], $[region]=[tokyo]} (1 row) _${region}.data' \u306e\u3046\u3061\u3001\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9``\u306b\u76f8\u5f53\u3059\u308b\u90e8\u5206\u304c 202408 \u306b\u30de\u30c3\u30c1\u3057\u3001 ${region} \u306b\u76f8\u5f53\u3059\u308b\u90e8\u5206\u304c tokyo`\u306b\u30de\u30c3\u30c1\u3057\u3066\u3044\u307e\u3059\u3002 } In the above example, for the file name data_202408_tokyo.data , the part of the pattern 'data_@ymd_${region}.data' that corresponds to the wildcard ` matches 202408 , and the part that corresponds to ${region} matches tokyo`. } record pgstrom.arrow_fdw_metadata_info(regclass) Displays the metadata (CustomMetadata) embedded in the supplied Arrow file. postgres=# select relid, filename, field, key, substring(value, 0, 64) from pgstrom.arrow_fdw_metadata_info('f_lineorder'); relid | filename | field | key | substring -------------+-------------------------------------+--------------+-------------+----------------------------------------------------------------- f_lineorder | /opt/arrow/f_lineorder_sorted.arrow | | sql_command | select * from v_lineorder order by lo_orderdate f_lineorder | /opt/arrow/f_lineorder_sorted.arrow | lo_orderdate | min_values | 19920101,19920102,19920103,19920104,19920105,19920107,19920108, f_lineorder | /opt/arrow/f_lineorder_sorted.arrow | lo_orderdate | max_values | 19920102,19920103,19920104,19920105,19920107,19920108,19920109, (3 rows) In the above example, the metadata (CustomMetadata) embedded in the f_lineorder foreign table managed by Arrow_Fdw is output in KEY=VALUE form. Metadata embedded in the schema has the field column set to NULL. Otherwise, the column name is displayed. In this case, you can see that the sql_command metadata is embedded in the schema, and the min_values and max_values are embedded in the lo_orderdate column. json pgstrom.arrow_fdw_metadata_stats() Displays the statistics of the Arrow_Fdw metadata-cache Arrow_Fdw somehow remembers the metadata (schema definition, data placement, etc.) of an Arrow file that has been referenced once in shared memory, and can improve responsiveness when referencing an Arrow_Fdw foreign table from the next time onwards. This function can display statistical information about the metadata cache in shared memory. If the metadata cache is insufficient, the reference responsiveness of Arrow_Fdw foreign tables may decrease. In that case, it is necessary to enlarge the metadata cache by increasing arrow_fdw.metadata_cache_size . s=# select pgstrom.arrow_fdw_metadata_stats(); arrow_fdw_metadata_stats ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- { \"total_cache_sz\" : 536870912, \"total_cache_allocated\" : 9568256, \"total_cache_usage\" : 9354072, \"num_file_entries\" : 2, \"cache_usage_efficiency\" : 0.977615, \"cache_num_blocks\" : 4096, \"cache_active_blocks\" : 73, \"cache_free_blocks\" : 4023, \"cache_used_ratio\" : 0.017822 \"last_allocate_timestamp\" : \"2025-03-29 13:24:44.447\" } (1 row) As in the example above, the statistics are displayed in JSON format, with the meaning of each field as follows: total_cache_sz The total size of the metadata cache (in bytes) total_cache_allocated Allocated metadata cache size (in bytes) total_cache_usage Used metadata cache size (in bytes) num_file_entries Number of Arrow files that are cached cache_usage_efficiency The ratio of used metadata cache to the allocated metadata cache cache_num_blocks Number of blocks in the metadata cache (in 128kB units) cache_active_blocks Number of allocated metadata cache blocks cache_free_blocks Number of free metadata cache blocks cache_used_ratio Percentage of metadata cache blocks allocated last_allocate_timestamp The timestamp of the last metadata cache allocation last_reclaim_timestamp The timestamp when the metadata cache was last cleared void pgstrom.arrow_fdw_import_file(text, text, text = null) This function tries to import Apache Arrow file, and defines a new foreign table. Its first argument is name of the new foreign table, the second argument is path of the Apache Arrow file, and the optional third argument is the schema name. This function is similar to IMPORT FOREIGN SCHEMA statement, but allows to import Apache Arrow files that have wider fields than the limitation of number of columns in PostgreSQL ( MaxTupleAttributeNumber = 1664). So, we recommend to use IMPORT FOREIGN SCHEMA statement for most cases. The example below shows the steps to import an Apache Arrow file with 2000 of Int16 fields by the pgstrom.arrow_fdw_import_file . The result of \\d mytest shows this foreign table has 2000 fields. Due to the internal data format of PostgreSQL, it is not possible to read all the columns at once, but possible to read a part of columns like the last example. =# select pgstrom.arrow_fdw_import_file('mytest', '/tmp/wide2000.arrow'); arrow_fdw_import_file ----------------------- (1 row) =# \\d List of relations Schema | Name | Type | Owner --------+--------+---------------+-------- public | mytest | foreign table | kaigai (1 row) =# \\d mytest Foreign table \"public.mytest\" Column | Type | Collation | Nullable | Default | FDW options -----------+----------+-----------+----------+---------+------------- object_id | integer | | | | c000 | smallint | | | | c001 | smallint | | | | c002 | smallint | | | | c003 | smallint | | | | : : : : : : c1997 | smallint | | | | c1998 | smallint | | | | c1999 | smallint | | | | Server: arrow_fdw FDW options: (file '/tmp/wide2000.arrow') =# select * from mytest ; ERROR: target lists can have at most 1664 entries =# select c0010,c1234,c1999 from mytest limit 3; c0010 | c1234 | c1999 -------+-------+------- 232 | 232 | 232 537 | 537 | 537 219 | 219 | 219 (3 rows) GPU Cache pgstrom.gpucache_info System View It shows the current status of GPU Cache. Below is schema definition of the view. name type description database_oid oid Database OID where the table with GPU Cache belongs to. database_name text Database name where the table with GPU Cache belongs to. table_oid oid Table OID that has GPU Cache. Note that it may not be in the current database. table_name text Table name that has GPU Cache. Note that it may not be in the current database. signature int8 An identifier hash value of GPU Cache. It may be changed after ALTER TABLE for example. phase text Phase of GPU cache construction: either not_built , is_empty , is_loading , is_ready , or corrupted rowid_num_used int8 Number of allocated row-id rowid_num_free int8 Number of free row-id gpu_main_sz int8 Size of the fixed-length values area on the GPU Cache. gpu_main_nitems int8 Number of tuples on the GPU Cache. gpu_extra_sz int8 Size of the variable-length values area on the GPU Cache. gpu_extra_usage int8 Size of the used variable-length values area on the GPU Cache. gpu_extra_dead int8 Size of the free variable-length values area on the GPU Cache. redo_write_ts timestamptz Last update timestamp on the REDO Log buffer redo_write_nitems int8 Total number of REDO Log entries written to the REDO Log buffer. redo_write_pos int8 Total bytes of REDO Log entries written to the REDO Log buffer. redo_read_nitems int8 Total number of REDO Log entries read from REDO Log buffer, and already applied to. redo_read_pos int8 Total bytes of REDO Log entries read from REDO Log buffer, and already applied to. redo_sync_pos int8 The latest position on the REDO Log buffer, where it is already required the background worker to synchronize onto the GPU Cache. When free space of REDO Log buffer becomes tight, it is internally used to avoid flood of simultaneous asynchronized requests by many sessions. config_options text Options string of the GPU Cache Below is an example of pgstrom.gpucache_info system view. =# select * from pgstrom.gpucache_info ; database_oid | database_name | table_oid | table_name | signature | phase | rowid_num_used | rowid_num_free | gpu_main_sz | gpu_main_nitems | gpu_extra_sz | gpu_extra_usage | gpu_extra_dead | redo_write_ts | redo_write_nitems | redo_write_pos | redo_read_nitems | redo_read_pos | redo_sync_pos | config_options --------------+---------------+-----------+------------------+------------+----------+----------------+----------------+-------------+-----------------+--------------+-----------------+----------------+-------------------------------+-------------------+----------------+------------------+---------------+---------------+--------------------------------------------------------------------------------------------------------------------- 193450 | hoge | 603029 | cache_test_table | 4529357070 | is_ready | 4000 | 6000 | 439904 | 4000 | 3200024 | 473848 | 0 | 2023-12-18 01:25:42.850193+09 | 4000 | 603368 | 4000 | 603368 | 603368 | gpu_device_id=0,max_num_rows=10000,redo_buffer_size=157286400,gpu_sync_interval=4000000,gpu_sync_threshold=10485760 (1 row) trigger pgstrom.gpucache_sync_trigger() A trigger function to synchronize GPU Cache on table updates. See GPU Cache chapter for more details. bigint pgstrom.gpucache_apply_redo(regclass) If the given table has GPU Cache configured, it forcibly applies the REDO log entries onto the GPU Cache. bigint pgstrom.gpucache_compaction(regclass) If the given table has GPU Cache configured, it forcibly run compaction of the variable-length data buffer. bigint pgstrom.gpucache_recovery(regclass) It tries to recover the corrupted GPU cache. Test Data Generator void pgstrom.random_setseed(int) It initializes the random seed. bigint pgstrom.random_int(float=0.0, bigint=0, bigint=INT_MAX) It generates random data in bigint type within the range. float pgstrom.random_float(float=0.0, float=0.0, float=1.0) It generates random data in float type within the range. date pgstrom.random_date(float=0.0, date='2015-01-01', date='2025-12-31') It generates random data in date type within the range. time pgstrom.random_time(float=0.0, time='00:00:00', time='23:59:59') It generates random data in time type within the range. timetz pgstrom.random_timetz(float=0.0, time='00:00:00', time='23:59:59') It generates random data in timetz type within the range. timestamp pgstrom.random_timestamp(float=0.0, timestamp='2015-01-01', timestamp='2025-01-01') It generates random data in timestamp type within the range. macaddr pgstrom.random_macaddr(float=0.0, macaddr='ab:cd:00:00:00', macaddr='ab:cd:ff:ff:ff:ff') It generates random data in macaddr type within the range. inet pgstrom.random_inet(float=0.0, inet='192.168.0.1/16') It generates random data in inet type within the range. text pgstrom.random_text(float=0.0, text='test_**') It generates random data in text type. The '*' characters in 2nd argument shall be replaced randomly. text pgstrom.random_text_len(float=0.0, int=10) It generates random data in text type within the specified length. int4range pgstrom.random_int4range(float=0.0, bigint=0, bigint=INT_MAX) It generates random data in int4range type within the range.} int8range pgstrom.random_int8range(float=0.0, bigint=0, bigint=LONG_MAX) It generates random data in int8range type within the range. tsrange pgstrom.random_tsrange(float=0.0, timestamp='2015-01-01', timestamp='2025-01-01') It generates random data in tsrange type within the range. tstzrange pgstrom.random_tstzrange(float=0.0, timestamptz='2015-01-01', timestamptz='2025-01-01') It generates random data in tstzrange type within the range. daterange pgstrom.random_daterange(float=0.0, date='2015-01-01', date='2025-12-31') It generates random data in daterange type within the range. Other Functions text pgstrom.githash() It displays the hash value of the source code revision from the currently loaded PG-Strom module is based. This value is useful in determining the software revision in the event of a failure. postgres=# select pgstrom.githash(); githash ------------------------------------------ 103984be24cafd1e7ce6330a050960d97675c196 text pgstrom.license_query() It displays the active commercial subscription, if loaded. =# select pgstrom.license_query(); license_query ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"2020-11-24\", \"expired_at\" : \"2025-12-31\", \"gpus\" : [ { \"uuid\" : \"GPU-8ba149db-53d8-c5f3-0f55-97ce8cfadb28\" } ]} (1 row)","title":"SQL Objects"},{"location":"ref_sqlfuncs/#sql-objects","text":"This chapter introduces SQL objects additionally provided by PG-Strom.","title":"SQL Objects"},{"location":"ref_sqlfuncs/#system-information","text":"pgstrom.device_info System View It shows properties of GPU devices installed for PG-Strom. Below is schema definition of the view. name type description gpu_id int GPU device number att_name text Attribute name att_value text Attribute value att_desc text Attribute description There are various kind of GPU device properties, but depending on the CUDA driver version where system is running. So, pgstrom.device_info system view identifies the target property by GPU device identifier ( gpu_id ) and attribute name ( att_name ). Below is an example of pgstrom.device_info system view. postgres=# select * from pgstrom.gpu_device_info limit 10; gpu_id | att_name | att_value | att_desc --------+-----------------------+------------------------------------------+------------------------------------- 0 | DEV_NAME | NVIDIA A100-PCIE-40GB | GPU Device Name 0 | DEV_ID | 0 | GPU Device ID 0 | DEV_UUID | GPU-13943bfd-5b30-38f5-0473-78979c134606 | GPU Device UUID 0 | DEV_TOTAL_MEMSZ | 39.39GB | GPU Total RAM Size 0 | DEV_BAR1_MEMSZ | 64.00GB | GPU PCI Bar1 Size 0 | NUMA_NODE_ID | -1 | GPU NUMA Node Id 0 | MAX_THREADS_PER_BLOCK | 1024 | Maximum number of threads per block 0 | MAX_BLOCK_DIM_X | 1024 | Maximum block dimension X 0 | MAX_BLOCK_DIM_Y | 1024 | Maximum block dimension Y 0 | MAX_BLOCK_DIM_Z | 64 | Maximum block dimension Z (10 rows)","title":"System Information"},{"location":"ref_sqlfuncs/#arrow_fdw","text":"fdw_handler pgstrom.arrow_fdw_handler() FDW handler function of Arrow_Fdw. Usually, users don't need to invoke this function. void pgstrom.arrow_fdw_validator(text[], oid) FDW options validation function of Arrow_Fdw. Usually, users don't need to invoke this function. event_trigger pgstrom.arrow_fdw_precheck_schema() Event trigger function to validate schema definition of Arrow files. Usually, users don't need to invoke this function. text pgstrom.arrow_fdw_check_pattern(text, text) Checks whether the file name given in the first argument matches the pattern given in the second argument. This is useful for checking how wildcards work when using the pattern option of Arrow_Fdw. =# select pgstrom.arrow_fdw_check_pattern('data_202408_tokyo.data', 'data__${region}.data'); arrow_fdw_check_pattern ------------------------------------------- true {@ymd]=[202408], $[region]=[tokyo]} (1 row) _${region}.data' \u306e\u3046\u3061\u3001\u30ef\u30a4\u30eb\u30c9\u30ab\u30fc\u30c9``\u306b\u76f8\u5f53\u3059\u308b\u90e8\u5206\u304c 202408 \u306b\u30de\u30c3\u30c1\u3057\u3001 ${region} \u306b\u76f8\u5f53\u3059\u308b\u90e8\u5206\u304c tokyo`\u306b\u30de\u30c3\u30c1\u3057\u3066\u3044\u307e\u3059\u3002 } In the above example, for the file name data_202408_tokyo.data , the part of the pattern 'data_@ymd_${region}.data' that corresponds to the wildcard ` matches 202408 , and the part that corresponds to ${region} matches tokyo`. } record pgstrom.arrow_fdw_metadata_info(regclass) Displays the metadata (CustomMetadata) embedded in the supplied Arrow file. postgres=# select relid, filename, field, key, substring(value, 0, 64) from pgstrom.arrow_fdw_metadata_info('f_lineorder'); relid | filename | field | key | substring -------------+-------------------------------------+--------------+-------------+----------------------------------------------------------------- f_lineorder | /opt/arrow/f_lineorder_sorted.arrow | | sql_command | select * from v_lineorder order by lo_orderdate f_lineorder | /opt/arrow/f_lineorder_sorted.arrow | lo_orderdate | min_values | 19920101,19920102,19920103,19920104,19920105,19920107,19920108, f_lineorder | /opt/arrow/f_lineorder_sorted.arrow | lo_orderdate | max_values | 19920102,19920103,19920104,19920105,19920107,19920108,19920109, (3 rows) In the above example, the metadata (CustomMetadata) embedded in the f_lineorder foreign table managed by Arrow_Fdw is output in KEY=VALUE form. Metadata embedded in the schema has the field column set to NULL. Otherwise, the column name is displayed. In this case, you can see that the sql_command metadata is embedded in the schema, and the min_values and max_values are embedded in the lo_orderdate column. json pgstrom.arrow_fdw_metadata_stats() Displays the statistics of the Arrow_Fdw metadata-cache Arrow_Fdw somehow remembers the metadata (schema definition, data placement, etc.) of an Arrow file that has been referenced once in shared memory, and can improve responsiveness when referencing an Arrow_Fdw foreign table from the next time onwards. This function can display statistical information about the metadata cache in shared memory. If the metadata cache is insufficient, the reference responsiveness of Arrow_Fdw foreign tables may decrease. In that case, it is necessary to enlarge the metadata cache by increasing arrow_fdw.metadata_cache_size . s=# select pgstrom.arrow_fdw_metadata_stats(); arrow_fdw_metadata_stats ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- { \"total_cache_sz\" : 536870912, \"total_cache_allocated\" : 9568256, \"total_cache_usage\" : 9354072, \"num_file_entries\" : 2, \"cache_usage_efficiency\" : 0.977615, \"cache_num_blocks\" : 4096, \"cache_active_blocks\" : 73, \"cache_free_blocks\" : 4023, \"cache_used_ratio\" : 0.017822 \"last_allocate_timestamp\" : \"2025-03-29 13:24:44.447\" } (1 row) As in the example above, the statistics are displayed in JSON format, with the meaning of each field as follows: total_cache_sz The total size of the metadata cache (in bytes) total_cache_allocated Allocated metadata cache size (in bytes) total_cache_usage Used metadata cache size (in bytes) num_file_entries Number of Arrow files that are cached cache_usage_efficiency The ratio of used metadata cache to the allocated metadata cache cache_num_blocks Number of blocks in the metadata cache (in 128kB units) cache_active_blocks Number of allocated metadata cache blocks cache_free_blocks Number of free metadata cache blocks cache_used_ratio Percentage of metadata cache blocks allocated last_allocate_timestamp The timestamp of the last metadata cache allocation last_reclaim_timestamp The timestamp when the metadata cache was last cleared void pgstrom.arrow_fdw_import_file(text, text, text = null) This function tries to import Apache Arrow file, and defines a new foreign table. Its first argument is name of the new foreign table, the second argument is path of the Apache Arrow file, and the optional third argument is the schema name. This function is similar to IMPORT FOREIGN SCHEMA statement, but allows to import Apache Arrow files that have wider fields than the limitation of number of columns in PostgreSQL ( MaxTupleAttributeNumber = 1664). So, we recommend to use IMPORT FOREIGN SCHEMA statement for most cases. The example below shows the steps to import an Apache Arrow file with 2000 of Int16 fields by the pgstrom.arrow_fdw_import_file . The result of \\d mytest shows this foreign table has 2000 fields. Due to the internal data format of PostgreSQL, it is not possible to read all the columns at once, but possible to read a part of columns like the last example. =# select pgstrom.arrow_fdw_import_file('mytest', '/tmp/wide2000.arrow'); arrow_fdw_import_file ----------------------- (1 row) =# \\d List of relations Schema | Name | Type | Owner --------+--------+---------------+-------- public | mytest | foreign table | kaigai (1 row) =# \\d mytest Foreign table \"public.mytest\" Column | Type | Collation | Nullable | Default | FDW options -----------+----------+-----------+----------+---------+------------- object_id | integer | | | | c000 | smallint | | | | c001 | smallint | | | | c002 | smallint | | | | c003 | smallint | | | | : : : : : : c1997 | smallint | | | | c1998 | smallint | | | | c1999 | smallint | | | | Server: arrow_fdw FDW options: (file '/tmp/wide2000.arrow') =# select * from mytest ; ERROR: target lists can have at most 1664 entries =# select c0010,c1234,c1999 from mytest limit 3; c0010 | c1234 | c1999 -------+-------+------- 232 | 232 | 232 537 | 537 | 537 219 | 219 | 219 (3 rows)","title":"Arrow_Fdw"},{"location":"ref_sqlfuncs/#gpu-cache","text":"pgstrom.gpucache_info System View It shows the current status of GPU Cache. Below is schema definition of the view. name type description database_oid oid Database OID where the table with GPU Cache belongs to. database_name text Database name where the table with GPU Cache belongs to. table_oid oid Table OID that has GPU Cache. Note that it may not be in the current database. table_name text Table name that has GPU Cache. Note that it may not be in the current database. signature int8 An identifier hash value of GPU Cache. It may be changed after ALTER TABLE for example. phase text Phase of GPU cache construction: either not_built , is_empty , is_loading , is_ready , or corrupted rowid_num_used int8 Number of allocated row-id rowid_num_free int8 Number of free row-id gpu_main_sz int8 Size of the fixed-length values area on the GPU Cache. gpu_main_nitems int8 Number of tuples on the GPU Cache. gpu_extra_sz int8 Size of the variable-length values area on the GPU Cache. gpu_extra_usage int8 Size of the used variable-length values area on the GPU Cache. gpu_extra_dead int8 Size of the free variable-length values area on the GPU Cache. redo_write_ts timestamptz Last update timestamp on the REDO Log buffer redo_write_nitems int8 Total number of REDO Log entries written to the REDO Log buffer. redo_write_pos int8 Total bytes of REDO Log entries written to the REDO Log buffer. redo_read_nitems int8 Total number of REDO Log entries read from REDO Log buffer, and already applied to. redo_read_pos int8 Total bytes of REDO Log entries read from REDO Log buffer, and already applied to. redo_sync_pos int8 The latest position on the REDO Log buffer, where it is already required the background worker to synchronize onto the GPU Cache. When free space of REDO Log buffer becomes tight, it is internally used to avoid flood of simultaneous asynchronized requests by many sessions. config_options text Options string of the GPU Cache Below is an example of pgstrom.gpucache_info system view. =# select * from pgstrom.gpucache_info ; database_oid | database_name | table_oid | table_name | signature | phase | rowid_num_used | rowid_num_free | gpu_main_sz | gpu_main_nitems | gpu_extra_sz | gpu_extra_usage | gpu_extra_dead | redo_write_ts | redo_write_nitems | redo_write_pos | redo_read_nitems | redo_read_pos | redo_sync_pos | config_options --------------+---------------+-----------+------------------+------------+----------+----------------+----------------+-------------+-----------------+--------------+-----------------+----------------+-------------------------------+-------------------+----------------+------------------+---------------+---------------+--------------------------------------------------------------------------------------------------------------------- 193450 | hoge | 603029 | cache_test_table | 4529357070 | is_ready | 4000 | 6000 | 439904 | 4000 | 3200024 | 473848 | 0 | 2023-12-18 01:25:42.850193+09 | 4000 | 603368 | 4000 | 603368 | 603368 | gpu_device_id=0,max_num_rows=10000,redo_buffer_size=157286400,gpu_sync_interval=4000000,gpu_sync_threshold=10485760 (1 row) trigger pgstrom.gpucache_sync_trigger() A trigger function to synchronize GPU Cache on table updates. See GPU Cache chapter for more details. bigint pgstrom.gpucache_apply_redo(regclass) If the given table has GPU Cache configured, it forcibly applies the REDO log entries onto the GPU Cache. bigint pgstrom.gpucache_compaction(regclass) If the given table has GPU Cache configured, it forcibly run compaction of the variable-length data buffer. bigint pgstrom.gpucache_recovery(regclass) It tries to recover the corrupted GPU cache.","title":"GPU Cache"},{"location":"ref_sqlfuncs/#test-data-generator","text":"void pgstrom.random_setseed(int) It initializes the random seed. bigint pgstrom.random_int(float=0.0, bigint=0, bigint=INT_MAX) It generates random data in bigint type within the range. float pgstrom.random_float(float=0.0, float=0.0, float=1.0) It generates random data in float type within the range. date pgstrom.random_date(float=0.0, date='2015-01-01', date='2025-12-31') It generates random data in date type within the range. time pgstrom.random_time(float=0.0, time='00:00:00', time='23:59:59') It generates random data in time type within the range. timetz pgstrom.random_timetz(float=0.0, time='00:00:00', time='23:59:59') It generates random data in timetz type within the range. timestamp pgstrom.random_timestamp(float=0.0, timestamp='2015-01-01', timestamp='2025-01-01') It generates random data in timestamp type within the range. macaddr pgstrom.random_macaddr(float=0.0, macaddr='ab:cd:00:00:00', macaddr='ab:cd:ff:ff:ff:ff') It generates random data in macaddr type within the range. inet pgstrom.random_inet(float=0.0, inet='192.168.0.1/16') It generates random data in inet type within the range. text pgstrom.random_text(float=0.0, text='test_**') It generates random data in text type. The '*' characters in 2nd argument shall be replaced randomly. text pgstrom.random_text_len(float=0.0, int=10) It generates random data in text type within the specified length. int4range pgstrom.random_int4range(float=0.0, bigint=0, bigint=INT_MAX) It generates random data in int4range type within the range.} int8range pgstrom.random_int8range(float=0.0, bigint=0, bigint=LONG_MAX) It generates random data in int8range type within the range. tsrange pgstrom.random_tsrange(float=0.0, timestamp='2015-01-01', timestamp='2025-01-01') It generates random data in tsrange type within the range. tstzrange pgstrom.random_tstzrange(float=0.0, timestamptz='2015-01-01', timestamptz='2025-01-01') It generates random data in tstzrange type within the range. daterange pgstrom.random_daterange(float=0.0, date='2015-01-01', date='2025-12-31') It generates random data in daterange type within the range.","title":"Test Data Generator"},{"location":"ref_sqlfuncs/#other-functions","text":"text pgstrom.githash() It displays the hash value of the source code revision from the currently loaded PG-Strom module is based. This value is useful in determining the software revision in the event of a failure. postgres=# select pgstrom.githash(); githash ------------------------------------------ 103984be24cafd1e7ce6330a050960d97675c196 text pgstrom.license_query() It displays the active commercial subscription, if loaded. =# select pgstrom.license_query(); license_query ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- { \"version\" : 2, \"serial_nr\" : \"HDB-TRIAL\", \"issued_at\" : \"2020-11-24\", \"expired_at\" : \"2025-12-31\", \"gpus\" : [ { \"uuid\" : \"GPU-8ba149db-53d8-c5f3-0f55-97ce8cfadb28\" } ]} (1 row)","title":"Other Functions"},{"location":"ref_types/","text":"Data Types PG-Strom support the following data types for use on GPU device. Numeric types int1 [length: 1byte] 8bit integer data type; enhanced data type by PG-Strom int2 (alias smallint ) [length: 2bytes] 16bit integer data type int4 (alias int ) [length: 4bytes] 32bit integer data type int8 (alias bigint ) [length: 8bytes] 64bit integer data type float2 [length: 2bytes] Half precision data type; enhanced data type by PG-Strom Note Even though GPU supports half-precision floating-point numbers by hardware, CPU (x86_64 processor) does not support it yet. So, when CPU processes float2 data types, it transform them to float or double on calculations. So, CPU has no advantages for calculation performance of float2 , unlike GPU. It is a feature to save storage/memory capacity for machine-learning / statistical-analytics. float4 (alias real ) [length: 4bytes] Single precision floating-point data type float8 (alias double precision ) [length: 8bytes] Double precision floating-point data type numeric [length: variable] Real number data type; handled as a 128bit fixed-point value in GPU Note When GPU processes values in numeric data type, it is converted to an internal 128bit fixed-point number because of implementation reason. (This layout is identical to Decimal type in Apache Arrow.) It is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert numaric datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if numeric data with large number of digits are supplied to GPU device. To avoid the problem, turn off the GUC option pg_strom.enable_numeric_type not to run operational expression including numeric data types on GPU devices. Date and time types date [length: 4bytes] Date data type time (alias time without time zone ) [length: 8bytes] Time data type timetz (alias time with time zone ) [length: 12bytes] Time with timezone data type timestamp (alias timestamp without time zone ) [length: 8bytes] Timestamp data type timestamptz (alias timestamp with time zone ) [length: 8bytes] Timestamp with timezone data type interval [length: 16bytes] Interval data type Variable length types bpchar [length: variable] variable length text with whitespace paddings varchar [length: variable] variable length text type text [length: variable] variable length text type bytea [length: variable] variable length binary type unstructured data types jsonb [length: variable] JSON data type with binary indexed keys Note Pay attention for the two points below, when GPU processes jsonb data types. jsonb is not performance efficient data types because it has to load unreferenced attributes onto GPU from the storage, so tend to consume I/O bandwidth by junk data. In case when jsonb data length exceeds the threshold of datum TOASTen , entire jsonb value is written out to TOAST table, thus, GPU cannot process these values and invokes inefficient CPU-fallback operations. Regarding to the 2nd problem, you can extend table's storage option toast_tuple_target to enlarge the threshold for datum TOASTen. Miscellaneous types boolean [length: 1byte] Boolean data type money [length: 8bytes] Money data type uuid [length: 16bytes] UUID data type macaddr [length: 6bytes] Network MAC address data type inet [length: 7 or 19bytes] Network address data type cidr [length: 7 or 19butes] Network address data type cube [length: variable] Extra data type provided by contrib/cube Geometry data types geometry [length: variable] Geometry object of PostGIS box2df [length: 16bytes] 2-dimension bounding box (used to GiST-index)","title":"Data Types"},{"location":"ref_types/#data-types","text":"PG-Strom support the following data types for use on GPU device.","title":"Data Types"},{"location":"ref_types/#numeric-types","text":"int1 [length: 1byte] 8bit integer data type; enhanced data type by PG-Strom int2 (alias smallint ) [length: 2bytes] 16bit integer data type int4 (alias int ) [length: 4bytes] 32bit integer data type int8 (alias bigint ) [length: 8bytes] 64bit integer data type float2 [length: 2bytes] Half precision data type; enhanced data type by PG-Strom Note Even though GPU supports half-precision floating-point numbers by hardware, CPU (x86_64 processor) does not support it yet. So, when CPU processes float2 data types, it transform them to float or double on calculations. So, CPU has no advantages for calculation performance of float2 , unlike GPU. It is a feature to save storage/memory capacity for machine-learning / statistical-analytics. float4 (alias real ) [length: 4bytes] Single precision floating-point data type float8 (alias double precision ) [length: 8bytes] Double precision floating-point data type numeric [length: variable] Real number data type; handled as a 128bit fixed-point value in GPU Note When GPU processes values in numeric data type, it is converted to an internal 128bit fixed-point number because of implementation reason. (This layout is identical to Decimal type in Apache Arrow.) It is transparently converted to/from the internal format, on the other hands, PG-Strom cannot convert numaric datum with large number of digits, so tries to fallback operations by CPU. Therefore, it may lead slowdown if numeric data with large number of digits are supplied to GPU device. To avoid the problem, turn off the GUC option pg_strom.enable_numeric_type not to run operational expression including numeric data types on GPU devices.","title":"Numeric types"},{"location":"ref_types/#date-and-time-types","text":"date [length: 4bytes] Date data type time (alias time without time zone ) [length: 8bytes] Time data type timetz (alias time with time zone ) [length: 12bytes] Time with timezone data type timestamp (alias timestamp without time zone ) [length: 8bytes] Timestamp data type timestamptz (alias timestamp with time zone ) [length: 8bytes] Timestamp with timezone data type interval [length: 16bytes] Interval data type","title":"Date and time types"},{"location":"ref_types/#variable-length-types","text":"bpchar [length: variable] variable length text with whitespace paddings varchar [length: variable] variable length text type text [length: variable] variable length text type bytea [length: variable] variable length binary type","title":"Variable length types"},{"location":"ref_types/#unstructured-data-types","text":"jsonb [length: variable] JSON data type with binary indexed keys Note Pay attention for the two points below, when GPU processes jsonb data types. jsonb is not performance efficient data types because it has to load unreferenced attributes onto GPU from the storage, so tend to consume I/O bandwidth by junk data. In case when jsonb data length exceeds the threshold of datum TOASTen , entire jsonb value is written out to TOAST table, thus, GPU cannot process these values and invokes inefficient CPU-fallback operations. Regarding to the 2nd problem, you can extend table's storage option toast_tuple_target to enlarge the threshold for datum TOASTen.","title":"unstructured data types"},{"location":"ref_types/#miscellaneous-types","text":"boolean [length: 1byte] Boolean data type money [length: 8bytes] Money data type uuid [length: 16bytes] UUID data type macaddr [length: 6bytes] Network MAC address data type inet [length: 7 or 19bytes] Network address data type cidr [length: 7 or 19butes] Network address data type cube [length: variable] Extra data type provided by contrib/cube","title":"Miscellaneous types"},{"location":"ref_types/#geometry-data-types","text":"geometry [length: variable] Geometry object of PostGIS box2df [length: 16bytes] 2-dimension bounding box (used to GiST-index)","title":"Geometry data types"},{"location":"release_v2.0/","text":"PG-Strom v2.0 Release PG-Strom Development Team (17-Apr-2018) Overview Major enhancement in PG-Strom v2.0 includes: Overall redesign of the internal infrastructure to manage GPU and stabilization CPU+GPU hybrid parallel execution SSD-to-GPU Direct SQL Execution In-memory columnar cache GPU memory store (gstore_fdw) Redesign of GpuJoin and GpuPreAgg and speed-up GpuPreAgg + GpuJoin + GpuScan combined GPU kernel You can download the summary of new features from: PG-Strom v2.0 Technical Brief . Prerequisites PostgreSQL v9.6, v10 CUDA Toolkit 9.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta) New Features Entire re-design and stabilization of the internal infrastructure to manage GPU device. PostgreSQL backend process simultaneously uses only one GPU at most. In case of multi-GPUs installation, it assumes combination use with CPU parallel execution of PostgreSQL. Usually, it is not a matter because throughput of CPU to provide data to GPU is much narrower than capability of GPU processors. We prioritized simpleness of the software architecture. We began to utilize the demand paging feature of GPU device memory supported at the GPU models since Pascal generation. In most of SQL workloads, we cannot know exact size of the required result buffer prior to its execution, therefore, we had allocated more buffer than estimated buffer length, and retried piece of the workloads if estimated buffer size is not sufficient actually. This design restricts available resources of GPU which can be potentially used for other concurrent processes, and complicated error-retry logic was a nightmare for software quality. The demand paging feature allows to eliminate and simplify these stuffs. We stop to use CUDA asynchronous interface. Use of the demand paging feature on GPU device memory makes asynchronous APIs for DMA (like cuMemCpyHtoD ) perform synchronously, then it reduces concurrency and usage ratio of GPU kernels. Instead of the CUDA asynchronous APIs, PG-Strom manages its own worker threads which call synchronous APIs for each. As a by-product, we also could eliminate asynchronous callbacks ( cuStreamAddCallback ), it allows to use MPS daemon which has a restriction at this API. CPU+GPU Hybrid Parallel Execution CPU parallel execution at PostgreSQL v9.6 is newly supported. CustomScan logic of GpuScan, GpuJoin and GpuPreAgg provided by PG-Strom are executable on multiple background worker processes of PostgreSQL in parallel. Limitation: PG-Strom's own statistics displayed at EXPLAIN ANALYZE if CPU parallel execution. Because PostgreSQL v9.6 does not provide ShutdownCustomScan callback of the CustomScan interface, coordinator process has no way to reclaim information of worker processes prior to the release of DSM (Dynamic Shared Memory) segment. SSD-to-GPU Direct SQL Execution By cooperation with the nvme_strom Linux kernel module, it enables to load PostgreSQL's data blocks on NVMe-SSD to GPU device memory directly, bypassing the CPU and host buffer. This feature enables to apply PG-Strom on the area which have to process large data set more than system RAM size. It allows to pull out pretty high throughput close to the hardware limitation because its data stream skips block-device or filesystem layer. Then, GPU runs SQL workloads that usually reduce the amount of data to be processed by CPU. The chemical reaction of these characteristics enables to redefine GPU's role as accelerator of I/O workloads also, not only computing intensive workloads. In-memory Columnar Cache For middle size data-set loadable onto the system RAM, it allows to cache data-blocks in column format which is more suitable for GPU computing. If cached data-blocks are found during table scan, PG-Strom prefers to reference the columnar cache more than shared buffer of PostgreSQL. In-memory columnar cache can be built synchronously, or asynchronously by the background workers. You may remember very early revision of PG-Strom had similar feature. In case when a cached tuple gets updated, the latest in-memory columnar cache which we newly implemented in v2.0 invalidates the cache block which includes the updated tuples. It never updates the columnar cache according to the updates of row-store, so performance degradation is quite limited. GPU Memory Store (gstore_fdw) It enables to write to / read from preserved GPU device memory region by SELECT/INSERT/UPDATE/DELETE in SQL-level, using foreign table interface. In v2.0, only pgstrom internal data format is supported. It saves written data using PG-Strom's buffer format of KDS_FORMAT_COLUMN . It can compress variable length data using LZ algorithm. In v2.0, GPU memory store can be used as data source of PL/CUDA user defined function. Redesign and performance improvement of GpuJoin and GpuPreAgg Stop using Dynamic Parallelism which we internally used in GpuJoin and GpuPreAgg, and revised entire logic of these operations. Old design had a problem of less GPU usage ratio because a GPU kernel which launches GPU sub-kernel and just waits for its completion occupied GPU's execution slot. A coproduct of this redesign is suspend/resume of GpuJoin. In principle, JOIN operation of SQL may generate larger number of rows than number of input rows, but preliminary not predictive. The new design allows to suspend GPU kernel once buffer available space gets lacked, then resume with new result buffer. It simplifies size estimation logic of the result buffer, and eliminates GPU kernel retry by lack of buffer on run-time. GpuPreAgg+GpuJoin+GpuScan combined GPU kernel In case when GPU executable SCAN, JOIN and GROUP BY are serially cascaded, a single GPU kernel invocation runs a series of tasks equivalent to the GpuScan, GpuJoin and GpuPreAgg. This is an approach to minimize data exchange between CPU and GPU. For example, result buffer of GpuJoin is used as input buffer of GpuPreAgg. This feature is especially valuable if combined with SSD-to-GPU Direct SQL Execution. PL/CUDA Enhancement #plcuda_include is enhanced to specify SQL function which returns text type. It can change the code block to inject according to the argument, so it also allows to generate multiple GPU kernel variations, not only inclusion of externally defined functions. If PL/CUDA takes reggstore type argument, GPU kernel function receives pointer of the GPU memory store. Note that it does not pass the OID value. Other Enhancement lo_import_gpu and lo_export_gpu functions allows to import contents of the GPU device memory acquired by external applications directly, or export contents of the largeobject to the GPU device memory. Packaging Add RPM packages to follow the PostgreSQL packages distributed by PostgreSQL Global Development Group. All the software packages are available at HeteroDB SWDC(Software Distribution Center) and downloadable. Document PG-Strom documentation was entirely rewritten using markdown and mkdocs. It makes documentation maintenance easier than the previous HTML based approach, so expects timely updates according to the development of new features. Test Regression test for PG-Strom was built on top of the regression test framework of PostgreSQL. Dropped features PostgreSQL v9.5 Support PostgreSQL v9.6 had big changes in both of the optimizer and executor to support CPU parallel query execution. The biggest change for extension modules that interact them is an enhancement of the interface called \"upper planner path-ification\". It allows to choose an optimal execution-plan from the multiple candidates based on the estimated cost, even if it is aggregation or sorting. It is fundamentally different from the older way where we rewrote query execution plan to inject GpuPreAgg using the hooks. It allows to inject GpuPreAgg node in more reasonable and reliable way, and we could drop complicated (and buggy) logic to rewrite query execution plan once constructed. CustomScan interface is also enhanced to support CPU parallel execution. Due to the reason, we dropped PostgreSQL v9.5 support to follow these new enhancement. GpuSort feature We dropped GpuSort because we have little advantages in the performance. Sorting is one of the GPU suitable workloads. However, in case when we try to sort data blocks larger than GPU device memory, we have to split the data blocks into multiple chunks, then partially sort them and merge them by CPU to generate final results. Larger chunk size is better to reduce the load to merge multiple chunks by CPU, on the other hands, larger chunk size takes larger lead time to launch GPU kernel to sort. It means here is a trade-off; which disallows asynchronous processing by PG-Strom to make data transfer latency invisible. It is hard to solve the problem, or too early to solve the problem, we dropped GpuSort feature once.","title":"PG-Strom v2.0"},{"location":"release_v2.0/#pg-strom-v20-release","text":"PG-Strom Development Team (17-Apr-2018)","title":"PG-Strom v2.0 Release"},{"location":"release_v2.0/#overview","text":"Major enhancement in PG-Strom v2.0 includes: Overall redesign of the internal infrastructure to manage GPU and stabilization CPU+GPU hybrid parallel execution SSD-to-GPU Direct SQL Execution In-memory columnar cache GPU memory store (gstore_fdw) Redesign of GpuJoin and GpuPreAgg and speed-up GpuPreAgg + GpuJoin + GpuScan combined GPU kernel You can download the summary of new features from: PG-Strom v2.0 Technical Brief .","title":"Overview"},{"location":"release_v2.0/#prerequisites","text":"PostgreSQL v9.6, v10 CUDA Toolkit 9.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta)","title":"Prerequisites"},{"location":"release_v2.0/#new-features","text":"Entire re-design and stabilization of the internal infrastructure to manage GPU device. PostgreSQL backend process simultaneously uses only one GPU at most. In case of multi-GPUs installation, it assumes combination use with CPU parallel execution of PostgreSQL. Usually, it is not a matter because throughput of CPU to provide data to GPU is much narrower than capability of GPU processors. We prioritized simpleness of the software architecture. We began to utilize the demand paging feature of GPU device memory supported at the GPU models since Pascal generation. In most of SQL workloads, we cannot know exact size of the required result buffer prior to its execution, therefore, we had allocated more buffer than estimated buffer length, and retried piece of the workloads if estimated buffer size is not sufficient actually. This design restricts available resources of GPU which can be potentially used for other concurrent processes, and complicated error-retry logic was a nightmare for software quality. The demand paging feature allows to eliminate and simplify these stuffs. We stop to use CUDA asynchronous interface. Use of the demand paging feature on GPU device memory makes asynchronous APIs for DMA (like cuMemCpyHtoD ) perform synchronously, then it reduces concurrency and usage ratio of GPU kernels. Instead of the CUDA asynchronous APIs, PG-Strom manages its own worker threads which call synchronous APIs for each. As a by-product, we also could eliminate asynchronous callbacks ( cuStreamAddCallback ), it allows to use MPS daemon which has a restriction at this API. CPU+GPU Hybrid Parallel Execution CPU parallel execution at PostgreSQL v9.6 is newly supported. CustomScan logic of GpuScan, GpuJoin and GpuPreAgg provided by PG-Strom are executable on multiple background worker processes of PostgreSQL in parallel. Limitation: PG-Strom's own statistics displayed at EXPLAIN ANALYZE if CPU parallel execution. Because PostgreSQL v9.6 does not provide ShutdownCustomScan callback of the CustomScan interface, coordinator process has no way to reclaim information of worker processes prior to the release of DSM (Dynamic Shared Memory) segment. SSD-to-GPU Direct SQL Execution By cooperation with the nvme_strom Linux kernel module, it enables to load PostgreSQL's data blocks on NVMe-SSD to GPU device memory directly, bypassing the CPU and host buffer. This feature enables to apply PG-Strom on the area which have to process large data set more than system RAM size. It allows to pull out pretty high throughput close to the hardware limitation because its data stream skips block-device or filesystem layer. Then, GPU runs SQL workloads that usually reduce the amount of data to be processed by CPU. The chemical reaction of these characteristics enables to redefine GPU's role as accelerator of I/O workloads also, not only computing intensive workloads. In-memory Columnar Cache For middle size data-set loadable onto the system RAM, it allows to cache data-blocks in column format which is more suitable for GPU computing. If cached data-blocks are found during table scan, PG-Strom prefers to reference the columnar cache more than shared buffer of PostgreSQL. In-memory columnar cache can be built synchronously, or asynchronously by the background workers. You may remember very early revision of PG-Strom had similar feature. In case when a cached tuple gets updated, the latest in-memory columnar cache which we newly implemented in v2.0 invalidates the cache block which includes the updated tuples. It never updates the columnar cache according to the updates of row-store, so performance degradation is quite limited. GPU Memory Store (gstore_fdw) It enables to write to / read from preserved GPU device memory region by SELECT/INSERT/UPDATE/DELETE in SQL-level, using foreign table interface. In v2.0, only pgstrom internal data format is supported. It saves written data using PG-Strom's buffer format of KDS_FORMAT_COLUMN . It can compress variable length data using LZ algorithm. In v2.0, GPU memory store can be used as data source of PL/CUDA user defined function. Redesign and performance improvement of GpuJoin and GpuPreAgg Stop using Dynamic Parallelism which we internally used in GpuJoin and GpuPreAgg, and revised entire logic of these operations. Old design had a problem of less GPU usage ratio because a GPU kernel which launches GPU sub-kernel and just waits for its completion occupied GPU's execution slot. A coproduct of this redesign is suspend/resume of GpuJoin. In principle, JOIN operation of SQL may generate larger number of rows than number of input rows, but preliminary not predictive. The new design allows to suspend GPU kernel once buffer available space gets lacked, then resume with new result buffer. It simplifies size estimation logic of the result buffer, and eliminates GPU kernel retry by lack of buffer on run-time. GpuPreAgg+GpuJoin+GpuScan combined GPU kernel In case when GPU executable SCAN, JOIN and GROUP BY are serially cascaded, a single GPU kernel invocation runs a series of tasks equivalent to the GpuScan, GpuJoin and GpuPreAgg. This is an approach to minimize data exchange between CPU and GPU. For example, result buffer of GpuJoin is used as input buffer of GpuPreAgg. This feature is especially valuable if combined with SSD-to-GPU Direct SQL Execution. PL/CUDA Enhancement #plcuda_include is enhanced to specify SQL function which returns text type. It can change the code block to inject according to the argument, so it also allows to generate multiple GPU kernel variations, not only inclusion of externally defined functions. If PL/CUDA takes reggstore type argument, GPU kernel function receives pointer of the GPU memory store. Note that it does not pass the OID value. Other Enhancement lo_import_gpu and lo_export_gpu functions allows to import contents of the GPU device memory acquired by external applications directly, or export contents of the largeobject to the GPU device memory. Packaging Add RPM packages to follow the PostgreSQL packages distributed by PostgreSQL Global Development Group. All the software packages are available at HeteroDB SWDC(Software Distribution Center) and downloadable. Document PG-Strom documentation was entirely rewritten using markdown and mkdocs. It makes documentation maintenance easier than the previous HTML based approach, so expects timely updates according to the development of new features. Test Regression test for PG-Strom was built on top of the regression test framework of PostgreSQL.","title":"New Features"},{"location":"release_v2.0/#dropped-features","text":"PostgreSQL v9.5 Support PostgreSQL v9.6 had big changes in both of the optimizer and executor to support CPU parallel query execution. The biggest change for extension modules that interact them is an enhancement of the interface called \"upper planner path-ification\". It allows to choose an optimal execution-plan from the multiple candidates based on the estimated cost, even if it is aggregation or sorting. It is fundamentally different from the older way where we rewrote query execution plan to inject GpuPreAgg using the hooks. It allows to inject GpuPreAgg node in more reasonable and reliable way, and we could drop complicated (and buggy) logic to rewrite query execution plan once constructed. CustomScan interface is also enhanced to support CPU parallel execution. Due to the reason, we dropped PostgreSQL v9.5 support to follow these new enhancement. GpuSort feature We dropped GpuSort because we have little advantages in the performance. Sorting is one of the GPU suitable workloads. However, in case when we try to sort data blocks larger than GPU device memory, we have to split the data blocks into multiple chunks, then partially sort them and merge them by CPU to generate final results. Larger chunk size is better to reduce the load to merge multiple chunks by CPU, on the other hands, larger chunk size takes larger lead time to launch GPU kernel to sort. It means here is a trade-off; which disallows asynchronous processing by PG-Strom to make data transfer latency invisible. It is hard to solve the problem, or too early to solve the problem, we dropped GpuSort feature once.","title":"Dropped features"},{"location":"release_v2.2/","text":"PG-Strom v2.2 Release PG-Strom Development Team (1-May-2019) Overview Major enhancement in PG-Strom v2.2 includes: Table partitioning support Columnar store support with Arrow_Fdw Pre-built GPU binary support Enables to implement GPU functions that returns variable length data GpuSort support on GPU memory store (Gstore_Fdw) NVME-oF support (Experimental) Prerequisites PostgreSQL v9.6, v10, v11 CUDA Toolkit 10.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta) New Features Table partitioning support If multi-GPUs configuration, an optimal GPU shall be chosen according to the physical distance between GPU and child tables that construct a partition. If PG-Strom cannot identify the distance from PCIe-bus topology, like NVME-oF configuration, DBA can configure the relation of GPU and NVME-SSD using pg_strom.nvme_distance_map . When we join a partitioned table with non-partition tables, this version can produce a query execution plan that preliminary joins the non-partitioned table with partition child tables for each, and gather the results from child tables. This feature is proposed to PostgreSQL v13 core, as Asymmetric Partition-wise JOIN. Columnar store support with Arrow_Fdw It supports to read external Apache Arrow files using foreign table. It also supports SSD-to-GPU Direct SQL on Apache Arrow files. Pre-built GPU binary support When GPU binary code is generated from SQL, the older version wrote out eitire CUDA C source code, including static portions like libraries, then NVRTC(NVIDIA Run-Time Compiker) built them on the fly. However, a part of complicated function consumed much longer compilation time. v2.2 preliminary builds static functions preliminary, and only dynamic portion from SQL are built dynamically. It reduces the time for GPU binary generation. JSONB data type support This version allows to reference elements of JSONB object, and to utilize them as numeric or test . Enables to implement GPU functions that returns variable length data This version allows to implement SQL functions that returns variable-length data, like textcat , on GPU devices. GpuSort support on GPU memory store (Gstore_Fdw) This version allows to read data from GPU memory store for SQL workloads execution, not only PL/CUDA. Addition of regression test Several simple regression tests are added. NVME-oF support (Experimental) It supports SSD-to-GPU Direct SQL from remote SSD disks which are mounted using NVME-over-Fabric. Please note that it is an experimental feature, and it needs to replace the nvme_rdma kernel module on Red Hat Enterprise Linux 7.x / CentOS 7.x. Features to be deprecated PostgreSQL v9.6 support CustomScan API in PostgreSQL v9.6 lacks a few APIs to handle dynamic shared memory (DSM), so it is unable to collect run-time statistics. It also changes the way to keep expression objects internally, therefore, we had to put #if ... #endif blocks at no little points. It has damaged to code maintainability. Due to the problems, this is the last version to support PostgreSQL v9.6. If you applied PG-Strom on PostgreSQL v9.6, let us recommend to move PostgreSQL v11 as soon as possible. The pgstrom format of Gstore_Fdw foreign table The internal data format on GPU memory store (Gstore_Fdw) is originally designed for data source of PL/CUDA procedures. It is our own format, and used PostgreSQL's data representations as is, like variable-length data, numeric, and so on. After that, NVIDIA released RAPIDS(cuDF), based on Apache Arrow, for data exchange on GPU, then its adoption becomes wider on machine-learning application and Python software stack. PG-Strom will switch its internal data format of Gstore_Fdw, to improve interoperability with these machine-learning software, then existing data format shall be deprecated. Dropped Features In-memory columnar cache As results of use-case analysis, we concluded Arrow_Fdw can replace this feature in most cases. Due to feature duplication, we dropped the in-memory columnar cache.","title":"PG-Strom v2.2"},{"location":"release_v2.2/#pg-strom-v22-release","text":"PG-Strom Development Team (1-May-2019)","title":"PG-Strom v2.2 Release"},{"location":"release_v2.2/#overview","text":"Major enhancement in PG-Strom v2.2 includes: Table partitioning support Columnar store support with Arrow_Fdw Pre-built GPU binary support Enables to implement GPU functions that returns variable length data GpuSort support on GPU memory store (Gstore_Fdw) NVME-oF support (Experimental)","title":"Overview"},{"location":"release_v2.2/#prerequisites","text":"PostgreSQL v9.6, v10, v11 CUDA Toolkit 10.1 Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta)","title":"Prerequisites"},{"location":"release_v2.2/#new-features","text":"Table partitioning support If multi-GPUs configuration, an optimal GPU shall be chosen according to the physical distance between GPU and child tables that construct a partition. If PG-Strom cannot identify the distance from PCIe-bus topology, like NVME-oF configuration, DBA can configure the relation of GPU and NVME-SSD using pg_strom.nvme_distance_map . When we join a partitioned table with non-partition tables, this version can produce a query execution plan that preliminary joins the non-partitioned table with partition child tables for each, and gather the results from child tables. This feature is proposed to PostgreSQL v13 core, as Asymmetric Partition-wise JOIN. Columnar store support with Arrow_Fdw It supports to read external Apache Arrow files using foreign table. It also supports SSD-to-GPU Direct SQL on Apache Arrow files. Pre-built GPU binary support When GPU binary code is generated from SQL, the older version wrote out eitire CUDA C source code, including static portions like libraries, then NVRTC(NVIDIA Run-Time Compiker) built them on the fly. However, a part of complicated function consumed much longer compilation time. v2.2 preliminary builds static functions preliminary, and only dynamic portion from SQL are built dynamically. It reduces the time for GPU binary generation. JSONB data type support This version allows to reference elements of JSONB object, and to utilize them as numeric or test . Enables to implement GPU functions that returns variable length data This version allows to implement SQL functions that returns variable-length data, like textcat , on GPU devices. GpuSort support on GPU memory store (Gstore_Fdw) This version allows to read data from GPU memory store for SQL workloads execution, not only PL/CUDA. Addition of regression test Several simple regression tests are added. NVME-oF support (Experimental) It supports SSD-to-GPU Direct SQL from remote SSD disks which are mounted using NVME-over-Fabric. Please note that it is an experimental feature, and it needs to replace the nvme_rdma kernel module on Red Hat Enterprise Linux 7.x / CentOS 7.x.","title":"New Features"},{"location":"release_v2.2/#features-to-be-deprecated","text":"PostgreSQL v9.6 support CustomScan API in PostgreSQL v9.6 lacks a few APIs to handle dynamic shared memory (DSM), so it is unable to collect run-time statistics. It also changes the way to keep expression objects internally, therefore, we had to put #if ... #endif blocks at no little points. It has damaged to code maintainability. Due to the problems, this is the last version to support PostgreSQL v9.6. If you applied PG-Strom on PostgreSQL v9.6, let us recommend to move PostgreSQL v11 as soon as possible. The pgstrom format of Gstore_Fdw foreign table The internal data format on GPU memory store (Gstore_Fdw) is originally designed for data source of PL/CUDA procedures. It is our own format, and used PostgreSQL's data representations as is, like variable-length data, numeric, and so on. After that, NVIDIA released RAPIDS(cuDF), based on Apache Arrow, for data exchange on GPU, then its adoption becomes wider on machine-learning application and Python software stack. PG-Strom will switch its internal data format of Gstore_Fdw, to improve interoperability with these machine-learning software, then existing data format shall be deprecated.","title":"Features to be deprecated"},{"location":"release_v2.2/#dropped-features","text":"In-memory columnar cache As results of use-case analysis, we concluded Arrow_Fdw can replace this feature in most cases. Due to feature duplication, we dropped the in-memory columnar cache.","title":"Dropped Features"},{"location":"release_v2.3/","text":"PG-Strom v2.3 Release PG-Strom Development Team (1-Apr-2020) Overview Major changes in PG-Strom v2.3 includes: GpuJoin supports parallel construction of inner buffer Arrow_Fdw now becomes writable; supports INSERT/TRUNCATE. pg2arrow command supports 'append' mode. mysql2arrow command was added. Prerequisites PostgreSQL v10, v11, v12 CUDA Toolkit 10.1 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta) New Features GpuJoin supports parallel construction of inner buffer The older version construct inner buffer of GpuJoin by the backend process only. This restriction leads a problem; parallel scan of partitioned table delays extremely. This version allows both of the backend and worker processes to construct inner buffer. In case when we scan a partitioned table, any processes that is assigned to a particular child table can start GpuJoin operations immediately. Refactoring of the partition-wise asymmetric GpuJoin By the refactoring of the partition-wise asymmetric GpuJoin, optimizer becomes to prefer multi-level GpuJoin in case when it offers cheaper execution cost. Arrow_Fdw becomes writable; INSERT/TRUNCATE supported Arrow_Fdw foreign table allows bulk-loading by INSERT and data elimination by pgstrom.arrow_fdw_truncate . pg2arrow command supports 'append' mode. We added --append option for pg2arrow command. As literal, it appends query results on existing Apache Arrow file. Also, -t table option was added as an alias of SELECT * FROM table . mysql2arrow command was added. We added mysql2arrow command that connects to MySQL server, not PostgreSQL, and write out SQL query results as Apache Arrow files. It has equivalent functionality to pg2arrow except for enum data type. mysql2arrow saves enum values as flat Utf8 values without DictionaryBatch chunks. Regression test was added Several test cases were added according to the PostgreSQL regression test framework. Significant bug fixes Revised cache invalidation logic for GPU device functions / types The older version had invalidated all the metadata cache entries of GPU device functions / type on execution of ALTER command. It was revised to invalidate the entries that are actually updated. Revised extreme performance degradation if GROUP BY has same grouping key twice or even number times. GpuPreAgg combined hash values of grouping key of GROUP BY using XOR. So, if case when same column appeared even number time, it always leads 0 for hash-index problematically. Now we add a randomization for better hash distribution. Potential infinite loop on GpuScan By uninitialized values, GpuScan potentially goes to infinite loop when SSD2GPU Direct SQL is available. Potential GPU kernel crash on GpuJoin By uninitialized values, GpuJoin potentially makes GPU kernel crash when 3 or more tables are joined. Deprecated Features PostgreSQL v9.6 Support CustomScan API in PostgreSQL v9.6 lacks a few APIs to handle dynamic shared memory (DSM). It has been a problem to handle a common code for v10 or later. To avoid the problem, we dropped PostgreSQL v9.6 support in this version. PL/CUDA According to the usecase analytics, users prefer familiar programming language environment like Python, rather than own special environment. A combination of Arrow_Fdw's GPU export functionality and CuPy invocation at PL/Python is a successor of PL/CUDA, for in-database machine-learning / statistical analytics. Gstore_Fdw This feature is replaced by the writable Arrow_Fdw and its GPU export functionality. Largeobject export to/import from GPU According to the usecase analytics, we determined this feature is not needed.","title":"PG-Strom v2.3"},{"location":"release_v2.3/#pg-strom-v23-release","text":"PG-Strom Development Team (1-Apr-2020)","title":"PG-Strom v2.3 Release"},{"location":"release_v2.3/#overview","text":"Major changes in PG-Strom v2.3 includes: GpuJoin supports parallel construction of inner buffer Arrow_Fdw now becomes writable; supports INSERT/TRUNCATE. pg2arrow command supports 'append' mode. mysql2arrow command was added.","title":"Overview"},{"location":"release_v2.3/#prerequisites","text":"PostgreSQL v10, v11, v12 CUDA Toolkit 10.1 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or Volta)","title":"Prerequisites"},{"location":"release_v2.3/#new-features","text":"GpuJoin supports parallel construction of inner buffer The older version construct inner buffer of GpuJoin by the backend process only. This restriction leads a problem; parallel scan of partitioned table delays extremely. This version allows both of the backend and worker processes to construct inner buffer. In case when we scan a partitioned table, any processes that is assigned to a particular child table can start GpuJoin operations immediately. Refactoring of the partition-wise asymmetric GpuJoin By the refactoring of the partition-wise asymmetric GpuJoin, optimizer becomes to prefer multi-level GpuJoin in case when it offers cheaper execution cost. Arrow_Fdw becomes writable; INSERT/TRUNCATE supported Arrow_Fdw foreign table allows bulk-loading by INSERT and data elimination by pgstrom.arrow_fdw_truncate . pg2arrow command supports 'append' mode. We added --append option for pg2arrow command. As literal, it appends query results on existing Apache Arrow file. Also, -t table option was added as an alias of SELECT * FROM table . mysql2arrow command was added. We added mysql2arrow command that connects to MySQL server, not PostgreSQL, and write out SQL query results as Apache Arrow files. It has equivalent functionality to pg2arrow except for enum data type. mysql2arrow saves enum values as flat Utf8 values without DictionaryBatch chunks. Regression test was added Several test cases were added according to the PostgreSQL regression test framework.","title":"New Features"},{"location":"release_v2.3/#significant-bug-fixes","text":"Revised cache invalidation logic for GPU device functions / types The older version had invalidated all the metadata cache entries of GPU device functions / type on execution of ALTER command. It was revised to invalidate the entries that are actually updated. Revised extreme performance degradation if GROUP BY has same grouping key twice or even number times. GpuPreAgg combined hash values of grouping key of GROUP BY using XOR. So, if case when same column appeared even number time, it always leads 0 for hash-index problematically. Now we add a randomization for better hash distribution. Potential infinite loop on GpuScan By uninitialized values, GpuScan potentially goes to infinite loop when SSD2GPU Direct SQL is available. Potential GPU kernel crash on GpuJoin By uninitialized values, GpuJoin potentially makes GPU kernel crash when 3 or more tables are joined.","title":"Significant bug fixes"},{"location":"release_v2.3/#deprecated-features","text":"PostgreSQL v9.6 Support CustomScan API in PostgreSQL v9.6 lacks a few APIs to handle dynamic shared memory (DSM). It has been a problem to handle a common code for v10 or later. To avoid the problem, we dropped PostgreSQL v9.6 support in this version. PL/CUDA According to the usecase analytics, users prefer familiar programming language environment like Python, rather than own special environment. A combination of Arrow_Fdw's GPU export functionality and CuPy invocation at PL/Python is a successor of PL/CUDA, for in-database machine-learning / statistical analytics. Gstore_Fdw This feature is replaced by the writable Arrow_Fdw and its GPU export functionality. Largeobject export to/import from GPU According to the usecase analytics, we determined this feature is not needed.","title":"Deprecated Features"},{"location":"release_v3.0/","text":"PG-Strom v3.0 Release PG-Strom Development Team (29-Jun-2021) Overview Major changes in PG-Strom v3.0 are as follows: NVIDIA GPUDirect Storage (cuFile) is now supported. Several PostGIS functions are executable on GPUs. GpuJoin using GiST index is now supported. GPU Cache mechanism is newly implemented. User-defined GPU data types/functions/operators are experimentally supported. Software license was switched from GPLv2 to PostgreSQL license. Prerequisites PostgreSQL v11, v12, v13 CUDA Toolkit 11.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or newer) NVIDIA GPUDirect Storage GPUDirect Storage , has been developed by NVIDIA, is now supported as a driver for GPU Direct SQL , in addition to the existing nvme_strom kernel module. Both of drivers have almost equivalent functionalities and performance, but supports of GPUDirect Storage enables P2P direct read from NVME-oF (NVME over Fabrics) devices, SDS (Software Defined Storage) devices and shared filesystem built on these devices. Therefore, it offers larger and more flexible storage configuration. We can use GPUDirect SQL to scan PostgreSQL's heap table and Apache Arrow files. It can expect significant performance improvement for the workloads where table scans are the major bottleneck, in either driver cases. The performance measurement below is by SSBM (Star Schema Benchmark) using 1xGPU and 4xNVME-SSDs under the GPUDirect Storage driver. It shows number of rows processed per unit time is significantly improved regardless of the storage system; either PostgreSQL heap or Apache Arrow. In comparison of the read throughput from NVME-SSD drives during the query execution, it shows the table scan by GPUDirect Storage pulls out almost optimal performance close to the hardware limitation, much faster than the scan by filesystem (PostgreSQL Heap Storage). GPU-PostGIS and GiST-index We have implemented GPU versions of several PostGIS functions. When these PostGIS functions are used in qualifier clauses (like, WHERE-clause), PG-Strom will automatically generate a GPU program to execute it on the GPU. The main target of GPU version of PostGIS is the workload to check the real-time location data of mobile devices, like smartphones or vehicles, against the area definition data like boundary of municipality or school districts. For example, when you want to deliver an advertisement to smartphonws in a particular area, or when you want to deliver traffic jam information to cara in a particular area, it is effective in the process of searching for the corresponding device using the position as a key. In the following example, it creates 16 million random points data in a rectangular area that includes the Tokyo region, then count up number of the points contained in the cities in Tokyo for each. The vanilla PostGIS and GiST index took more than 160sec, on the other hand, GPU-version of PostGIS and GiST index responded in 0.830 sec. GPU Cache GPU Cache mechanism can store a copy of the target table in a pre-allocated area on the GPU device memory. It was designed for efficient execution of analytical/search queries on frequently updated data with relatively small data size (~10GB). The GPU can process SQL workloads by referring to GPU Cache instead of loading data from tables when executing analytical/search queries. This is typically a workload that keeps real-time data from millions of devices on the GPU and frequently updates timestamps and location information. When the table with GPU cache is updated, the update history is stored in the on-memory redo log buffer, then applied to the GPU cache at a regular intervals or before executing the analysis / search workload. By this mechanism, it achieved both of frequent updates and consistency of GPU cache. User-defined GPU datatype/functions A new API is provided to add user-defined GPU data types/functions. This allows users to define and implement their own niche data types and SQL functions to process them, without modifying PG-Strom itself. Notice This API is still under the experimental state, so its specifications may be changed without notifications. Also note that we assume the users of this API well understand PG-Strom internal, so no documentations are provided right now. PostgreSQL License Adoption PG-Strom v3.0 or later adopt the PostgreSQL License. The earlier version of PG-Strom has used GPLv2 due to the historical background, however, we recognized several concerns that license mismatch prevents joint solution development using PG-Strom core features and comprehensive tools. Other updates Unique int1 (8-bit integer) data type and related operators are now supported. --inner-join and --outer-join options are now available for pg2arrow . Apache Arrow files having more columns than the limit of PostgreSQL can now be generated. In a multi-GPU environment, the GPU Memory Keeper background worker will now be launched for each GPU. PostgreSQL v13.x is now supported. CUDA 11.2 and Ampere generation GPUs are now supported. GPUDirect SQL now supports ScaleFlux's Computational Storage CSD2000 series (only cuFile driver). Miscellaneous bug fixes Deprecated Features Support for PostgreSQL v10.x has been discontinued. The feature to link data with Python scripts (PyStrom) has been discontinued.","title":"PG-Strom v3.0"},{"location":"release_v3.0/#pg-strom-v30-release","text":"PG-Strom Development Team (29-Jun-2021)","title":"PG-Strom v3.0 Release"},{"location":"release_v3.0/#overview","text":"Major changes in PG-Strom v3.0 are as follows: NVIDIA GPUDirect Storage (cuFile) is now supported. Several PostGIS functions are executable on GPUs. GpuJoin using GiST index is now supported. GPU Cache mechanism is newly implemented. User-defined GPU data types/functions/operators are experimentally supported. Software license was switched from GPLv2 to PostgreSQL license.","title":"Overview"},{"location":"release_v3.0/#prerequisites","text":"PostgreSQL v11, v12, v13 CUDA Toolkit 11.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal or newer)","title":"Prerequisites"},{"location":"release_v3.0/#nvidia-gpudirect-storage","text":"GPUDirect Storage , has been developed by NVIDIA, is now supported as a driver for GPU Direct SQL , in addition to the existing nvme_strom kernel module. Both of drivers have almost equivalent functionalities and performance, but supports of GPUDirect Storage enables P2P direct read from NVME-oF (NVME over Fabrics) devices, SDS (Software Defined Storage) devices and shared filesystem built on these devices. Therefore, it offers larger and more flexible storage configuration. We can use GPUDirect SQL to scan PostgreSQL's heap table and Apache Arrow files. It can expect significant performance improvement for the workloads where table scans are the major bottleneck, in either driver cases. The performance measurement below is by SSBM (Star Schema Benchmark) using 1xGPU and 4xNVME-SSDs under the GPUDirect Storage driver. It shows number of rows processed per unit time is significantly improved regardless of the storage system; either PostgreSQL heap or Apache Arrow. In comparison of the read throughput from NVME-SSD drives during the query execution, it shows the table scan by GPUDirect Storage pulls out almost optimal performance close to the hardware limitation, much faster than the scan by filesystem (PostgreSQL Heap Storage).","title":"NVIDIA GPUDirect Storage"},{"location":"release_v3.0/#gpu-postgis-and-gist-index","text":"We have implemented GPU versions of several PostGIS functions. When these PostGIS functions are used in qualifier clauses (like, WHERE-clause), PG-Strom will automatically generate a GPU program to execute it on the GPU. The main target of GPU version of PostGIS is the workload to check the real-time location data of mobile devices, like smartphones or vehicles, against the area definition data like boundary of municipality or school districts. For example, when you want to deliver an advertisement to smartphonws in a particular area, or when you want to deliver traffic jam information to cara in a particular area, it is effective in the process of searching for the corresponding device using the position as a key. In the following example, it creates 16 million random points data in a rectangular area that includes the Tokyo region, then count up number of the points contained in the cities in Tokyo for each. The vanilla PostGIS and GiST index took more than 160sec, on the other hand, GPU-version of PostGIS and GiST index responded in 0.830 sec.","title":"GPU-PostGIS and GiST-index"},{"location":"release_v3.0/#gpu-cache","text":"GPU Cache mechanism can store a copy of the target table in a pre-allocated area on the GPU device memory. It was designed for efficient execution of analytical/search queries on frequently updated data with relatively small data size (~10GB). The GPU can process SQL workloads by referring to GPU Cache instead of loading data from tables when executing analytical/search queries. This is typically a workload that keeps real-time data from millions of devices on the GPU and frequently updates timestamps and location information. When the table with GPU cache is updated, the update history is stored in the on-memory redo log buffer, then applied to the GPU cache at a regular intervals or before executing the analysis / search workload. By this mechanism, it achieved both of frequent updates and consistency of GPU cache.","title":"GPU Cache"},{"location":"release_v3.0/#user-defined-gpu-datatypefunctions","text":"A new API is provided to add user-defined GPU data types/functions. This allows users to define and implement their own niche data types and SQL functions to process them, without modifying PG-Strom itself. Notice This API is still under the experimental state, so its specifications may be changed without notifications. Also note that we assume the users of this API well understand PG-Strom internal, so no documentations are provided right now.","title":"User-defined GPU datatype/functions"},{"location":"release_v3.0/#postgresql-license-adoption","text":"PG-Strom v3.0 or later adopt the PostgreSQL License. The earlier version of PG-Strom has used GPLv2 due to the historical background, however, we recognized several concerns that license mismatch prevents joint solution development using PG-Strom core features and comprehensive tools.","title":"PostgreSQL License Adoption"},{"location":"release_v3.0/#other-updates","text":"Unique int1 (8-bit integer) data type and related operators are now supported. --inner-join and --outer-join options are now available for pg2arrow . Apache Arrow files having more columns than the limit of PostgreSQL can now be generated. In a multi-GPU environment, the GPU Memory Keeper background worker will now be launched for each GPU. PostgreSQL v13.x is now supported. CUDA 11.2 and Ampere generation GPUs are now supported. GPUDirect SQL now supports ScaleFlux's Computational Storage CSD2000 series (only cuFile driver). Miscellaneous bug fixes","title":"Other updates"},{"location":"release_v3.0/#deprecated-features","text":"Support for PostgreSQL v10.x has been discontinued. The feature to link data with Python scripts (PyStrom) has been discontinued.","title":"Deprecated Features"},{"location":"release_v5.0/","text":"PG-Strom v5.0 Release PG-Strom Development Team (15-Dec-2023) Overview Major changes in PG-Strom v5.0 are as follows: The code base has been re-designed entirely with various improvement. Process model was revised to multi-threaded background worker (PG-Strom GPU Service) from multi-process model. It reduced GPU resource consumption and overhead of task-switching. GPU device code dynamically generated using CUDA C++ was replaced by the pseudo kernel code. It eliminates just-in-time compilation using NVRTC, and improved the first response time. This is also a groundwork for the future support of CSD(Computational Storage Drive) and DPU(Data Processing Unit). GPU-Cache is now deployed on CUDA managed memory that allows overcommit of GPU device memory. Data layout of PostgreSQL data types were revised to adjust Coalesced Memory Access. GpuPreAgg replaced entire GROUP BY implementation, then improved whole performance. GpuJoin extract tuples only once regardless of the depth of Join. Arrow_Fdw and Pg2Arrow supports min/max statistics of arrow files. Two arrow tools were added: Pca2Arrow captures network packets, and arrow2csv dumps arrow files in CSV format. Prerequisites PostgreSQL v15.x, v16.x CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Volta or newer is recommended) New Process Model In v5.0, the multi-threaded background worker process (PG-Strom GPU Service) coordinates GPU resources and task executions, and individual PostgreSQL backend processes send requests to and receive results from the GPU service over IPC. Before the v3.x series, each PostgreSQL backend controls GPU devices individually. This design helps software debugging by easy identification of the problematic code when software quality of CUDA and PG-Strom were not sufficient, however, it extremely consumed GPU resources according to increase of database sessions, and was not recommended software architecture from the standpoint of task-switching. This design change makes PG-Strom v5.0 more stable towards increase of concurrent database sessions, and improves heavy GPU task's performance. Pseudo device code PG-Strom v5.0 now generates its own \"pseudo-code\" from the supplied SQL, and the GPU device code works as an interpreter to execute this \"pseudo-code\". Unlike v3.x series, it does not generate CUDA C++ native code no longer. At first glance, this may appear to be a factor in performance degradation. However, dynamic code generation was originally targeted for only a small part of the code that changes with each query, such as the WHERE clause; most implementations were statically built, and runtime compilation was handled by NVRTC. (approximately 150ms) can now be omitted, contributing to improved response time. The \"pseudo-code\" is a set of low-level commands, can be displayed in EXPLAIN VERBOSE. For example, the query below contains the expression lo_quantity > 10 in the WHERE clause. This operation is defined as Scan Quals OpCode to call the numeric_gt function which compares the magnitude relationship between the lo_quantity column and the constant 10 . postgres=# explain verbose select count(*), sum(lo_quantity), lo_shipmode from lineorder where lo_quantity > 10 group by lo_shipmode; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- HashAggregate (cost=3242387.01..3242387.10 rows=7 width=51) Output: pgstrom.fcount((pgstrom.nrows())), pgstrom.sum_fp_num((pgstrom.psum((lo_quantity)::double precision))), lo_shipmode Group Key: lineorder.lo_shipmode -> Custom Scan (GpuPreAgg) on public.lineorder (cost=3242386.89..3242386.96 rows=7 width=51) Output: (pgstrom.nrows()), (pgstrom.psum((lo_quantity)::double precision)), lo_shipmode GPU Projection: pgstrom.nrows(), pgstrom.psum((lo_quantity)::double precision), lo_shipmode GPU Scan Quals: (lo_quantity > '10'::numeric) [rows: 600128800 -> 479262800] GPU-Direct SQL: enabled (GPU-0) KVars-Slot: <slot=0, type='numeric', expr='lo_quantity'>, <slot=1, type='bpchar', expr='lo_shipmode'>, <slot=2, type='bpchar', expr='lo_shipmode'>, <slot=3, type='float8', expr='lo_quantity'> KVecs-Buffer: nbytes: 83968, ndims: 2, items=[kvec0=<0x0000-dfff, type='numeric', expr='lo_quantity'>, kvec1=<0xe000-147ff, type='bpchar', expr='lo_shipmode'>] LoadVars OpCode: {Packed items[0]={LoadVars(depth=0): kvars=[<slot=0, type='numeric' resno=9(lo_quantity)>, <slot=1, type='bpchar' resno=17(lo_shipmode)>]}} MoveVars OpCode: {Packed items[0]={MoveVars(depth=0): items=[<slot=0, offset=0x0000-dfff, type='numeric', expr='lo_quantity'>, <slot=1, offset=0xe000-147ff, type='bpchar', expr='lo_shipmode'>]}}} Scan Quals OpCode: {Func(bool)::numeric_gt args=[{Var(numeric): slot=0, expr='lo_quantity'}, {Const(numeric): value='10'}]} Group-By KeyHash OpCode: {HashValue arg={SaveExpr: <slot=1, type='bpchar'> arg={Var(bpchar): kvec=0xe000-14800, expr='lo_shipmode'}}} Group-By KeyLoad OpCode: {LoadVars(depth=-2): kvars=[<slot=2, type='bpchar' resno=3(lo_shipmode)>]} Group-By KeyComp OpCode: {Func(bool)::bpchareq args=[{Var(bpchar): slot=1, expr='lo_shipmode'}, {Var(bpchar): slot=2, expr='lo_shipmode'}]} Partial Aggregation OpCode: {AggFuncs <nrows[*], psum::fp[slot=3, expr='lo_quantity'], vref[slot=1, expr='lo_shipmode']> args=[{SaveExpr: <slot=3, type='float8'> arg={Func(float8)::float8 arg={Var(numeric): kvec=0x0000-e000, expr='lo_quantity'}}}, {SaveExpr: <slot=1, type='bpchar'> arg={Var(bpchar): kvec=0xe000-14800, expr='lo_shipmode'}}]} Partial Function BufSz: 24 (18 rows) Although not currently implemented, this pseudo-code is also designed to offload SQL processing to a CSD (Computational Storage Drive) or DPU (Data Processing Unit) in the future. Improvement of data layout GPU has a wider memory bandwidth than CPU, but in order to take advantage of this performance, it is necessary to satisfy the condition of coalesced memory access, which accesses nearby memory areas at the same time. In v5.0, the layout of PostgreSQL data types in GPU device code has been improved to make them more suitable for Coalesced Memory Access. If we would use the PostgreSQL data type as is, fields that are referenced at certain times will be placed in discrete positions, making it impossible to effectively utilize the read bandwidth from DRAM. By arranging multiple of these for each field, adjacent cores can read data from adjacent areas, making it easier to satisfy the conditions of Coalesced Memory Access. This improvement is aimed at bringing out sufficient execution performance not only for high-end GPU products with extremely high performance memory bandwidth, but also for mid-end GPUs. Arrow_Fdw supports min/max statistics Pg2Arrow can now generate Apache Arrow files with min/max statistics. Its new option --stat=COLUMN_NAME records the maximum/minimum value of the specified column for each RecordBatch and embeds it in the footer using Apache Arrow's Custom-Metadata mechanism. When reading an Apache Arrow file using Arrow_Fdw, perform like a range index scan using the above min/max statistics. For example, if the WHERE-clause for the Arrow_Fdw foreign table is as follows: WHERE ymd BETERRN '2020-01-01'::date AND '2021-12-31'::date Arrow_Fdw will skip the record-batch where the maximum value of the ymd field is less than '2020-01-01'::date , or the record-batch where the minimum value of ymd field is greater than '2021-12-31 ::date`, because it is obvious that it does not match the search conditions. As a result, performance equivalent to narrowing down using a range index can be obtained for datasets with patterns in which records with similar values \u200b\u200bare clustered nearby, such as log data timestamps. Other changes PG-Strom v5.0 stopped support of PostgreSQL v14 or older. Plan version up v15 or later. Due to development schedule reason, v5.0 disables partition-wise GpuJoin. It shall be re-implemented at the near future version.","title":"PG-Strom v5.0"},{"location":"release_v5.0/#pg-strom-v50-release","text":"PG-Strom Development Team (15-Dec-2023)","title":"PG-Strom v5.0 Release"},{"location":"release_v5.0/#overview","text":"Major changes in PG-Strom v5.0 are as follows: The code base has been re-designed entirely with various improvement. Process model was revised to multi-threaded background worker (PG-Strom GPU Service) from multi-process model. It reduced GPU resource consumption and overhead of task-switching. GPU device code dynamically generated using CUDA C++ was replaced by the pseudo kernel code. It eliminates just-in-time compilation using NVRTC, and improved the first response time. This is also a groundwork for the future support of CSD(Computational Storage Drive) and DPU(Data Processing Unit). GPU-Cache is now deployed on CUDA managed memory that allows overcommit of GPU device memory. Data layout of PostgreSQL data types were revised to adjust Coalesced Memory Access. GpuPreAgg replaced entire GROUP BY implementation, then improved whole performance. GpuJoin extract tuples only once regardless of the depth of Join. Arrow_Fdw and Pg2Arrow supports min/max statistics of arrow files. Two arrow tools were added: Pca2Arrow captures network packets, and arrow2csv dumps arrow files in CSV format.","title":"Overview"},{"location":"release_v5.0/#prerequisites","text":"PostgreSQL v15.x, v16.x CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Volta or newer is recommended)","title":"Prerequisites"},{"location":"release_v5.0/#new-process-model","text":"In v5.0, the multi-threaded background worker process (PG-Strom GPU Service) coordinates GPU resources and task executions, and individual PostgreSQL backend processes send requests to and receive results from the GPU service over IPC. Before the v3.x series, each PostgreSQL backend controls GPU devices individually. This design helps software debugging by easy identification of the problematic code when software quality of CUDA and PG-Strom were not sufficient, however, it extremely consumed GPU resources according to increase of database sessions, and was not recommended software architecture from the standpoint of task-switching. This design change makes PG-Strom v5.0 more stable towards increase of concurrent database sessions, and improves heavy GPU task's performance.","title":"New Process Model"},{"location":"release_v5.0/#pseudo-device-code","text":"PG-Strom v5.0 now generates its own \"pseudo-code\" from the supplied SQL, and the GPU device code works as an interpreter to execute this \"pseudo-code\". Unlike v3.x series, it does not generate CUDA C++ native code no longer. At first glance, this may appear to be a factor in performance degradation. However, dynamic code generation was originally targeted for only a small part of the code that changes with each query, such as the WHERE clause; most implementations were statically built, and runtime compilation was handled by NVRTC. (approximately 150ms) can now be omitted, contributing to improved response time. The \"pseudo-code\" is a set of low-level commands, can be displayed in EXPLAIN VERBOSE. For example, the query below contains the expression lo_quantity > 10 in the WHERE clause. This operation is defined as Scan Quals OpCode to call the numeric_gt function which compares the magnitude relationship between the lo_quantity column and the constant 10 . postgres=# explain verbose select count(*), sum(lo_quantity), lo_shipmode from lineorder where lo_quantity > 10 group by lo_shipmode; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- HashAggregate (cost=3242387.01..3242387.10 rows=7 width=51) Output: pgstrom.fcount((pgstrom.nrows())), pgstrom.sum_fp_num((pgstrom.psum((lo_quantity)::double precision))), lo_shipmode Group Key: lineorder.lo_shipmode -> Custom Scan (GpuPreAgg) on public.lineorder (cost=3242386.89..3242386.96 rows=7 width=51) Output: (pgstrom.nrows()), (pgstrom.psum((lo_quantity)::double precision)), lo_shipmode GPU Projection: pgstrom.nrows(), pgstrom.psum((lo_quantity)::double precision), lo_shipmode GPU Scan Quals: (lo_quantity > '10'::numeric) [rows: 600128800 -> 479262800] GPU-Direct SQL: enabled (GPU-0) KVars-Slot: <slot=0, type='numeric', expr='lo_quantity'>, <slot=1, type='bpchar', expr='lo_shipmode'>, <slot=2, type='bpchar', expr='lo_shipmode'>, <slot=3, type='float8', expr='lo_quantity'> KVecs-Buffer: nbytes: 83968, ndims: 2, items=[kvec0=<0x0000-dfff, type='numeric', expr='lo_quantity'>, kvec1=<0xe000-147ff, type='bpchar', expr='lo_shipmode'>] LoadVars OpCode: {Packed items[0]={LoadVars(depth=0): kvars=[<slot=0, type='numeric' resno=9(lo_quantity)>, <slot=1, type='bpchar' resno=17(lo_shipmode)>]}} MoveVars OpCode: {Packed items[0]={MoveVars(depth=0): items=[<slot=0, offset=0x0000-dfff, type='numeric', expr='lo_quantity'>, <slot=1, offset=0xe000-147ff, type='bpchar', expr='lo_shipmode'>]}}} Scan Quals OpCode: {Func(bool)::numeric_gt args=[{Var(numeric): slot=0, expr='lo_quantity'}, {Const(numeric): value='10'}]} Group-By KeyHash OpCode: {HashValue arg={SaveExpr: <slot=1, type='bpchar'> arg={Var(bpchar): kvec=0xe000-14800, expr='lo_shipmode'}}} Group-By KeyLoad OpCode: {LoadVars(depth=-2): kvars=[<slot=2, type='bpchar' resno=3(lo_shipmode)>]} Group-By KeyComp OpCode: {Func(bool)::bpchareq args=[{Var(bpchar): slot=1, expr='lo_shipmode'}, {Var(bpchar): slot=2, expr='lo_shipmode'}]} Partial Aggregation OpCode: {AggFuncs <nrows[*], psum::fp[slot=3, expr='lo_quantity'], vref[slot=1, expr='lo_shipmode']> args=[{SaveExpr: <slot=3, type='float8'> arg={Func(float8)::float8 arg={Var(numeric): kvec=0x0000-e000, expr='lo_quantity'}}}, {SaveExpr: <slot=1, type='bpchar'> arg={Var(bpchar): kvec=0xe000-14800, expr='lo_shipmode'}}]} Partial Function BufSz: 24 (18 rows) Although not currently implemented, this pseudo-code is also designed to offload SQL processing to a CSD (Computational Storage Drive) or DPU (Data Processing Unit) in the future.","title":"Pseudo device code"},{"location":"release_v5.0/#improvement-of-data-layout","text":"GPU has a wider memory bandwidth than CPU, but in order to take advantage of this performance, it is necessary to satisfy the condition of coalesced memory access, which accesses nearby memory areas at the same time. In v5.0, the layout of PostgreSQL data types in GPU device code has been improved to make them more suitable for Coalesced Memory Access. If we would use the PostgreSQL data type as is, fields that are referenced at certain times will be placed in discrete positions, making it impossible to effectively utilize the read bandwidth from DRAM. By arranging multiple of these for each field, adjacent cores can read data from adjacent areas, making it easier to satisfy the conditions of Coalesced Memory Access. This improvement is aimed at bringing out sufficient execution performance not only for high-end GPU products with extremely high performance memory bandwidth, but also for mid-end GPUs.","title":"Improvement of data layout"},{"location":"release_v5.0/#arrow_fdw-supports-minmax-statistics","text":"Pg2Arrow can now generate Apache Arrow files with min/max statistics. Its new option --stat=COLUMN_NAME records the maximum/minimum value of the specified column for each RecordBatch and embeds it in the footer using Apache Arrow's Custom-Metadata mechanism. When reading an Apache Arrow file using Arrow_Fdw, perform like a range index scan using the above min/max statistics. For example, if the WHERE-clause for the Arrow_Fdw foreign table is as follows: WHERE ymd BETERRN '2020-01-01'::date AND '2021-12-31'::date Arrow_Fdw will skip the record-batch where the maximum value of the ymd field is less than '2020-01-01'::date , or the record-batch where the minimum value of ymd field is greater than '2021-12-31 ::date`, because it is obvious that it does not match the search conditions. As a result, performance equivalent to narrowing down using a range index can be obtained for datasets with patterns in which records with similar values \u200b\u200bare clustered nearby, such as log data timestamps.","title":"Arrow_Fdw supports min/max statistics"},{"location":"release_v5.0/#other-changes","text":"PG-Strom v5.0 stopped support of PostgreSQL v14 or older. Plan version up v15 or later. Due to development schedule reason, v5.0 disables partition-wise GpuJoin. It shall be re-implemented at the near future version.","title":"Other changes"},{"location":"release_v5.1/","text":"PG-Strom v5.1 Release PG-Strom Development Team (17-Apr-2024) Overview Major changes in PG-Strom v5.1 are as follows: Added support for partition-wise GPU-Join/PreAgg. GPU code is now built in the execution environment at startup. pg2arrow now support parallel execution. CUDA Stack size is now set adaptically. Cumulative bug fixes Prerequisites PostgreSQL v15.x, v16.x CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Volta or newer is recommended) Partition-wise GpuJoin/GpuPreAgg Support for PostgreSQL partitions itself was also included in PG-Strom v3.0, but execution plans often could not be created properly, therefore it could not be moved out of its experimental status. Then, in PG-Strom v5.1, we fundamentally revised the internal design, re-implemented it, and incorporated it as an official feature again. If the lineorder table below is partitioned and the date1 table is a non-partitioned table, previously all the data read from the partitioned tables under lineorder must be joined with date1 table after the consolidation of all the partition leafs by the Append node. Usually, PG-Strom bypasses the CPU and loads data from the NVME-SSD to the GPU to perform various SQL processing (GPU-Direct SQL), so the data must be returned to the CPU before JOIN. It has been a big penalty. ssbm=# explain (costs off) select sum(lo_extendedprice*lo_discount) as revenue from lineorder,date1 where lo_orderdate = d_datekey and d_year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- Aggregate -> Hash Join Hash Cond: (lineorder.lo_orderdate = date1.d_datekey) -> Append -> Custom Scan (GpuScan) on lineorder__p1992 lineorder_1 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91250920 -> 11911380] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1993 lineorder_2 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91008500 -> 11980460] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1994 lineorder_3 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91044060 -> 12150700] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1995 lineorder_4 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91011720 -> 11779920] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1996 lineorder_5 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91305650 -> 11942810] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1997 lineorder_6 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91049100 -> 12069740] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1998 lineorder_7 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 53370560 -> 6898138] GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on lineorder__p1999 lineorder_8 Filter: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) -> Hash -> Seq Scan on date1 Filter: (d_year = 1993) (37 rows) In PG-Strom v5.1, it is now possible to push-down JOINs with non-partitioned tables to partitioned child tables. In some cases, it is also possible to complete the GROUP-BY processing and then return much smaller results to CPU. For example, in the example below, 70 million rows extracted from a total of 600 million rows in the partitioned child tables. By performing a JOIN with the non-partitioned table date1 and then aggregation function SUM() pushed-down to the partitioned child tables, the CPU only needs to process 8 rows. Although there is a disadvantage that reading on the INNER side occurs multiple times (* This will be fixed in a future version), this type of rewriting will significantly reduce the amount of data that must be processed by the CPU, contributing to improved processing speed. To do. ssbm=# explain (costs off) select sum(lo_extendedprice*lo_discount) as revenue from lineorder,date1 where lo_orderdate = d_datekey and d_year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ---------------------------------------------------------------------------------------------------- Aggregate -> Append -> Custom Scan (GpuPreAgg) on lineorder__p1992 lineorder_1 GPU Projection: pgstrom.psum(((lineorder_1.lo_extendedprice * lineorder_1.lo_discount))::double precision) GPU Scan Quals: ((lineorder_1.lo_discount >= '1'::numeric) AND (lineorder_1.lo_discount <= '3'::numeric) AND (lineorder_1.lo_quantity < '25'::numeric)) [rows: 91250920 -> 11911380] GPU Join Quals [1]: (lineorder_1.lo_orderdate = date1.d_datekey) ... [nrows: 11911380 -> 1700960] GPU Outer Hash [1]: lineorder_1.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1993 lineorder_2 GPU Projection: pgstrom.psum(((lineorder_2.lo_extendedprice * lineorder_2.lo_discount))::double precision) GPU Scan Quals: ((lineorder_2.lo_discount >= '1'::numeric) AND (lineorder_2.lo_discount <= '3'::numeric) AND (lineorder_2.lo_quantity < '25'::numeric)) [rows: 91008500 -> 11980460] GPU Join Quals [1]: (lineorder_2.lo_orderdate = date1.d_datekey) ... [nrows: 11980460 -> 1710824] GPU Outer Hash [1]: lineorder_2.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1994 lineorder_3 GPU Projection: pgstrom.psum(((lineorder_3.lo_extendedprice * lineorder_3.lo_discount))::double precision) GPU Scan Quals: ((lineorder_3.lo_discount >= '1'::numeric) AND (lineorder_3.lo_discount <= '3'::numeric) AND (lineorder_3.lo_quantity < '25'::numeric)) [rows: 91044060 -> 12150700] GPU Join Quals [1]: (lineorder_3.lo_orderdate = date1.d_datekey) ... [nrows: 12150700 -> 1735135] GPU Outer Hash [1]: lineorder_3.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1995 lineorder_4 GPU Projection: pgstrom.psum(((lineorder_4.lo_extendedprice * lineorder_4.lo_discount))::double precision) GPU Scan Quals: ((lineorder_4.lo_discount >= '1'::numeric) AND (lineorder_4.lo_discount <= '3'::numeric) AND (lineorder_4.lo_quantity < '25'::numeric)) [rows: 91011720 -> 11779920] GPU Join Quals [1]: (lineorder_4.lo_orderdate = date1.d_datekey) ... [nrows: 11779920 -> 1682188] GPU Outer Hash [1]: lineorder_4.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1996 lineorder_5 GPU Projection: pgstrom.psum(((lineorder_5.lo_extendedprice * lineorder_5.lo_discount))::double precision) GPU Scan Quals: ((lineorder_5.lo_discount >= '1'::numeric) AND (lineorder_5.lo_discount <= '3'::numeric) AND (lineorder_5.lo_quantity < '25'::numeric)) [rows: 91305650 -> 11942810] GPU Join Quals [1]: (lineorder_5.lo_orderdate = date1.d_datekey) ... [nrows: 11942810 -> 1705448] GPU Outer Hash [1]: lineorder_5.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1997 lineorder_6 GPU Projection: pgstrom.psum(((lineorder_6.lo_extendedprice * lineorder_6.lo_discount))::double precision) GPU Scan Quals: ((lineorder_6.lo_discount >= '1'::numeric) AND (lineorder_6.lo_discount <= '3'::numeric) AND (lineorder_6.lo_quantity < '25'::numeric)) [rows: 91049100 -> 12069740] GPU Join Quals [1]: (lineorder_6.lo_orderdate = date1.d_datekey) ... [nrows: 12069740 -> 1723574] GPU Outer Hash [1]: lineorder_6.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1998 lineorder_7 GPU Projection: pgstrom.psum(((lineorder_7.lo_extendedprice * lineorder_7.lo_discount))::double precision) GPU Scan Quals: ((lineorder_7.lo_discount >= '1'::numeric) AND (lineorder_7.lo_discount <= '3'::numeric) AND (lineorder_7.lo_quantity < '25'::numeric)) [rows: 53370560 -> 6898138] GPU Join Quals [1]: (lineorder_7.lo_orderdate = date1.d_datekey) ... [nrows: 6898138 -> 985063] GPU Outer Hash [1]: lineorder_7.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1999 lineorder_8 GPU Projection: pgstrom.psum(((lineorder_8.lo_extendedprice * lineorder_8.lo_discount))::double precision) GPU Scan Quals: ((lineorder_8.lo_discount >= '1'::numeric) AND (lineorder_8.lo_discount <= '3'::numeric) AND (lineorder_8.lo_quantity < '25'::numeric)) [rows: 150 -> 1] GPU Join Quals [1]: (lineorder_8.lo_orderdate = date1.d_datekey) ... [nrows: 1 -> 1] GPU Outer Hash [1]: lineorder_8.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) (82 rows) Build GPU code on startup Previous versions of PG-Strom was distributing pre-built binary modules for GPUs. Although this is simple, the GPU binary module often raised a runtime error depending on the combination of CUDA Toolkit and NVIDIA driver versions in the PG-Strom (PostgreSQL) execution environment. Typically, this is when the execution environment has an older version of the CUDA Toolkit or NVIDIA driver installed than the environment in which the RPM package was built. PG-Strom v5.1 has been changed to check the GPU source code and CUDA Toolkit version at startup, and build a GPU binary module if there are any difference. With this fix, PG-Strom can now utilize GPU devices and GPU binary modules for CUDA Toolkit in the execution environment. pg2arrow parallel execution pg2arrow now supports the new -n|--num-workers and -k|--parallel-keys options. -n N_WORKERS option launches the specified number of threads to connect to PostgreSQL for each, execute queries in parallel, and write the results to the Apache Arrow file. Queries can contain the special token $(N_WORKERS) and $(WORKER_ID) , which will be replaced by the number of workers and worker-specific ID values, respectively, when querying PostgreSQL. It is user's responsibility to ensure the tuples read by each worker thread do not overlap or are missing. Another -k|--parallel-key option starts a worker thread for each comma-separated key value given by the argument, and replaces $(PARALLEL_KEY) with the key in the query. The worker thread runs this query for each, then write the result as an Apache Arrow file. For example, if the lineorder table is partitioned and there are child tables lineorder__sun , lineorder__mon , ... lineorder__sat , each worker thread scans each child table of the partition according to the keys given by the -k sun,mon,tue,wed,thu,fri,sat option. This parallel key is replaced by the $(PARALLEL_KEY) in the query given by -c 'SELECT * FROM lineorder__$(PARALLEL_KEY)' option. It launches 7 worker threads which shall scan the partitioned child table for each. $ pg2arrow -d ssbm -c 'SELECT * FROM lineorder__$(PARALLEL_KEY)' -o /path/to/f_lineorder.arrow -k=sun,mon,tue,wed,thu,fri,sat --progress worker:1 SQL=[SELECT * FROM lineorder__sun] worker:3 SQL=[SELECT * FROM lineorder__tue] worker:2 SQL=[SELECT * FROM lineorder__mon] worker:4 SQL=[SELECT * FROM lineorder__wed] worker:5 SQL=[SELECT * FROM lineorder__thu] : :","title":"PG-Strom v5.1"},{"location":"release_v5.1/#pg-strom-v51-release","text":"PG-Strom Development Team (17-Apr-2024)","title":"PG-Strom v5.1 Release"},{"location":"release_v5.1/#overview","text":"Major changes in PG-Strom v5.1 are as follows: Added support for partition-wise GPU-Join/PreAgg. GPU code is now built in the execution environment at startup. pg2arrow now support parallel execution. CUDA Stack size is now set adaptically. Cumulative bug fixes","title":"Overview"},{"location":"release_v5.1/#prerequisites","text":"PostgreSQL v15.x, v16.x CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Volta or newer is recommended)","title":"Prerequisites"},{"location":"release_v5.1/#partition-wise-gpujoingpupreagg","text":"Support for PostgreSQL partitions itself was also included in PG-Strom v3.0, but execution plans often could not be created properly, therefore it could not be moved out of its experimental status. Then, in PG-Strom v5.1, we fundamentally revised the internal design, re-implemented it, and incorporated it as an official feature again. If the lineorder table below is partitioned and the date1 table is a non-partitioned table, previously all the data read from the partitioned tables under lineorder must be joined with date1 table after the consolidation of all the partition leafs by the Append node. Usually, PG-Strom bypasses the CPU and loads data from the NVME-SSD to the GPU to perform various SQL processing (GPU-Direct SQL), so the data must be returned to the CPU before JOIN. It has been a big penalty. ssbm=# explain (costs off) select sum(lo_extendedprice*lo_discount) as revenue from lineorder,date1 where lo_orderdate = d_datekey and d_year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- Aggregate -> Hash Join Hash Cond: (lineorder.lo_orderdate = date1.d_datekey) -> Append -> Custom Scan (GpuScan) on lineorder__p1992 lineorder_1 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91250920 -> 11911380] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1993 lineorder_2 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91008500 -> 11980460] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1994 lineorder_3 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91044060 -> 12150700] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1995 lineorder_4 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91011720 -> 11779920] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1996 lineorder_5 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91305650 -> 11942810] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1997 lineorder_6 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 91049100 -> 12069740] GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on lineorder__p1998 lineorder_7 GPU Projection: lo_extendedprice, lo_discount, lo_orderdate GPU Scan Quals: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) [rows: 53370560 -> 6898138] GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on lineorder__p1999 lineorder_8 Filter: ((lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric)) -> Hash -> Seq Scan on date1 Filter: (d_year = 1993) (37 rows) In PG-Strom v5.1, it is now possible to push-down JOINs with non-partitioned tables to partitioned child tables. In some cases, it is also possible to complete the GROUP-BY processing and then return much smaller results to CPU. For example, in the example below, 70 million rows extracted from a total of 600 million rows in the partitioned child tables. By performing a JOIN with the non-partitioned table date1 and then aggregation function SUM() pushed-down to the partitioned child tables, the CPU only needs to process 8 rows. Although there is a disadvantage that reading on the INNER side occurs multiple times (* This will be fixed in a future version), this type of rewriting will significantly reduce the amount of data that must be processed by the CPU, contributing to improved processing speed. To do. ssbm=# explain (costs off) select sum(lo_extendedprice*lo_discount) as revenue from lineorder,date1 where lo_orderdate = d_datekey and d_year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ---------------------------------------------------------------------------------------------------- Aggregate -> Append -> Custom Scan (GpuPreAgg) on lineorder__p1992 lineorder_1 GPU Projection: pgstrom.psum(((lineorder_1.lo_extendedprice * lineorder_1.lo_discount))::double precision) GPU Scan Quals: ((lineorder_1.lo_discount >= '1'::numeric) AND (lineorder_1.lo_discount <= '3'::numeric) AND (lineorder_1.lo_quantity < '25'::numeric)) [rows: 91250920 -> 11911380] GPU Join Quals [1]: (lineorder_1.lo_orderdate = date1.d_datekey) ... [nrows: 11911380 -> 1700960] GPU Outer Hash [1]: lineorder_1.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1993 lineorder_2 GPU Projection: pgstrom.psum(((lineorder_2.lo_extendedprice * lineorder_2.lo_discount))::double precision) GPU Scan Quals: ((lineorder_2.lo_discount >= '1'::numeric) AND (lineorder_2.lo_discount <= '3'::numeric) AND (lineorder_2.lo_quantity < '25'::numeric)) [rows: 91008500 -> 11980460] GPU Join Quals [1]: (lineorder_2.lo_orderdate = date1.d_datekey) ... [nrows: 11980460 -> 1710824] GPU Outer Hash [1]: lineorder_2.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1994 lineorder_3 GPU Projection: pgstrom.psum(((lineorder_3.lo_extendedprice * lineorder_3.lo_discount))::double precision) GPU Scan Quals: ((lineorder_3.lo_discount >= '1'::numeric) AND (lineorder_3.lo_discount <= '3'::numeric) AND (lineorder_3.lo_quantity < '25'::numeric)) [rows: 91044060 -> 12150700] GPU Join Quals [1]: (lineorder_3.lo_orderdate = date1.d_datekey) ... [nrows: 12150700 -> 1735135] GPU Outer Hash [1]: lineorder_3.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1995 lineorder_4 GPU Projection: pgstrom.psum(((lineorder_4.lo_extendedprice * lineorder_4.lo_discount))::double precision) GPU Scan Quals: ((lineorder_4.lo_discount >= '1'::numeric) AND (lineorder_4.lo_discount <= '3'::numeric) AND (lineorder_4.lo_quantity < '25'::numeric)) [rows: 91011720 -> 11779920] GPU Join Quals [1]: (lineorder_4.lo_orderdate = date1.d_datekey) ... [nrows: 11779920 -> 1682188] GPU Outer Hash [1]: lineorder_4.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1996 lineorder_5 GPU Projection: pgstrom.psum(((lineorder_5.lo_extendedprice * lineorder_5.lo_discount))::double precision) GPU Scan Quals: ((lineorder_5.lo_discount >= '1'::numeric) AND (lineorder_5.lo_discount <= '3'::numeric) AND (lineorder_5.lo_quantity < '25'::numeric)) [rows: 91305650 -> 11942810] GPU Join Quals [1]: (lineorder_5.lo_orderdate = date1.d_datekey) ... [nrows: 11942810 -> 1705448] GPU Outer Hash [1]: lineorder_5.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1997 lineorder_6 GPU Projection: pgstrom.psum(((lineorder_6.lo_extendedprice * lineorder_6.lo_discount))::double precision) GPU Scan Quals: ((lineorder_6.lo_discount >= '1'::numeric) AND (lineorder_6.lo_discount <= '3'::numeric) AND (lineorder_6.lo_quantity < '25'::numeric)) [rows: 91049100 -> 12069740] GPU Join Quals [1]: (lineorder_6.lo_orderdate = date1.d_datekey) ... [nrows: 12069740 -> 1723574] GPU Outer Hash [1]: lineorder_6.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1998 lineorder_7 GPU Projection: pgstrom.psum(((lineorder_7.lo_extendedprice * lineorder_7.lo_discount))::double precision) GPU Scan Quals: ((lineorder_7.lo_discount >= '1'::numeric) AND (lineorder_7.lo_discount <= '3'::numeric) AND (lineorder_7.lo_quantity < '25'::numeric)) [rows: 53370560 -> 6898138] GPU Join Quals [1]: (lineorder_7.lo_orderdate = date1.d_datekey) ... [nrows: 6898138 -> 985063] GPU Outer Hash [1]: lineorder_7.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) -> Custom Scan (GpuPreAgg) on lineorder__p1999 lineorder_8 GPU Projection: pgstrom.psum(((lineorder_8.lo_extendedprice * lineorder_8.lo_discount))::double precision) GPU Scan Quals: ((lineorder_8.lo_discount >= '1'::numeric) AND (lineorder_8.lo_discount <= '3'::numeric) AND (lineorder_8.lo_quantity < '25'::numeric)) [rows: 150 -> 1] GPU Join Quals [1]: (lineorder_8.lo_orderdate = date1.d_datekey) ... [nrows: 1 -> 1] GPU Outer Hash [1]: lineorder_8.lo_orderdate GPU Inner Hash [1]: date1.d_datekey Inner Siblings-Id: 2 GPU-Direct SQL: enabled (GPU-0) -> Seq Scan on date1 Filter: (d_year = 1993) (82 rows)","title":"Partition-wise GpuJoin/GpuPreAgg"},{"location":"release_v5.1/#build-gpu-code-on-startup","text":"Previous versions of PG-Strom was distributing pre-built binary modules for GPUs. Although this is simple, the GPU binary module often raised a runtime error depending on the combination of CUDA Toolkit and NVIDIA driver versions in the PG-Strom (PostgreSQL) execution environment. Typically, this is when the execution environment has an older version of the CUDA Toolkit or NVIDIA driver installed than the environment in which the RPM package was built. PG-Strom v5.1 has been changed to check the GPU source code and CUDA Toolkit version at startup, and build a GPU binary module if there are any difference. With this fix, PG-Strom can now utilize GPU devices and GPU binary modules for CUDA Toolkit in the execution environment.","title":"Build GPU code on startup"},{"location":"release_v5.1/#pg2arrow-parallel-execution","text":"pg2arrow now supports the new -n|--num-workers and -k|--parallel-keys options. -n N_WORKERS option launches the specified number of threads to connect to PostgreSQL for each, execute queries in parallel, and write the results to the Apache Arrow file. Queries can contain the special token $(N_WORKERS) and $(WORKER_ID) , which will be replaced by the number of workers and worker-specific ID values, respectively, when querying PostgreSQL. It is user's responsibility to ensure the tuples read by each worker thread do not overlap or are missing. Another -k|--parallel-key option starts a worker thread for each comma-separated key value given by the argument, and replaces $(PARALLEL_KEY) with the key in the query. The worker thread runs this query for each, then write the result as an Apache Arrow file. For example, if the lineorder table is partitioned and there are child tables lineorder__sun , lineorder__mon , ... lineorder__sat , each worker thread scans each child table of the partition according to the keys given by the -k sun,mon,tue,wed,thu,fri,sat option. This parallel key is replaced by the $(PARALLEL_KEY) in the query given by -c 'SELECT * FROM lineorder__$(PARALLEL_KEY)' option. It launches 7 worker threads which shall scan the partitioned child table for each. $ pg2arrow -d ssbm -c 'SELECT * FROM lineorder__$(PARALLEL_KEY)' -o /path/to/f_lineorder.arrow -k=sun,mon,tue,wed,thu,fri,sat --progress worker:1 SQL=[SELECT * FROM lineorder__sun] worker:3 SQL=[SELECT * FROM lineorder__tue] worker:2 SQL=[SELECT * FROM lineorder__mon] worker:4 SQL=[SELECT * FROM lineorder__wed] worker:5 SQL=[SELECT * FROM lineorder__thu] : :","title":"pg2arrow parallel execution"},{"location":"release_v5.2/","text":"PG-Strom v5.2 Release PG-Strom Development Team (14-Jul-2024) Overview Major changes in PG-Strom v5.2 are as follows: GpuJoin Pinned Inner Buffer Improved GPU-Direct SQL performance 64bit GPU Buffer representation Per-tuple CPU-Fallback SELECT DISTINCT support Improced parallel pg2arrow Cumulative bug fixes Prerequisites PostgreSQL v15.x, v16.x CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Turing or newer is recommended) GpuJoin Pinned Inner Buffer PG-Strom's GpuJoin is designed to perform tables JOIN based on the Hash-Join algorithm using GPU in parallel. Due to the nature of the Hash-Join algorithm, the table on the INNER side must be read onto the buffer before the JOIN processing. In the older version, the contents of the INNER tables were read line by line using PostgreSQL's internal APIs. This design was wasteful when the INNER side was a GpuScan that reads a huge table. For example, consider the following query. This query includes a JOIN of lineitem (882GB) and about 1/4 of orders (205GB). There is a GpuScan in the lower node of GpuPreAgg that includes JOIN, which reads the orders table into the INNER buffer, but this requires about 360 million GpuScan calls and uses 30GB of INNER buffer which should be moved to the GPU. =# explain select l_shipmode, o_shippriority, sum(l_extendedprice) from lineitem, orders where l_orderkey = o_orderkey and o_orderdate >= '1997-01-01' group by l_shipmode, o_shippriority; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- HashAggregate (cost=38665701.61..38665701.69 rows=7 width=47) Group Key: lineitem.l_shipmode, orders.o_shippriority -> Custom Scan (GpuPreAgg) on lineitem (cost=38665701.48..38665701.55 rows=7 width=47) GPU Projection: pgstrom.psum((lineitem.l_extendedprice)::double precision), lineitem.l_shipmode, orders.o_shippriority GPU Join Quals [1]: (lineitem.l_orderkey = orders.o_orderkey) ... [nrows: 6000244000 -> 1454290000] GPU Outer Hash [1]: lineitem.l_orderkey GPU Inner Hash [1]: orders.o_orderkey GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on orders (cost=100.00..10580836.56 rows=363551222 width=12) GPU Projection: o_shippriority, o_orderkey GPU Pinned Buffer: enabled GPU Scan Quals: (o_orderdate >= '1997-01-01'::date) [rows: 1499973000 -> 363551200] GPU-Direct SQL: enabled (GPU-0) (13 rows) In the v5.2, GpuJoin's INNER buffer initial setup process has been redesigned to be more effectively. GpuScan checks the WHERE clause on the rows read from the table and writes back to the host side (PostgreSQL backend process). GpuJoin reads the results to setup the INNER buffer, and copies this to the GPU side, but originally all the necessary data was on the GPU when GpuScan processed the INNER side table, and it is not very reasonable to get GpuScan's results back to the host side then copy to the GPU side again. GpuJoin Pinned Inner Buffer allows GpuScan to keep the processing results, when child node of GpuJoin is GpuScan, to use a part of INNER buffer on the next GpuJoin, instead of returning the GpuScan's results to the host side once. Thie mechanism allows to save a lot of initial setup time of GpuJoin when size of the INNER tables are very large. On the other hand, setting up of the INNER buffer on GPU side means that the GpuJoin INNER buffer does not physically exist on the CPU memory, therefore, SQL must be aborted by error if CPU fallback processing is required. Due to the potential side effects, the GpuJoin Pinned Inner Buffer function is disabled by default. You must explicitly enable the feature using command below; that means GpuJoin uses Pinned Inner Buffer if the estimated INNER buffer size exceeds 100MB. =# set pg_strom.pinned_inner_buffer_threshold = '100MB'; SET Per-tuple CPU-Fallback When processing SQL workloads on GPUs, input data may have patterns that cannot be executed on GPUs in principle. For example, if long variable-length data does not fit into the PostgreSQL block size and is fragmented into an external table (this is called the TOAST mechanism), GPUs that do not have data in the external table will not continue processing. PG-Strom has a mechanism called CPU-Fallback, which allows processing of such data on the CPU side. Typically, processing logic such as GpuJoin and GpuPreAgg reads 64MB of data (called chunks) from a table and launches a GPU kernel to process the SQL workload. In the older version, if a CPU-Fallback error occurred while processing a SQL workload, the GPU kernel performing on the entire chunk was canceled and written back to the CPU side. However, this strategy was problematic in two points. First, if there is just one piece of bad data in a chunk containing hundreds of thousands of rows of data, the GPU processing of the entire chunk will be canceled. Another problem is that GpuPreAgg that keep updating the aggregation buffer, a situation may occur where it is difficult to know how much has been reflected in the aggregation table. (Therefore, before v5.1, GpuPreAgg's CPU-Fallback was treated as an error) PG-Strom v5.2 improves the CPU-Fallback implementation to resolve these issues. When a CPU-Fallback error occurs, instead of canceling the processing of the entire chunk as in the past, we prepare a \"Fallback Buffer\" in addition to the \"Destination Buffer\" that writes the normal processing result to handle the CPU-Fallback error. Only the generated tuples are written. The contents of \"Fallback Buffer\" are later written back to the CPU side and evaluated again by the CPU. Therefore, not only do you need to perform CPU-Fallback processing on only the minimum number of tuples required, but you also do not have to worry about the GpuPreAgg aggregation buffer being updated redundantly. 64bit GPU Buffer representation GPUs with memory such as 48GB or 80GB are now on sale, but the data format of the GPU buffer used internally by PG-Strom was designed around 2017 during the development of v2.0. In other words, even high-end GPUs were typically 16GB or 24GB, and others were generally 8GB or less. Under such a premise, a data format that can reduce memory usage was prioritized rather than being able to express physically impossible large amounts of data without too much or too little. Tuples loaded into PG-Strom's GPU buffer are always guaranteed to be 8-byte aligned. Therefore, by shifting the 32-bit offset value by 3 bits, it was actually possible to express an address width of 35 bits (= 32 GB). In 2020, the NVIDIA A100 with 40GB of memory was announced, but the 32GB buffer length limit was essentially a meaningless restriction. This is because PG-Strom reads data from storage in 64MB chunks, so it was almost impossible for the GPU buffer size to exceed 32GB. However, in the following situations it becomes necessary to assume a very large result buffer. When the GPU cache holds a very large amount of data. A case where the GpuJoin INNER buffer swelled to a huge size. When the number of rows resulting from GROUP BY or SELECT DISTINCT increases significantly. In PG-Strom v5.2, all offset values \u200b\u200bon GPU buffers have been replaced with 64-bit representation. As a result, it is now possible to handle GPU buffers larger than 32GB, and even with workloads such as those mentioned above, it is now possible to handle up to the physical RAM size range. Other new features Improved GPU-Direct SQL performance NVIDIA's cuFile library assumed Devive Primary Context internally. As a result, there was a cost to switch the CUDA Context for API calls from the PG-Strom GPU Service, which was using a uniquely generated CUDA Context. In PG-Strom v5.2, GPU Service has also been redesigned to use Device Primary Context, which has improved performance by approximately 10% by eliminating switching costs in the cuFile library and the associated CPU Busy Loop. is being carried out. SELECT DISTINCT support PG-Strom v5.2 supports the SELECT DISTINCT... clause. In previous versions, this had to be rewritten as a GROUP BY clause. Improced parallel pg2arrow If you use the -t option and -n option together with the pg2arrow command, check the table size to be read, adjust the scan range so that each worker thread does not read the table redundantly, and execute the query. Issue. Aliases for duplicated names on IMPORT FOREIGN SCHEMA When importing an Arrow file using the IMPORT FOREIGN SCHEMA or pgstrom.arrow_fdw_import_file() function, if a Field has a duplicate name, give an alias to the second and subsequent columns with duplicate names. Now it looks like this. Partial concurrent results responding When the Destination Buffer is used up in various processes such as GpuJoin or GpuPreAgg, execution of the GPU kernel will be paused, partial processing results will be returned to the backend process, and then execution of the GPU kernel will be resumed again. became. Until now, the structure was such that the Destination Buffer was expanded to hold all the processing results of the chunk, so if the result set was huge for the input, it would consume excessive CPU and GPU memory. There was a problem with the system becoming unstable. Cumulative bug fixes [#664] Too much CPU consumption ratio with cuFile/GDS on many threads [#757] wrong varnullingrels at setrefs.c [#762] arrow_fdw: scan-hint needs to resolve expressions using INDEV_VAR [#763] Var-nodes on kvdef->expr was not compared correctly [#764][#757] Var::varnullingrels come from the prior level join are not consistent [#673] lookup_input_varnode_defitem() should not use equal() to compare Var-nodes [#729] update RPM build chain [#765] Add regression test for PostgreSQL 16 [#771] Update regression test for PostgreSQL 15 [#768] fix dead loop in `gpuCacheAutoPreloadConnectDatabase [#xxx] Wrong GiST-Index based JOIN results [#774] add support of SELECT DISTINCT [#778] Disable overuse of kds_dst buffer in projection/gpupreagg [#752] add KDS_FORMAT_HASH support in execGpuJoinProjection() [#784] CPU-Fallback JOIN didn't handle LEFT/FULL OUTER case if tuple has no matched inner row [#777] Fix the bug of dynamically allocating fallback buffer size and nrooms [#776] Fix the out of range bug in pgfn_interval_um() [#706] gpucache: drop active cache at DROP DATABASE from shmem / gpumem [#791] gpujoin: wrong logic to detect unmatched inner tuple [#794] assertion failure at cost_memoize_rescan() [#xxx] pg2arrow: outer/inner-join subcommand initialization [#xxx] IMPORT FOREIGN SCHEMA renames duplicated field name [#xxx] arrow_fdw: correct pg_type hint parsing [#748] Add support CPU-fallback on GpuPreAgg, and revise fallback implementation [#778] Add XpuCommandTag__SuccessHalfWay response tag","title":"PG-Strom v5.2"},{"location":"release_v5.2/#pg-strom-v52-release","text":"PG-Strom Development Team (14-Jul-2024)","title":"PG-Strom v5.2 Release"},{"location":"release_v5.2/#overview","text":"Major changes in PG-Strom v5.2 are as follows: GpuJoin Pinned Inner Buffer Improved GPU-Direct SQL performance 64bit GPU Buffer representation Per-tuple CPU-Fallback SELECT DISTINCT support Improced parallel pg2arrow Cumulative bug fixes","title":"Overview"},{"location":"release_v5.2/#prerequisites","text":"PostgreSQL v15.x, v16.x CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Turing or newer is recommended)","title":"Prerequisites"},{"location":"release_v5.2/#gpujoin-pinned-inner-buffer","text":"PG-Strom's GpuJoin is designed to perform tables JOIN based on the Hash-Join algorithm using GPU in parallel. Due to the nature of the Hash-Join algorithm, the table on the INNER side must be read onto the buffer before the JOIN processing. In the older version, the contents of the INNER tables were read line by line using PostgreSQL's internal APIs. This design was wasteful when the INNER side was a GpuScan that reads a huge table. For example, consider the following query. This query includes a JOIN of lineitem (882GB) and about 1/4 of orders (205GB). There is a GpuScan in the lower node of GpuPreAgg that includes JOIN, which reads the orders table into the INNER buffer, but this requires about 360 million GpuScan calls and uses 30GB of INNER buffer which should be moved to the GPU. =# explain select l_shipmode, o_shippriority, sum(l_extendedprice) from lineitem, orders where l_orderkey = o_orderkey and o_orderdate >= '1997-01-01' group by l_shipmode, o_shippriority; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- HashAggregate (cost=38665701.61..38665701.69 rows=7 width=47) Group Key: lineitem.l_shipmode, orders.o_shippriority -> Custom Scan (GpuPreAgg) on lineitem (cost=38665701.48..38665701.55 rows=7 width=47) GPU Projection: pgstrom.psum((lineitem.l_extendedprice)::double precision), lineitem.l_shipmode, orders.o_shippriority GPU Join Quals [1]: (lineitem.l_orderkey = orders.o_orderkey) ... [nrows: 6000244000 -> 1454290000] GPU Outer Hash [1]: lineitem.l_orderkey GPU Inner Hash [1]: orders.o_orderkey GPU-Direct SQL: enabled (GPU-0) -> Custom Scan (GpuScan) on orders (cost=100.00..10580836.56 rows=363551222 width=12) GPU Projection: o_shippriority, o_orderkey GPU Pinned Buffer: enabled GPU Scan Quals: (o_orderdate >= '1997-01-01'::date) [rows: 1499973000 -> 363551200] GPU-Direct SQL: enabled (GPU-0) (13 rows) In the v5.2, GpuJoin's INNER buffer initial setup process has been redesigned to be more effectively. GpuScan checks the WHERE clause on the rows read from the table and writes back to the host side (PostgreSQL backend process). GpuJoin reads the results to setup the INNER buffer, and copies this to the GPU side, but originally all the necessary data was on the GPU when GpuScan processed the INNER side table, and it is not very reasonable to get GpuScan's results back to the host side then copy to the GPU side again. GpuJoin Pinned Inner Buffer allows GpuScan to keep the processing results, when child node of GpuJoin is GpuScan, to use a part of INNER buffer on the next GpuJoin, instead of returning the GpuScan's results to the host side once. Thie mechanism allows to save a lot of initial setup time of GpuJoin when size of the INNER tables are very large. On the other hand, setting up of the INNER buffer on GPU side means that the GpuJoin INNER buffer does not physically exist on the CPU memory, therefore, SQL must be aborted by error if CPU fallback processing is required. Due to the potential side effects, the GpuJoin Pinned Inner Buffer function is disabled by default. You must explicitly enable the feature using command below; that means GpuJoin uses Pinned Inner Buffer if the estimated INNER buffer size exceeds 100MB. =# set pg_strom.pinned_inner_buffer_threshold = '100MB'; SET","title":"GpuJoin Pinned Inner Buffer"},{"location":"release_v5.2/#per-tuple-cpu-fallback","text":"When processing SQL workloads on GPUs, input data may have patterns that cannot be executed on GPUs in principle. For example, if long variable-length data does not fit into the PostgreSQL block size and is fragmented into an external table (this is called the TOAST mechanism), GPUs that do not have data in the external table will not continue processing. PG-Strom has a mechanism called CPU-Fallback, which allows processing of such data on the CPU side. Typically, processing logic such as GpuJoin and GpuPreAgg reads 64MB of data (called chunks) from a table and launches a GPU kernel to process the SQL workload. In the older version, if a CPU-Fallback error occurred while processing a SQL workload, the GPU kernel performing on the entire chunk was canceled and written back to the CPU side. However, this strategy was problematic in two points. First, if there is just one piece of bad data in a chunk containing hundreds of thousands of rows of data, the GPU processing of the entire chunk will be canceled. Another problem is that GpuPreAgg that keep updating the aggregation buffer, a situation may occur where it is difficult to know how much has been reflected in the aggregation table. (Therefore, before v5.1, GpuPreAgg's CPU-Fallback was treated as an error) PG-Strom v5.2 improves the CPU-Fallback implementation to resolve these issues. When a CPU-Fallback error occurs, instead of canceling the processing of the entire chunk as in the past, we prepare a \"Fallback Buffer\" in addition to the \"Destination Buffer\" that writes the normal processing result to handle the CPU-Fallback error. Only the generated tuples are written. The contents of \"Fallback Buffer\" are later written back to the CPU side and evaluated again by the CPU. Therefore, not only do you need to perform CPU-Fallback processing on only the minimum number of tuples required, but you also do not have to worry about the GpuPreAgg aggregation buffer being updated redundantly.","title":"Per-tuple CPU-Fallback"},{"location":"release_v5.2/#64bit-gpu-buffer-representation","text":"GPUs with memory such as 48GB or 80GB are now on sale, but the data format of the GPU buffer used internally by PG-Strom was designed around 2017 during the development of v2.0. In other words, even high-end GPUs were typically 16GB or 24GB, and others were generally 8GB or less. Under such a premise, a data format that can reduce memory usage was prioritized rather than being able to express physically impossible large amounts of data without too much or too little. Tuples loaded into PG-Strom's GPU buffer are always guaranteed to be 8-byte aligned. Therefore, by shifting the 32-bit offset value by 3 bits, it was actually possible to express an address width of 35 bits (= 32 GB). In 2020, the NVIDIA A100 with 40GB of memory was announced, but the 32GB buffer length limit was essentially a meaningless restriction. This is because PG-Strom reads data from storage in 64MB chunks, so it was almost impossible for the GPU buffer size to exceed 32GB. However, in the following situations it becomes necessary to assume a very large result buffer. When the GPU cache holds a very large amount of data. A case where the GpuJoin INNER buffer swelled to a huge size. When the number of rows resulting from GROUP BY or SELECT DISTINCT increases significantly. In PG-Strom v5.2, all offset values \u200b\u200bon GPU buffers have been replaced with 64-bit representation. As a result, it is now possible to handle GPU buffers larger than 32GB, and even with workloads such as those mentioned above, it is now possible to handle up to the physical RAM size range.","title":"64bit GPU Buffer representation"},{"location":"release_v5.2/#other-new-features","text":"","title":"Other new features"},{"location":"release_v5.2/#improved-gpu-direct-sql-performance","text":"NVIDIA's cuFile library assumed Devive Primary Context internally. As a result, there was a cost to switch the CUDA Context for API calls from the PG-Strom GPU Service, which was using a uniquely generated CUDA Context. In PG-Strom v5.2, GPU Service has also been redesigned to use Device Primary Context, which has improved performance by approximately 10% by eliminating switching costs in the cuFile library and the associated CPU Busy Loop. is being carried out.","title":"Improved GPU-Direct SQL performance"},{"location":"release_v5.2/#select-distinct-support","text":"PG-Strom v5.2 supports the SELECT DISTINCT... clause. In previous versions, this had to be rewritten as a GROUP BY clause.","title":"SELECT DISTINCT support"},{"location":"release_v5.2/#improced-parallel-pg2arrow","text":"If you use the -t option and -n option together with the pg2arrow command, check the table size to be read, adjust the scan range so that each worker thread does not read the table redundantly, and execute the query. Issue.","title":"Improced parallel pg2arrow"},{"location":"release_v5.2/#aliases-for-duplicated-names-on-import-foreign-schema","text":"When importing an Arrow file using the IMPORT FOREIGN SCHEMA or pgstrom.arrow_fdw_import_file() function, if a Field has a duplicate name, give an alias to the second and subsequent columns with duplicate names. Now it looks like this.","title":"Aliases for duplicated names on IMPORT FOREIGN SCHEMA"},{"location":"release_v5.2/#partial-concurrent-results-responding","text":"When the Destination Buffer is used up in various processes such as GpuJoin or GpuPreAgg, execution of the GPU kernel will be paused, partial processing results will be returned to the backend process, and then execution of the GPU kernel will be resumed again. became. Until now, the structure was such that the Destination Buffer was expanded to hold all the processing results of the chunk, so if the result set was huge for the input, it would consume excessive CPU and GPU memory. There was a problem with the system becoming unstable.","title":"Partial concurrent results responding"},{"location":"release_v5.2/#cumulative-bug-fixes","text":"[#664] Too much CPU consumption ratio with cuFile/GDS on many threads [#757] wrong varnullingrels at setrefs.c [#762] arrow_fdw: scan-hint needs to resolve expressions using INDEV_VAR [#763] Var-nodes on kvdef->expr was not compared correctly [#764][#757] Var::varnullingrels come from the prior level join are not consistent [#673] lookup_input_varnode_defitem() should not use equal() to compare Var-nodes [#729] update RPM build chain [#765] Add regression test for PostgreSQL 16 [#771] Update regression test for PostgreSQL 15 [#768] fix dead loop in `gpuCacheAutoPreloadConnectDatabase [#xxx] Wrong GiST-Index based JOIN results [#774] add support of SELECT DISTINCT [#778] Disable overuse of kds_dst buffer in projection/gpupreagg [#752] add KDS_FORMAT_HASH support in execGpuJoinProjection() [#784] CPU-Fallback JOIN didn't handle LEFT/FULL OUTER case if tuple has no matched inner row [#777] Fix the bug of dynamically allocating fallback buffer size and nrooms [#776] Fix the out of range bug in pgfn_interval_um() [#706] gpucache: drop active cache at DROP DATABASE from shmem / gpumem [#791] gpujoin: wrong logic to detect unmatched inner tuple [#794] assertion failure at cost_memoize_rescan() [#xxx] pg2arrow: outer/inner-join subcommand initialization [#xxx] IMPORT FOREIGN SCHEMA renames duplicated field name [#xxx] arrow_fdw: correct pg_type hint parsing [#748] Add support CPU-fallback on GpuPreAgg, and revise fallback implementation [#778] Add XpuCommandTag__SuccessHalfWay response tag","title":"Cumulative bug fixes"},{"location":"release_v6.0/","text":"PG-Strom v6.0 Release PG-Strom Development Team (1st-Apr-2025) Overview Major changes in PG-Strom v6.0 are as follows: GPU-Sort and a few Window-functions Partitioned Pinned Inner Buffer Arrow_Fdw Virtual Columns AVG(numeric), SUM(numeric) accuracy improvement GpuPreAgg final merge on GPU device Improved GPU-tasks scheduling Cumulative bug fixes Prerequisites PostgreSQL v15 or later CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Turing or newer is recommended) GPU-Sort and a few Window-functions PG-Strom v6.0 now supports GPU-Sort. Sorting is a typical workload suited to GPUs, and we had tried to implement it in the early version of PG-Strom. However, the RAM of early stage GPUs was small at that time, so sorting data that fits in the GPU's RAM did not improve the speed significantly. In addition, the sorting workload does not reduce the amount of data, so CPU processing tends to get stucked after receiving the data from the GPU. Therefore, we decided not to implement it at that time. However, as of 2025, the memory of high-end GPUs has reached tens of GB, and it has become clear that combining sorting with LIMIT clauses and window functions is effective for many workloads. Due to the nature of sorting, all target data must be stored in GPU memory. On the other hand, various GPU-operators in PG-Strom have a mechanism called CPU-Fallback, which re-executes the processing of operators or SQL functions on the CPU when they cannot be completed on the GPU. A typical example is when variable-length data does not fit into the PostgreSQL block size (8kB) and is stored in an external TOAST table. CPU-Fallback is a function to ensure continuity of processing even for extremely exceptional data, but rows that are subject to CPU-Fallback are stored in the CPU and do not exist in GPU memory, which causes an obstacle when sorting. Therefore, the GPU-Sort function works only when the CPU-Fallback function is disabled, i.e., when pg_strom.cpu_fallback=off is set. When CPU-Fallback is disabled, the complete result-set of GPU-Scan/Join/PreAgg should be in GPU memory, so PG-Strom can perform parallel sorting based on the Bitonic-Sorting algorithm and return the sorted results to the CPU. When used with a LIMIT clause or a window function that limits the number of rows, such as rank() < 4 , it will reduce the number of data returned to the CPU based on these optimization hints. This should contribute to speedup by reducing the number of data to be processed by the CPU. Please see the execution plan below. This is the result of narrowing down the number of rows using a window function ( rank() < 4 ) without enabling GPU-Sort. =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=32013352.01..33893039.51 rows=75187500 width=84) (actual time=13158.987..13335.106 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Sort (cost=32013352.01..32201320.76 rows=75187500 width=76) (actual time=13158.976..13238.136 rows=601500 loops=1) Sort Key: customer.c_region, customer.c_nation, customer.c_city, (pgstrom.sum_numeric((pgstrom.psum(lineorder.lo_revenue)))) Sort Method: quicksort Memory: 76268kB -> HashAggregate (cost=15987574.35..18836475.71 rows=75187500 width=76) (actual time=9990.801..10271.543 rows=601500 loops=1) Group Key: customer.c_region, customer.c_nation, customer.c_city, lineorder.lo_orderdate Planned Partitions: 8 Batches: 1 Memory Usage: 516113kB -> Custom Scan (GpuPreAgg) on lineorder (cost=4967906.38..5907750.13 rows=75187500 width=76) (actual time=9175.476..9352.529 rows=1203000 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.008..519.064 rows=3000000 loops=1) Planning Time: 1.395 ms Execution Time: 13494.808 ms (19 rows) After GPU-PreAgg, HashAggregate is run to aggregate the partial aggregation results, and Sort is run to sort the aggregated values. Finally, WindowAgg is run to narrow down the results to the top three sum(lo_revenue) for each c_region , c_nation , and c_city . The processing time for GPU-PreAgg is 9.352 seconds, so we can see that roughly 4 seconds of the latter half of the process was spent on the CPU. On the other hand, the following execution plan disables CPU-Fallback by setting pg_strom.cpu_fallback=off (i.e. enables GPU-Sort). =# set pg_strom.cpu_fallback = off; SET =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=5595978.47..5602228.47 rows=125000 width=84) (actual time=9596.930..9598.194 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Result (cost=5595978.47..5599415.97 rows=125000 width=76) (actual time=9596.918..9597.292 rows=750 loops=1) -> Custom Scan (GpuPreAgg) on lineorder (cost=5595978.47..5597540.97 rows=125000 width=76) (actual time=9596.912..9597.061 rows=750 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 GPU-Sort keys: c_region, c_nation, c_city, pgstrom.fsum_numeric((pgstrom.psum(lo_revenue))) Window-Rank Filter: rank() over(PARTITION BY c_region, c_nation, c_city ORDER BY pgstrom.fsum_numeric((pgstrom.psum(lo_revenue)))) < 4 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.006..475.006 rows=3000000 loops=1) Planning Time: 0.381 ms Execution Time: 9710.616 ms (16 rows) The HashAggregate and Sort that were in the original query plan have gone, and instead, the lines GPU-Sort keys and Window-Rank Filter have appeared as options for GpuPreAgg . This indicates that GpuPreAgg creates the complete aggregation on the GPU, then sorts and outputs it. In addition, in this query, filtering is performed using the window function rank() . By pushing down this condition to the lower node GpuPreAgg , rows that are known to be filtered out in advance are removed from the result set, reducing the amount of data transferred from the GPU to the CPU and the number of rows that the CPU needs to copy. These processes are processed in parallel on the GPU memory, so they are generally faster than CPU processing. Multi-GPUs Pinned Inner Buffer PG-Strom v5.2 supported GPU-Join's Pinned Inner Buffer . This feature speeds up the construction of a hash table when GPU-Scan or GPU-Join is connected as an INNER lower plan of GPU-Join, and the processing results can be used as a hash table for GPU-Join, which is one of the parallelized HashJoins. Previously, the processing results of GPU-Scan or GPU-Join were sent back to the CPU to construct a hash table, but now the results are stored in GPU memory and used in the subsequent GPU-Join. This prevents data from moving from GPU to CPU and back again, which improves processing speed, especially when the size of the INNER side is large. However, in many systems, the capacity of the GPU's RAM is limited compared to the RAM of the server itself, and there are also restrictions on the size of the hash table. This restriction can be alleviated by dividing the hash table into multiple GPUs, but if an INNER row placed on another GPU is referenced while HashJoin is being executed on one GPU, a phenomenon called GPU memory thrashing occurs, resulting in a severe slowdown in speed. Therefore, a mechanism was needed to ensure the locality of memory access while HashJoin is being executed. In PG-Strom v6.0, the Pinned-Inner-Buffer of GPU-Join supports multiple GPUs. Please see the following diagram. When scanning the INNER table of GPU-Join is executed on multiple GPUs and the result is stored in the GPU memory to build a hash table, it is completely random which rows are stored on each GPU. In the next step, Hash-Join, if a row read from the OUTER side is first joined with an INNER row on GPU1, then with an INNER row on GPU2, and finally with an INNER row on GPU0, extreme thrashing will occur, causing a severe performance drop. Therefore, in PG-Strom v6.0's GPU-Join, when using Pinned-Inner-Buffer on multiple GPUs, a reconstruction process is inserted to reallocate the hash table on the appropriate GPU. For example, in a system equipped with three GPUs, if the size of the hash table fits roughly into the RAM of the three GPUs, after the GPU-Scan of the INNER table is completed, the hash value of the join key to be used in the next GPU-Join is calculated, and if the remainder when dividing this by 3 is 0, it is reallocated to GPU0, if it is 1 then it is reallocated to GPU1, and if it is 2 then it is reallocated to GPU2. By inserting this process, it is possible to create a state in which when GPU-Join is executed on GPU0, the hash table will only contain INNER rows whose remainder when the hash value is divided by 3 is 0, and similarly, GPU1 will only contain INNER rows whose remainder when the hash value is divided by 3 is 1. Next, when GPU-Join is executed using this divided hash table, when the GPU that first loaded data from the OUTER table (let's call it GPU2 here) references the hash table, if the remainder when dividing the hash value calculated from the OUTER row by 3 is other than 2, then there will obviously be no matching INNER row on that GPU. Therefore, GPU2 will generate a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 2. Next, this OUTER data is transferred to GPU1 by GPU-to-GPU Copy, which generates a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 1. By repeating this process, \"partial Hash-Join results\" are generated on each GPU, but the combination of these is equal to the complete Hash-Join result, and as a result, it is now possible to execute GPU-Join even if the INNER hash table is larger in size than the GPU's on-board RAM. In relation to this feature, the pg_strom.pinned_inner_buffer_partition_size parameter has been added. This specifies the threshold size for dividing the Pinned-Inner-Buffer among multiple GPUs. The initial value is set to about 80-90% of the GPU's installed memory, so administrators usually do not need to change this. Arrow_Fdw Virtual Columns When backing up data, it is common to set up a naming convention so that part of the file name indicates the attributes of the data. For example, a file name like my_data_2018_tokyo.arrow might convey that the data stored here is from the Tokyo area in 2018. In Arrow_Fdw foreign table, you can use dir option to access multiple Apache Arrow files with SQL. If the file name has some meaning, you can use it as a hint to speed up access to Arrow_Fdw foreign table. Here is an example. $ ls /opt/arrow/mydata f_lineorder_1993_AIR.arrow f_lineorder_1994_SHIP.arrow f_lineorder_1996_MAIL.arrow f_lineorder_1993_FOB.arrow f_lineorder_1994_TRUCK.arrow f_lineorder_1996_RAIL.arrow f_lineorder_1993_MAIL.arrow f_lineorder_1995_AIR.arrow f_lineorder_1996_SHIP.arrow f_lineorder_1993_RAIL.arrow f_lineorder_1995_FOB.arrow f_lineorder_1996_TRUCK.arrow f_lineorder_1993_SHIP.arrow f_lineorder_1995_MAIL.arrow f_lineorder_1997_AIR.arrow f_lineorder_1993_TRUCK.arrow f_lineorder_1995_RAIL.arrow f_lineorder_1997_FOB.arrow f_lineorder_1994_AIR.arrow f_lineorder_1995_SHIP.arrow f_lineorder_1997_MAIL.arrow f_lineorder_1994_FOB.arrow f_lineorder_1995_TRUCK.arrow f_lineorder_1997_RAIL.arrow f_lineorder_1994_MAIL.arrow f_lineorder_1996_AIR.arrow f_lineorder_1997_SHIP.arrow f_lineorder_1994_RAIL.arrow f_lineorder_1996_FOB.arrow f_lineorder_1997_TRUCK.arrow postgres=# IMPORT FOREIGN SCHEMA f_lineorder FROM SERVER arrow_fdw INTO public OPTIONS (dir '/opt/arrow/mydata', pattern 'f_lineorder__${shipmode}.arrow'); IMPORT FOREIGN SCHEMA postgres=# \\d f_lineorder Foreign table \"public.f_lineorder\" Column | Type | Collation | Nullable | Default | FDW options --------------------+---------------+-----------+----------+---------+---------------------- lo_orderkey | numeric | | | | lo_linenumber | integer | | | | lo_custkey | numeric | | | | lo_partkey | integer | | | | lo_suppkey | numeric | | | | lo_orderdate | integer | | | | lo_orderpriority | character(15) | | | | lo_shippriority | character(1) | | | | lo_quantity | numeric | | | | lo_extendedprice | numeric | | | | lo_ordertotalprice | numeric | | | | lo_discount | numeric | | | | lo_revenue | numeric | | | | lo_supplycost | numeric | | | | lo_tax | numeric | | | | lo_commit_date | character(8) | | | | lo_shipmode | character(10) | | | | year | bigint | | | | (virtual 'year') shipmode | text | | | | (virtual 'shipmode') Server: arrow_fdw FDW options: (dir '/opt/arrow/mydata', pattern 'f_lineorder__${shipmode}.arrow') In this system, data output from the lineorder table of SSBM (Star Schema Benchmark) is stored in the /opt/arrow/mydata directory by year of lo_ordate and value of lo_shipmode . In other words, the value read from the lo_orderdate field in the f_lineorder_1995_RAIL.arrow file is always contains the value larger than or equal to 19950101 and less than or equal to 19951231. This gives rise to the idea that by using the \"year\" value embedded in the file name, it may be possible to skip Apache Arrow files that clearly do not have matching rows, thereby speeding up the response time of queries. This is the virtual column feature of Arrow_Fdw, and in this example, the year column and shipmode column with the column option virtual correspond to the virtual columns. Although these columns do not actually exist in the Apache Arrow files under the directory /opt/arrow/mydata , @{year} and ${shipmode} act as wildcards in the string specified in the foreign table option pattern . When year and shipmode are specified in the virtual column options, the part of the file name that matches the specified wildcard will behave as if it were the value of that virtual column. For example, the value of the virtual column year in the row derived from f_lineorder_1995_RAIL.arrow will be 1995 , and the value of the virtual column shipmode will be 'RAIL' . You can take advantage of this property to optimize your queries. The following example searches for aggregate values on this foreign table by searching data from 1993 and some additional conditions. Compared to adding the range condition between 19930101 and 19931231 to the value of the column lo_orderdate that physically exists in the Apache Arrow file, the search condition adjusted to compare the virtual columns year and 1993 skips 48 of the total 60 Record-Batches according to Stats-Hint: in the EXPLAIN ANALYZE output, and actually processes only 12 Record-Batches (1/5, but the result is the same). without virtual-column optimization =# explain analyze select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where lo_orderdate between 19930101 and 19931231 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ------------------------------------------------------------------------------------------ Aggregate (cost=463921.94..463921.95 rows=1 width=32) (actual time=175.826..175.828 rows=1 loops=1) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=463921.92..463921.93 rows=1 width=32) \\ (actual time=175.808..175.811 rows=2 loops=1) GPU Projection: pgstrom.psum((lo_extendedprice * lo_discount)) GPU Scan Quals: ((lo_orderdate >= 19930101) AND (lo_orderdate <= 19931231) AND \\ (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND \\ (lo_quantity < '25'::numeric)) [plan: 65062080 -> 542, exec: 65062081 -> 1703647] GPU Group Key: referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 107.83MB, size: 427.16MB) file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 107.82MB, size: 427.13MB) : file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 107.51MB, size: 425.91MB) GPU-Direct SQL: enabled (N=2,GPU0,1; direct=413081, ntuples=65062081) Planning Time: 0.769 ms Execution Time: 176.390 ms (39 rows) =# select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where lo_orderdate between 19930101 and 19931231 and lo_discount between 1 and 3 and lo_quantity < 25; revenue --------------- 6385711057885 (1 row) with virtual-column optimization =# explain analyze select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ------------------------------------------------------------------------------------------ Aggregate (cost=421986.99..421987.00 rows=1 width=32) (actual time=54.624..54.625 rows=1 loops=1) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=421986.97..421986.98 rows=1 width=32) \\ (actual time=54.616..54.618 rows=2 loops=1) GPU Projection: pgstrom.psum((lo_extendedprice * lo_discount)) GPU Scan Quals: ((year = 1993) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric) \\ AND (lo_discount >= '1'::numeric)) [plan: 65062080 -> 542, exec:13010375 -> 1703647] GPU Group Key: referenced: lo_quantity, lo_extendedprice, lo_discount, year Stats-Hint: (year = 1993) [loaded: 12, skipped: 48] file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 99.53MB, size: 427.16MB) file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 99.52MB, size: 427.13MB) file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 99.24MB, size: 425.91MB) GPU-Direct SQL: enabled (N=2,GPU0,1; direct=76245, ntuples=13010375) Planning Time: 0.640 ms Execution Time: 55.078 ms (40 rows) =# select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; revenue --------------- 6385711057885 (1 row) AVG(numeric), SUM(numeric) accuracy improvement Due to limitations on atomic operations on GPUs (floating-point type atomicAdd() only supports up to 64 bits), numeric aggregation has previously been implemented with values tranformed to 64bit floating-point type (float8). However, in this case, double-precision floating-point data, which has a mantissa of only 53 bits, is added millions of times, posing the issue of severe accumulation of calculation errors. For this reason, we have gone to the trouble of providing an option to prevent numeric aggregation functions from being executed on the GPU. ( pg_strom.enable_numeric_aggfuncs option) In v6.0, the calculation process has been improved to use the 128-bit fixed-point representation, which is the GPU internal implementation of the numeric data type. This does not mean that calculation errors have completely disappeared, but the calculation errors are much milder due to the increased number of digits used to represent real numbers. ### by CPU(PostgreSQL) postgres=# select count(*), sum(x) from t; count | sum ----------+------------------------------- 10000000 | 5000502773.174181378779819237 (1 row) ### by GPU(PG-Strom v5.2) postgres=# select count(*), sum(x) from t; count | sum ----------+------------------ 10000000 | 5022247013.24539 (1 row) postgres=# select count(*), sum(x) from t; count | sum ----------+------------------ 10000000 | 5011118562.96062 (1 row) ### by GPU(PG-Strom v6.0) postgres=# select count(*), sum(x) from t; count | sum ----------+----------------------------- 10000000 | 5000502773.1741813787793780 (1 row) postgres=# select count(*), sum(x) from t; count | sum ----------+----------------------------- 10000000 | 5000502773.1741813787793780 (1 row) GpuPreAgg final merge on GPU device When processing an aggregate operation in parallel, as is the case with CPU parallel processing in PostgreSQL, partial aggregation processing is performed first, then the intermediate results are merged at the end. For example, when calculating the average value of X, the average (AVG(X)) can be calculated using the number of occurrences of X and the total sum of X, so each worker process calculates the \"partial\" number of occurrences of X and the total sum of X, and then aggregates them in a single process at the end. This method works very effectively for low cardinality processing, such as calculating aggregate values for a small number of categories from a huge table. On the other hand, for the workloads with low cardinality and a large number of groups, such as simple duplicate removal queries, the effect of parallel processing tends to be limited because partial aggregation processing and final merge processing are performed in a single CPU thread. ### by CPU (PostgreSQL) =# EXPLAIN SELECT t1.cat, count(*) cnt, sum(a) FROM t0 JOIN t1 ON t0.cat = t1.cat GROUP BY t1.cat; QUERY PLAN ---------------------------------------------------------------------------------------------- Finalize HashAggregate (cost=193413.59..193413.89 rows=30 width=20) Group Key: t1.cat -> Gather (cost=193406.84..193413.14 rows=60 width=20) Workers Planned: 2 -> Partial HashAggregate (cost=192406.84..192407.14 rows=30 width=20) Group Key: t1.cat -> Hash Join (cost=1.68..161799.20 rows=4081019 width=12) Hash Cond: (t0.cat = t1.cat) -> Parallel Seq Scan on t0 (cost=0.00..105362.15 rows=4166715 width=4) -> Hash (cost=1.30..1.30 rows=30 width=12) -> Seq Scan on t1 (cost=0.00..1.30 rows=30 width=12) (11 rows) Previous versions of PG-Strom had the same problem. That is, although GpuPreAgg, which runs under the control of parallel worker processes, performs \"partial\" aggregation processing, the \"partial\" aggregation results processed by these CPU parallel processes must be merged in the end by a single-threaded CPU. In the example below, the execution plan is such that the Gather node (which controls the parallel worker processes) receives the results of GpuPreAgg, and then HashAggregate (which runs on a single-threaded CPU) receives them. As the number of groups increases, the proportion of the workload processed by a single CPU thread becomes significant. ### by GPU (PG-Strom v5.2) =# EXPLAIN SELECT t1.cat, count(*) cnt, sum(a) FROM t0 JOIN t1 ON t0.cat = t1.cat GROUP BY t1.cat; QUERY PLAN ------------------------------------------------------------------------------------------------ HashAggregate (cost=30100.15..30100.53 rows=30 width=20) Group Key: t1.cat -> Gather (cost=30096.63..30099.93 rows=30 width=44) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on t0 (cost=29096.63..29096.93 rows=30 width=44) GPU Projection: pgstrom.nrows(), pgstrom.psum(t1.a), t1.cat GPU Join Quals [1]: (t0.cat = t1.cat) ... [nrows: 4166715 -> 4081019] GPU Outer Hash [1]: t0.cat GPU Inner Hash [1]: t1.cat GPU Group Key: t1.cat GPU-Direct SQL: enabled (N=2,GPU0,1) -> Parallel Seq Scan on t1 (cost=0.00..1.18 rows=18 width=12) (12 rows) If the aggregation results that GpuPreAgg constructs on the GPU are not \"partial\", there is no need to perform the aggregation process again on the CPU side. The only problem that can arise in this case is CPU-Fallback processing. If some rows are processed by the CPU due to some reasons such as variable-length data being stored in an external table (TOAST is possible), it is not possible to output a consistent aggregation result using only the results in GPU memory. However, in the real world, there are not many cases where CPU-Fallback occurs. Therefore, PG-Strom v6.0 has a mode that performs the final merge process on the GPU device memory when CPU-Fallback is disabled, and omits the CPU-side aggregation process. In the example execution plan below, GpuPreAgg is placed under the Gather node, but HashAggregate is not included to finally merge it as in the previous example. Because GpuPreAgg returns a consistent result, no additional aggregation processing is required on the CPU side. ### by GPU (PG-Strom v6.0) =# set pg_strom.cpu_fallback = off; SET =# EXPLAIN SELECT t1.cat, count(*) cnt, sum(a) FROM t0 JOIN t1 ON t0.cat = t1.cat GROUP BY t1.cat; QUERY PLAN ------------------------------------------------------------------------------------------------ Gather (cost=30096.63..30100.45 rows=30 width=20) Workers Planned: 2 -> Result (cost=29096.63..29097.45 rows=30 width=20) -> Parallel Custom Scan (GpuPreAgg) on t0 (cost=29096.63..29096.93 rows=30 width=44) GPU Projection: pgstrom.nrows(), pgstrom.psum(t1.a), t1.cat GPU Join Quals [1]: (t0.cat = t1.cat) ... [nrows: 4166715 -> 4081019] GPU Outer Hash [1]: t0.cat GPU Inner Hash [1]: t1.cat GPU Group Key: t1.cat GPU-Direct SQL: enabled (N=2,GPU0,1) -> Parallel Seq Scan on t1 (cost=0.00..1.18 rows=18 width=12) (11 rows) Other New Features Improved GPU-tasks scheduling In the previous version, one GPU was assigned to a PostgreSQL backend process, and the use of multiple GPUs was premised on the use of PostgreSQL parallel queries. This design originated from the architecture of v3.x, and while it had the advantage of simplifying the implementation, it also had the problem of making multi-GPU scheduling difficult. In v6.0, the GPU-Service is responsible for all GPU task scheduling. In a multi-GPU environment, a GPU task request received from the PostgreSQL backend is assigned to the schedulable GPU with the smallest number of tasks currently queued. This makes possible to utilize the processing power of multiple GPUs in a more natural way. CUDA Stack Limit Checker A logic to check for excessive stack usage in GPU code has been added, in the recursive CUDA functions. This is expected to prevent GPU kernel crashes caused by unexpected excessive stack usage, for example in LIKE clauses that include complex patterns. RIGHT OUTER JOIN processing on GPU RIGHT OUTER JOIN, which was previously implemented using a CPU-Fallback mechanism, is now executed on the GPU. GPU-Direct SQL Decision Making Logs We got some hints on why GPU-Direct SQL does not work in some system environments. It is enabled using environment variable HETERODB_EXTRA_EREPORT_LEVEL=1 before starting PostgreSQL server process. pgstrom.arrow_fdw_metadata_info() allows to reference the metadata of Apache Arrow files. column IN (X,Y,Z,...) operator now refers MIN/MAX statistics of Arrow_Fdw. GPU assignment policy is now configurable from: optimal , numa , and system . optimal is the same as before, where the storage and GPU are closest on the PCI-E bus. numa pairs storage and GPU connected under the same CPU, preventing data transfer across QPI. system allows scheduling of all GPUs available in the system. Error messages from the heterodb-extra module can now be output to the PostgreSQL log. The log output level can be controlled with the pg_strom.extra_ereport_level parameter. The converter vcf2arrow is now included to convert the VCF format, which is a standard format for storing and exchanging genomic data, into Apache Arrow. Cumulative bug fixes [#900] bugfix: groupby_prepfn_bufsz may be initialized to 0, if num_groups is extremely large. [#890] allows wide VFS read without heterodb-extra [#886] bugfix: catalog corruption of gpupreagg [#885] pg2arrow --append overwrites the field name of the original file [#884] bugfix: arrow2csv checks 'pg_type' metadata too strictly [#879] gpupreagg: add support of FILTER-clause for aggregate functions [#871] bugfix: non-key distinct clause must be a simple Var or device executable [#865] add logs to report decision making of GPU-Direct SQL. [#864] arrow_fdw: metadata cache refactoring for custom key-value displaying [#860] bugfix: MIN() MAX() returned empty result if nitems is multiple of 2^32 [#856] add fallback by managed-memory if raw-gpu-memory exceeds the hard limit [#852] wrong template deployment of move_XXXX_value() callback [#851] bugfix: pgstromExecResetTaskState didn't work correctly [#847] bugfix: max(float) used wrong operator [#844] postgis: st_crosses() should return false for identical linestring geometry [#831] arrow-fdw: incorrect record-batch index calculation [#829] bugfix: GpuScan considered inheritance-table as if it is normal table [#829] bugfix: pgstrom_partial_nrows() didn't return correct value if '0' is given [#827] bugfix: RIGHT OUTER tuple didn't check 'other_quals' in 'join_quals' [#825] arrowFieldTypeIsEqual didn't work for FixedSizeBinary [#824] pg2arrow: moved empty results check to the end of file creation. [#820] bugfix: CPU-fallback ExprState was not initialized correctly [#820] additional fix on CPU-fallback expression references [#819] bugfix: a bunch of rows were skipped after GPU kernel suspend/resume [#817] GPU-Service didn't detach worker thread's exit status. [#815] fluentd: arrow_file_write module was updated to sync fluentd4 [#812] CPU-fallback at depth>0 didn't set ecxt_scanslot correctly. [#812] bugfix: pgfn_st_crosses() returned uninitialized results [#812] fix wrong CPU fallback at GiST-Join [#811] add delay to apply pg_strom.max_async_tasks [#809][#810] Documentation fix [#808] pg2arrow: put_decimal_value() handles numeric with negative weight incorrectly. [#805] CREATE OPERATOR makes a pseudo commutor/negator operators in the default namespace [#743][#838] nvcc working directory is moved to $PGDATA/.pgstrom_fatbin_build_XXXXXX [#xxx] pg2arrow: raise an error if numeric value is Nan, +Inf or -Inf [#xxx] bugfix: CPU-fallback handling of system columns [#xxx] bugfix: cuMemcpyPeer() caused SEGV when # of threads > 20 [#xxx] bugfix: scan_block_count was not initialized on the DSM [#xxx] bugfix: some GpuPreAgg final functions didn't care NULL input [#xxx] bugfix: aggregate function (co-variation) wrong definition [#xxx] bugfix: __releaseMetadataCacheBlock referenced NULL when multiple cache blocks released. [#xxx] bugfix: PROCOID cache entry was not released [#xxx] bugfix: kern_global_stair_sum_u32() didn't return right value","title":"PG-Strom v6.0"},{"location":"release_v6.0/#pg-strom-v60-release","text":"PG-Strom Development Team (1st-Apr-2025)","title":"PG-Strom v6.0 Release"},{"location":"release_v6.0/#overview","text":"Major changes in PG-Strom v6.0 are as follows: GPU-Sort and a few Window-functions Partitioned Pinned Inner Buffer Arrow_Fdw Virtual Columns AVG(numeric), SUM(numeric) accuracy improvement GpuPreAgg final merge on GPU device Improved GPU-tasks scheduling Cumulative bug fixes","title":"Overview"},{"location":"release_v6.0/#prerequisites","text":"PostgreSQL v15 or later CUDA Toolkit 12.2 or later Linux distributions supported by CUDA Toolkit Intel x86 64bit architecture (x86_64) NVIDIA GPU CC 6.0 or later (Pascal at least; Turing or newer is recommended)","title":"Prerequisites"},{"location":"release_v6.0/#gpu-sort-and-a-few-window-functions","text":"PG-Strom v6.0 now supports GPU-Sort. Sorting is a typical workload suited to GPUs, and we had tried to implement it in the early version of PG-Strom. However, the RAM of early stage GPUs was small at that time, so sorting data that fits in the GPU's RAM did not improve the speed significantly. In addition, the sorting workload does not reduce the amount of data, so CPU processing tends to get stucked after receiving the data from the GPU. Therefore, we decided not to implement it at that time. However, as of 2025, the memory of high-end GPUs has reached tens of GB, and it has become clear that combining sorting with LIMIT clauses and window functions is effective for many workloads. Due to the nature of sorting, all target data must be stored in GPU memory. On the other hand, various GPU-operators in PG-Strom have a mechanism called CPU-Fallback, which re-executes the processing of operators or SQL functions on the CPU when they cannot be completed on the GPU. A typical example is when variable-length data does not fit into the PostgreSQL block size (8kB) and is stored in an external TOAST table. CPU-Fallback is a function to ensure continuity of processing even for extremely exceptional data, but rows that are subject to CPU-Fallback are stored in the CPU and do not exist in GPU memory, which causes an obstacle when sorting. Therefore, the GPU-Sort function works only when the CPU-Fallback function is disabled, i.e., when pg_strom.cpu_fallback=off is set. When CPU-Fallback is disabled, the complete result-set of GPU-Scan/Join/PreAgg should be in GPU memory, so PG-Strom can perform parallel sorting based on the Bitonic-Sorting algorithm and return the sorted results to the CPU. When used with a LIMIT clause or a window function that limits the number of rows, such as rank() < 4 , it will reduce the number of data returned to the CPU based on these optimization hints. This should contribute to speedup by reducing the number of data to be processed by the CPU. Please see the execution plan below. This is the result of narrowing down the number of rows using a window function ( rank() < 4 ) without enabling GPU-Sort. =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=32013352.01..33893039.51 rows=75187500 width=84) (actual time=13158.987..13335.106 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Sort (cost=32013352.01..32201320.76 rows=75187500 width=76) (actual time=13158.976..13238.136 rows=601500 loops=1) Sort Key: customer.c_region, customer.c_nation, customer.c_city, (pgstrom.sum_numeric((pgstrom.psum(lineorder.lo_revenue)))) Sort Method: quicksort Memory: 76268kB -> HashAggregate (cost=15987574.35..18836475.71 rows=75187500 width=76) (actual time=9990.801..10271.543 rows=601500 loops=1) Group Key: customer.c_region, customer.c_nation, customer.c_city, lineorder.lo_orderdate Planned Partitions: 8 Batches: 1 Memory Usage: 516113kB -> Custom Scan (GpuPreAgg) on lineorder (cost=4967906.38..5907750.13 rows=75187500 width=76) (actual time=9175.476..9352.529 rows=1203000 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.008..519.064 rows=3000000 loops=1) Planning Time: 1.395 ms Execution Time: 13494.808 ms (19 rows) After GPU-PreAgg, HashAggregate is run to aggregate the partial aggregation results, and Sort is run to sort the aggregated values. Finally, WindowAgg is run to narrow down the results to the top three sum(lo_revenue) for each c_region , c_nation , and c_city . The processing time for GPU-PreAgg is 9.352 seconds, so we can see that roughly 4 seconds of the latter half of the process was spent on the CPU. On the other hand, the following execution plan disables CPU-Fallback by setting pg_strom.cpu_fallback=off (i.e. enables GPU-Sort). =# set pg_strom.cpu_fallback = off; SET =# explain analyze select * from ( select c_region, c_nation, c_city, lo_orderdate, sum(lo_revenue) lo_rev, rank() over(partition by c_region, c_nation, c_city order by sum(lo_revenue)) cnt from lineorder, customer where lo_custkey = c_custkey and lo_shipmode in ('RAIL','SHIP') group by c_region, c_nation, c_city, lo_orderdate ) subqry where cnt < 4; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------------------- WindowAgg (cost=5595978.47..5602228.47 rows=125000 width=84) (actual time=9596.930..9598.194 rows=750 loops=1) Run Condition: (rank() OVER (?) < 4) -> Result (cost=5595978.47..5599415.97 rows=125000 width=76) (actual time=9596.918..9597.292 rows=750 loops=1) -> Custom Scan (GpuPreAgg) on lineorder (cost=5595978.47..5597540.97 rows=125000 width=76) (actual time=9596.912..9597.061 rows=750 loops=1) GPU Projection: pgstrom.psum(lo_revenue), c_region, c_nation, c_city, lo_orderdate GPU Scan Quals: (lo_shipmode = ANY ('{RAIL,SHIP}'::bpchar[])) [plan: 600046000 -> 171773200, exec: 1311339 -> 362780] GPU Join Quals [1]: (lo_custkey = c_custkey) [plan: 171773200 -> 171773200, exec: 362780 -> 322560 GPU Outer Hash [1]: lo_custkey GPU Inner Hash [1]: c_custkey GPU Group Key: c_region, c_nation, c_city, lo_orderdate Scan-Engine: GPU-Direct with 2 GPUs <0,1>; direct=11395910, ntuples=1311339 GPU-Sort keys: c_region, c_nation, c_city, pgstrom.fsum_numeric((pgstrom.psum(lo_revenue))) Window-Rank Filter: rank() over(PARTITION BY c_region, c_nation, c_city ORDER BY pgstrom.fsum_numeric((pgstrom.psum(lo_revenue)))) < 4 -> Seq Scan on customer (cost=0.00..81963.11 rows=3000011 width=46) (actual time=0.006..475.006 rows=3000000 loops=1) Planning Time: 0.381 ms Execution Time: 9710.616 ms (16 rows) The HashAggregate and Sort that were in the original query plan have gone, and instead, the lines GPU-Sort keys and Window-Rank Filter have appeared as options for GpuPreAgg . This indicates that GpuPreAgg creates the complete aggregation on the GPU, then sorts and outputs it. In addition, in this query, filtering is performed using the window function rank() . By pushing down this condition to the lower node GpuPreAgg , rows that are known to be filtered out in advance are removed from the result set, reducing the amount of data transferred from the GPU to the CPU and the number of rows that the CPU needs to copy. These processes are processed in parallel on the GPU memory, so they are generally faster than CPU processing.","title":"GPU-Sort and a few Window-functions"},{"location":"release_v6.0/#multi-gpus-pinned-inner-buffer","text":"PG-Strom v5.2 supported GPU-Join's Pinned Inner Buffer . This feature speeds up the construction of a hash table when GPU-Scan or GPU-Join is connected as an INNER lower plan of GPU-Join, and the processing results can be used as a hash table for GPU-Join, which is one of the parallelized HashJoins. Previously, the processing results of GPU-Scan or GPU-Join were sent back to the CPU to construct a hash table, but now the results are stored in GPU memory and used in the subsequent GPU-Join. This prevents data from moving from GPU to CPU and back again, which improves processing speed, especially when the size of the INNER side is large. However, in many systems, the capacity of the GPU's RAM is limited compared to the RAM of the server itself, and there are also restrictions on the size of the hash table. This restriction can be alleviated by dividing the hash table into multiple GPUs, but if an INNER row placed on another GPU is referenced while HashJoin is being executed on one GPU, a phenomenon called GPU memory thrashing occurs, resulting in a severe slowdown in speed. Therefore, a mechanism was needed to ensure the locality of memory access while HashJoin is being executed. In PG-Strom v6.0, the Pinned-Inner-Buffer of GPU-Join supports multiple GPUs. Please see the following diagram. When scanning the INNER table of GPU-Join is executed on multiple GPUs and the result is stored in the GPU memory to build a hash table, it is completely random which rows are stored on each GPU. In the next step, Hash-Join, if a row read from the OUTER side is first joined with an INNER row on GPU1, then with an INNER row on GPU2, and finally with an INNER row on GPU0, extreme thrashing will occur, causing a severe performance drop. Therefore, in PG-Strom v6.0's GPU-Join, when using Pinned-Inner-Buffer on multiple GPUs, a reconstruction process is inserted to reallocate the hash table on the appropriate GPU. For example, in a system equipped with three GPUs, if the size of the hash table fits roughly into the RAM of the three GPUs, after the GPU-Scan of the INNER table is completed, the hash value of the join key to be used in the next GPU-Join is calculated, and if the remainder when dividing this by 3 is 0, it is reallocated to GPU0, if it is 1 then it is reallocated to GPU1, and if it is 2 then it is reallocated to GPU2. By inserting this process, it is possible to create a state in which when GPU-Join is executed on GPU0, the hash table will only contain INNER rows whose remainder when the hash value is divided by 3 is 0, and similarly, GPU1 will only contain INNER rows whose remainder when the hash value is divided by 3 is 1. Next, when GPU-Join is executed using this divided hash table, when the GPU that first loaded data from the OUTER table (let's call it GPU2 here) references the hash table, if the remainder when dividing the hash value calculated from the OUTER row by 3 is other than 2, then there will obviously be no matching INNER row on that GPU. Therefore, GPU2 will generate a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 2. Next, this OUTER data is transferred to GPU1 by GPU-to-GPU Copy, which generates a join result consisting of only hash values \u200b\u200bwhose remainder when divided by 3 is 1. By repeating this process, \"partial Hash-Join results\" are generated on each GPU, but the combination of these is equal to the complete Hash-Join result, and as a result, it is now possible to execute GPU-Join even if the INNER hash table is larger in size than the GPU's on-board RAM. In relation to this feature, the pg_strom.pinned_inner_buffer_partition_size parameter has been added. This specifies the threshold size for dividing the Pinned-Inner-Buffer among multiple GPUs. The initial value is set to about 80-90% of the GPU's installed memory, so administrators usually do not need to change this.","title":"Multi-GPUs Pinned Inner Buffer"},{"location":"release_v6.0/#arrow_fdw-virtual-columns","text":"When backing up data, it is common to set up a naming convention so that part of the file name indicates the attributes of the data. For example, a file name like my_data_2018_tokyo.arrow might convey that the data stored here is from the Tokyo area in 2018. In Arrow_Fdw foreign table, you can use dir option to access multiple Apache Arrow files with SQL. If the file name has some meaning, you can use it as a hint to speed up access to Arrow_Fdw foreign table. Here is an example. $ ls /opt/arrow/mydata f_lineorder_1993_AIR.arrow f_lineorder_1994_SHIP.arrow f_lineorder_1996_MAIL.arrow f_lineorder_1993_FOB.arrow f_lineorder_1994_TRUCK.arrow f_lineorder_1996_RAIL.arrow f_lineorder_1993_MAIL.arrow f_lineorder_1995_AIR.arrow f_lineorder_1996_SHIP.arrow f_lineorder_1993_RAIL.arrow f_lineorder_1995_FOB.arrow f_lineorder_1996_TRUCK.arrow f_lineorder_1993_SHIP.arrow f_lineorder_1995_MAIL.arrow f_lineorder_1997_AIR.arrow f_lineorder_1993_TRUCK.arrow f_lineorder_1995_RAIL.arrow f_lineorder_1997_FOB.arrow f_lineorder_1994_AIR.arrow f_lineorder_1995_SHIP.arrow f_lineorder_1997_MAIL.arrow f_lineorder_1994_FOB.arrow f_lineorder_1995_TRUCK.arrow f_lineorder_1997_RAIL.arrow f_lineorder_1994_MAIL.arrow f_lineorder_1996_AIR.arrow f_lineorder_1997_SHIP.arrow f_lineorder_1994_RAIL.arrow f_lineorder_1996_FOB.arrow f_lineorder_1997_TRUCK.arrow postgres=# IMPORT FOREIGN SCHEMA f_lineorder FROM SERVER arrow_fdw INTO public OPTIONS (dir '/opt/arrow/mydata', pattern 'f_lineorder__${shipmode}.arrow'); IMPORT FOREIGN SCHEMA postgres=# \\d f_lineorder Foreign table \"public.f_lineorder\" Column | Type | Collation | Nullable | Default | FDW options --------------------+---------------+-----------+----------+---------+---------------------- lo_orderkey | numeric | | | | lo_linenumber | integer | | | | lo_custkey | numeric | | | | lo_partkey | integer | | | | lo_suppkey | numeric | | | | lo_orderdate | integer | | | | lo_orderpriority | character(15) | | | | lo_shippriority | character(1) | | | | lo_quantity | numeric | | | | lo_extendedprice | numeric | | | | lo_ordertotalprice | numeric | | | | lo_discount | numeric | | | | lo_revenue | numeric | | | | lo_supplycost | numeric | | | | lo_tax | numeric | | | | lo_commit_date | character(8) | | | | lo_shipmode | character(10) | | | | year | bigint | | | | (virtual 'year') shipmode | text | | | | (virtual 'shipmode') Server: arrow_fdw FDW options: (dir '/opt/arrow/mydata', pattern 'f_lineorder__${shipmode}.arrow') In this system, data output from the lineorder table of SSBM (Star Schema Benchmark) is stored in the /opt/arrow/mydata directory by year of lo_ordate and value of lo_shipmode . In other words, the value read from the lo_orderdate field in the f_lineorder_1995_RAIL.arrow file is always contains the value larger than or equal to 19950101 and less than or equal to 19951231. This gives rise to the idea that by using the \"year\" value embedded in the file name, it may be possible to skip Apache Arrow files that clearly do not have matching rows, thereby speeding up the response time of queries. This is the virtual column feature of Arrow_Fdw, and in this example, the year column and shipmode column with the column option virtual correspond to the virtual columns. Although these columns do not actually exist in the Apache Arrow files under the directory /opt/arrow/mydata , @{year} and ${shipmode} act as wildcards in the string specified in the foreign table option pattern . When year and shipmode are specified in the virtual column options, the part of the file name that matches the specified wildcard will behave as if it were the value of that virtual column. For example, the value of the virtual column year in the row derived from f_lineorder_1995_RAIL.arrow will be 1995 , and the value of the virtual column shipmode will be 'RAIL' . You can take advantage of this property to optimize your queries. The following example searches for aggregate values on this foreign table by searching data from 1993 and some additional conditions. Compared to adding the range condition between 19930101 and 19931231 to the value of the column lo_orderdate that physically exists in the Apache Arrow file, the search condition adjusted to compare the virtual columns year and 1993 skips 48 of the total 60 Record-Batches according to Stats-Hint: in the EXPLAIN ANALYZE output, and actually processes only 12 Record-Batches (1/5, but the result is the same). without virtual-column optimization =# explain analyze select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where lo_orderdate between 19930101 and 19931231 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ------------------------------------------------------------------------------------------ Aggregate (cost=463921.94..463921.95 rows=1 width=32) (actual time=175.826..175.828 rows=1 loops=1) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=463921.92..463921.93 rows=1 width=32) \\ (actual time=175.808..175.811 rows=2 loops=1) GPU Projection: pgstrom.psum((lo_extendedprice * lo_discount)) GPU Scan Quals: ((lo_orderdate >= 19930101) AND (lo_orderdate <= 19931231) AND \\ (lo_discount >= '1'::numeric) AND (lo_discount <= '3'::numeric) AND \\ (lo_quantity < '25'::numeric)) [plan: 65062080 -> 542, exec: 65062081 -> 1703647] GPU Group Key: referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 107.83MB, size: 427.16MB) file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 107.82MB, size: 427.13MB) : file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 107.51MB, size: 425.91MB) GPU-Direct SQL: enabled (N=2,GPU0,1; direct=413081, ntuples=65062081) Planning Time: 0.769 ms Execution Time: 176.390 ms (39 rows) =# select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where lo_orderdate between 19930101 and 19931231 and lo_discount between 1 and 3 and lo_quantity < 25; revenue --------------- 6385711057885 (1 row) with virtual-column optimization =# explain analyze select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; QUERY PLAN ------------------------------------------------------------------------------------------ Aggregate (cost=421986.99..421987.00 rows=1 width=32) (actual time=54.624..54.625 rows=1 loops=1) -> Custom Scan (GpuPreAgg) on f_lineorder (cost=421986.97..421986.98 rows=1 width=32) \\ (actual time=54.616..54.618 rows=2 loops=1) GPU Projection: pgstrom.psum((lo_extendedprice * lo_discount)) GPU Scan Quals: ((year = 1993) AND (lo_discount <= '3'::numeric) AND (lo_quantity < '25'::numeric) \\ AND (lo_discount >= '1'::numeric)) [plan: 65062080 -> 542, exec:13010375 -> 1703647] GPU Group Key: referenced: lo_quantity, lo_extendedprice, lo_discount, year Stats-Hint: (year = 1993) [loaded: 12, skipped: 48] file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 99.53MB, size: 427.16MB) file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 99.52MB, size: 427.13MB) file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 99.24MB, size: 425.91MB) GPU-Direct SQL: enabled (N=2,GPU0,1; direct=76245, ntuples=13010375) Planning Time: 0.640 ms Execution Time: 55.078 ms (40 rows) =# select sum(lo_extendedprice*lo_discount) as revenue from f_lineorder where year = 1993 and lo_discount between 1 and 3 and lo_quantity < 25; revenue --------------- 6385711057885 (1 row)","title":"Arrow_Fdw Virtual Columns"},{"location":"release_v6.0/#avgnumeric-sumnumeric-accuracy-improvement","text":"Due to limitations on atomic operations on GPUs (floating-point type atomicAdd() only supports up to 64 bits), numeric aggregation has previously been implemented with values tranformed to 64bit floating-point type (float8). However, in this case, double-precision floating-point data, which has a mantissa of only 53 bits, is added millions of times, posing the issue of severe accumulation of calculation errors. For this reason, we have gone to the trouble of providing an option to prevent numeric aggregation functions from being executed on the GPU. ( pg_strom.enable_numeric_aggfuncs option) In v6.0, the calculation process has been improved to use the 128-bit fixed-point representation, which is the GPU internal implementation of the numeric data type. This does not mean that calculation errors have completely disappeared, but the calculation errors are much milder due to the increased number of digits used to represent real numbers. ### by CPU(PostgreSQL) postgres=# select count(*), sum(x) from t; count | sum ----------+------------------------------- 10000000 | 5000502773.174181378779819237 (1 row) ### by GPU(PG-Strom v5.2) postgres=# select count(*), sum(x) from t; count | sum ----------+------------------ 10000000 | 5022247013.24539 (1 row) postgres=# select count(*), sum(x) from t; count | sum ----------+------------------ 10000000 | 5011118562.96062 (1 row) ### by GPU(PG-Strom v6.0) postgres=# select count(*), sum(x) from t; count | sum ----------+----------------------------- 10000000 | 5000502773.1741813787793780 (1 row) postgres=# select count(*), sum(x) from t; count | sum ----------+----------------------------- 10000000 | 5000502773.1741813787793780 (1 row)","title":"AVG(numeric), SUM(numeric) accuracy improvement"},{"location":"release_v6.0/#gpupreagg-final-merge-on-gpu-device","text":"When processing an aggregate operation in parallel, as is the case with CPU parallel processing in PostgreSQL, partial aggregation processing is performed first, then the intermediate results are merged at the end. For example, when calculating the average value of X, the average (AVG(X)) can be calculated using the number of occurrences of X and the total sum of X, so each worker process calculates the \"partial\" number of occurrences of X and the total sum of X, and then aggregates them in a single process at the end. This method works very effectively for low cardinality processing, such as calculating aggregate values for a small number of categories from a huge table. On the other hand, for the workloads with low cardinality and a large number of groups, such as simple duplicate removal queries, the effect of parallel processing tends to be limited because partial aggregation processing and final merge processing are performed in a single CPU thread. ### by CPU (PostgreSQL) =# EXPLAIN SELECT t1.cat, count(*) cnt, sum(a) FROM t0 JOIN t1 ON t0.cat = t1.cat GROUP BY t1.cat; QUERY PLAN ---------------------------------------------------------------------------------------------- Finalize HashAggregate (cost=193413.59..193413.89 rows=30 width=20) Group Key: t1.cat -> Gather (cost=193406.84..193413.14 rows=60 width=20) Workers Planned: 2 -> Partial HashAggregate (cost=192406.84..192407.14 rows=30 width=20) Group Key: t1.cat -> Hash Join (cost=1.68..161799.20 rows=4081019 width=12) Hash Cond: (t0.cat = t1.cat) -> Parallel Seq Scan on t0 (cost=0.00..105362.15 rows=4166715 width=4) -> Hash (cost=1.30..1.30 rows=30 width=12) -> Seq Scan on t1 (cost=0.00..1.30 rows=30 width=12) (11 rows) Previous versions of PG-Strom had the same problem. That is, although GpuPreAgg, which runs under the control of parallel worker processes, performs \"partial\" aggregation processing, the \"partial\" aggregation results processed by these CPU parallel processes must be merged in the end by a single-threaded CPU. In the example below, the execution plan is such that the Gather node (which controls the parallel worker processes) receives the results of GpuPreAgg, and then HashAggregate (which runs on a single-threaded CPU) receives them. As the number of groups increases, the proportion of the workload processed by a single CPU thread becomes significant. ### by GPU (PG-Strom v5.2) =# EXPLAIN SELECT t1.cat, count(*) cnt, sum(a) FROM t0 JOIN t1 ON t0.cat = t1.cat GROUP BY t1.cat; QUERY PLAN ------------------------------------------------------------------------------------------------ HashAggregate (cost=30100.15..30100.53 rows=30 width=20) Group Key: t1.cat -> Gather (cost=30096.63..30099.93 rows=30 width=44) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on t0 (cost=29096.63..29096.93 rows=30 width=44) GPU Projection: pgstrom.nrows(), pgstrom.psum(t1.a), t1.cat GPU Join Quals [1]: (t0.cat = t1.cat) ... [nrows: 4166715 -> 4081019] GPU Outer Hash [1]: t0.cat GPU Inner Hash [1]: t1.cat GPU Group Key: t1.cat GPU-Direct SQL: enabled (N=2,GPU0,1) -> Parallel Seq Scan on t1 (cost=0.00..1.18 rows=18 width=12) (12 rows) If the aggregation results that GpuPreAgg constructs on the GPU are not \"partial\", there is no need to perform the aggregation process again on the CPU side. The only problem that can arise in this case is CPU-Fallback processing. If some rows are processed by the CPU due to some reasons such as variable-length data being stored in an external table (TOAST is possible), it is not possible to output a consistent aggregation result using only the results in GPU memory. However, in the real world, there are not many cases where CPU-Fallback occurs. Therefore, PG-Strom v6.0 has a mode that performs the final merge process on the GPU device memory when CPU-Fallback is disabled, and omits the CPU-side aggregation process. In the example execution plan below, GpuPreAgg is placed under the Gather node, but HashAggregate is not included to finally merge it as in the previous example. Because GpuPreAgg returns a consistent result, no additional aggregation processing is required on the CPU side. ### by GPU (PG-Strom v6.0) =# set pg_strom.cpu_fallback = off; SET =# EXPLAIN SELECT t1.cat, count(*) cnt, sum(a) FROM t0 JOIN t1 ON t0.cat = t1.cat GROUP BY t1.cat; QUERY PLAN ------------------------------------------------------------------------------------------------ Gather (cost=30096.63..30100.45 rows=30 width=20) Workers Planned: 2 -> Result (cost=29096.63..29097.45 rows=30 width=20) -> Parallel Custom Scan (GpuPreAgg) on t0 (cost=29096.63..29096.93 rows=30 width=44) GPU Projection: pgstrom.nrows(), pgstrom.psum(t1.a), t1.cat GPU Join Quals [1]: (t0.cat = t1.cat) ... [nrows: 4166715 -> 4081019] GPU Outer Hash [1]: t0.cat GPU Inner Hash [1]: t1.cat GPU Group Key: t1.cat GPU-Direct SQL: enabled (N=2,GPU0,1) -> Parallel Seq Scan on t1 (cost=0.00..1.18 rows=18 width=12) (11 rows)","title":"GpuPreAgg final merge on GPU device"},{"location":"release_v6.0/#other-new-features","text":"Improved GPU-tasks scheduling In the previous version, one GPU was assigned to a PostgreSQL backend process, and the use of multiple GPUs was premised on the use of PostgreSQL parallel queries. This design originated from the architecture of v3.x, and while it had the advantage of simplifying the implementation, it also had the problem of making multi-GPU scheduling difficult. In v6.0, the GPU-Service is responsible for all GPU task scheduling. In a multi-GPU environment, a GPU task request received from the PostgreSQL backend is assigned to the schedulable GPU with the smallest number of tasks currently queued. This makes possible to utilize the processing power of multiple GPUs in a more natural way. CUDA Stack Limit Checker A logic to check for excessive stack usage in GPU code has been added, in the recursive CUDA functions. This is expected to prevent GPU kernel crashes caused by unexpected excessive stack usage, for example in LIKE clauses that include complex patterns. RIGHT OUTER JOIN processing on GPU RIGHT OUTER JOIN, which was previously implemented using a CPU-Fallback mechanism, is now executed on the GPU. GPU-Direct SQL Decision Making Logs We got some hints on why GPU-Direct SQL does not work in some system environments. It is enabled using environment variable HETERODB_EXTRA_EREPORT_LEVEL=1 before starting PostgreSQL server process. pgstrom.arrow_fdw_metadata_info() allows to reference the metadata of Apache Arrow files. column IN (X,Y,Z,...) operator now refers MIN/MAX statistics of Arrow_Fdw. GPU assignment policy is now configurable from: optimal , numa , and system . optimal is the same as before, where the storage and GPU are closest on the PCI-E bus. numa pairs storage and GPU connected under the same CPU, preventing data transfer across QPI. system allows scheduling of all GPUs available in the system. Error messages from the heterodb-extra module can now be output to the PostgreSQL log. The log output level can be controlled with the pg_strom.extra_ereport_level parameter. The converter vcf2arrow is now included to convert the VCF format, which is a standard format for storing and exchanging genomic data, into Apache Arrow.","title":"Other New Features"},{"location":"release_v6.0/#cumulative-bug-fixes","text":"[#900] bugfix: groupby_prepfn_bufsz may be initialized to 0, if num_groups is extremely large. [#890] allows wide VFS read without heterodb-extra [#886] bugfix: catalog corruption of gpupreagg [#885] pg2arrow --append overwrites the field name of the original file [#884] bugfix: arrow2csv checks 'pg_type' metadata too strictly [#879] gpupreagg: add support of FILTER-clause for aggregate functions [#871] bugfix: non-key distinct clause must be a simple Var or device executable [#865] add logs to report decision making of GPU-Direct SQL. [#864] arrow_fdw: metadata cache refactoring for custom key-value displaying [#860] bugfix: MIN() MAX() returned empty result if nitems is multiple of 2^32 [#856] add fallback by managed-memory if raw-gpu-memory exceeds the hard limit [#852] wrong template deployment of move_XXXX_value() callback [#851] bugfix: pgstromExecResetTaskState didn't work correctly [#847] bugfix: max(float) used wrong operator [#844] postgis: st_crosses() should return false for identical linestring geometry [#831] arrow-fdw: incorrect record-batch index calculation [#829] bugfix: GpuScan considered inheritance-table as if it is normal table [#829] bugfix: pgstrom_partial_nrows() didn't return correct value if '0' is given [#827] bugfix: RIGHT OUTER tuple didn't check 'other_quals' in 'join_quals' [#825] arrowFieldTypeIsEqual didn't work for FixedSizeBinary [#824] pg2arrow: moved empty results check to the end of file creation. [#820] bugfix: CPU-fallback ExprState was not initialized correctly [#820] additional fix on CPU-fallback expression references [#819] bugfix: a bunch of rows were skipped after GPU kernel suspend/resume [#817] GPU-Service didn't detach worker thread's exit status. [#815] fluentd: arrow_file_write module was updated to sync fluentd4 [#812] CPU-fallback at depth>0 didn't set ecxt_scanslot correctly. [#812] bugfix: pgfn_st_crosses() returned uninitialized results [#812] fix wrong CPU fallback at GiST-Join [#811] add delay to apply pg_strom.max_async_tasks [#809][#810] Documentation fix [#808] pg2arrow: put_decimal_value() handles numeric with negative weight incorrectly. [#805] CREATE OPERATOR makes a pseudo commutor/negator operators in the default namespace [#743][#838] nvcc working directory is moved to $PGDATA/.pgstrom_fatbin_build_XXXXXX [#xxx] pg2arrow: raise an error if numeric value is Nan, +Inf or -Inf [#xxx] bugfix: CPU-fallback handling of system columns [#xxx] bugfix: cuMemcpyPeer() caused SEGV when # of threads > 20 [#xxx] bugfix: scan_block_count was not initialized on the DSM [#xxx] bugfix: some GpuPreAgg final functions didn't care NULL input [#xxx] bugfix: aggregate function (co-variation) wrong definition [#xxx] bugfix: __releaseMetadataCacheBlock referenced NULL when multiple cache blocks released. [#xxx] bugfix: PROCOID cache entry was not released [#xxx] bugfix: kern_global_stair_sum_u32() didn't return right value","title":"Cumulative bug fixes"},{"location":"ssd2gpu/","text":"GPU-Direct SQL Overview For the fast execution of SQL workloads, it needs to provide processors rapid data stream from storage or memory, in addition to processor's execution efficiency. Processor will run idle if data stream would not be delivered. GPUDirect SQL Execution directly connects NVMe-SSD which enables high-speed I/O processing by direct attach to the PCIe bus and GPU device that is also attached on the same PCIe bus, and runs SQL workloads very high speed by supplying data stream close to the wired speed of the hardware. Usually, PostgreSQL data blocks on the storage shall be once loaded to CPU/RAM through the PCIe bus, then, PostgreSQL runs WHERE-clause for filtering or JOIN/GROUP BY according to the query execution plan. Due to the characteristics of analytic workloads, the amount of result data set is much smaller than the source data set. For example, it is not rare case to read billions rows but output just hundreds rows after the aggregation operations with GROUP BY. In the other words, we consume bandwidth of the PCIe bus to move junk data, however, we cannot determine whether rows are necessary or not prior to the evaluation by SQL workloads on CPU. So, it is not avoidable restriction in usual implementation. GPU Direct SQL Execution changes the flow to read blocks from the storage sequentially. It directly loads data blocks to GPU using peer-to-peer DMA over PCIe bus, then runs SQL workloads on GPU device to reduce number of rows to be processed by CPU. In other words, it utilizes GPU as a pre-processor of SQL which locates in the middle of the storage and CPU/RAM for reduction of CPU's load, then tries to accelerate I/O processing in the results. This feature internally uses the NVIDIA GPUDirect Storage module ( nvidia-fs ) to coordinate P2P data transfer from NVME storage to GPU device memory. So, this feature requires this Linux kernel module, in addition to PG-Strom as an extension of PostgreSQL. In addition, an NVME-compatible SSD is required as a storage device, but other storage systems supported by NVIDIA GPUDirect Storage (such as remote disks connected via NVME-oF or NFS-over-RDMA) can also be used. However, it cannot be said that these have been sufficiently tested, so it is best to perform separate operation testing before use. System Setup Driver Installation The previous version of PG-Strom required its original Linux kernel module developed by HeteroDB for GPU-Direct SQL support, however, the version 3.0 revised the software design to use GPUDirect Storage provided by NVIDIA, as a part of CUDA Toolkit. The Linux kernel module for GPUDirect Storage ( nvidia-fs ) is integrated into the CUDA Toolkit installation process and requires no additional configuration if you have set up your system as described in the Installation chapter of this manual. You can check whether the required Linux kernel drivers are installed using the modinfo command or lsmod command. $ modinfo nvidia-fs filename: /lib/modules/5.14.0-427.18.1.el9_4.x86_64/extra/nvidia-fs.ko.xz description: NVIDIA GPUDirect Storage license: GPL v2 version: 2.20.5 rhelversion: 9.4 srcversion: 096A726CAEC0A059E24049E depends: retpoline: Y name: nvidia_fs vermagic: 5.14.0-427.18.1.el9_4.x86_64 SMP preempt mod_unload modversions sig_id: PKCS#7 signer: DKMS module signing key sig_key: 18:B4:AE:27:B8:7D:74:4F:C2:27:68:2A:EB:E0:6A:F0:84:B2:94:EE sig_hashalgo: sha512 : : $ lsmod | grep nvidia nvidia_fs 323584 32 nvidia_uvm 6877184 4 nvidia 8822784 43 nvidia_uvm,nvidia_fs drm 741376 2 drm_kms_helper,nvidia Designing Tablespace GPU Direct SQL Execution shall be invoked in the following case. The target table to be scanned locates on the partition being consist of NVMe-SSD. /dev/nvmeXXXX block device, or md-raid0 volume which consists of NVMe-SSDs only. Or, the table to be scanned is located in a directory explicitly specified by pg_strom.manual_optimal_gpus . This setting is required when using remote storage connected via a high-speed network such as NFS-over-RDMA. The target table size is larger than pg_strom.gpudirect_threshold . You can adjust this configuration. Its default is physical RAM size of the system plus 1/3 of shared_buffers configuration. Note Striping reads from multiple NVME-SSD volumes using md-raid0 or reading from remote disks over high-speed network requires the enterprise subscription provided by HeteroDB,Inc. In order to deploy the tables on the partition consists of NVME-SSD, you can use the tablespace function of PostgreSQL to specify particular tables or databases to place them on NVME-SSD volume, in addition to construction of the entire database cluster on the NVME-SSD volume. For example, you can create a new tablespace below, if NVME-SSD is mounted at /opt/nvme . CREATE TABLESPACE my_nvme LOCATION '/opt/nvme'; In order to create a new table on the tablespace, specify the TABLESPACE option at the CREATE TABLE command below. CREATE TABLE my_table (...) TABLESPACE my_nvme; Or, use ALTER DATABASE command as follows, to change the default tablespace of the database. Note that tablespace of the existing tables are not changed in thie case. ALTER DATABASE my_database SET TABLESPACE my_nvme; Operations Distance between GPU and NVME On selection of server hardware and installation of GPU and NVME-SSD, hardware configuration needs to pay attention to the distance between devices, to pull out maximum performance of the device. The process of reading from an NVME-SSD (or a high-speed NIC used for remote reading) and loading directly to a GPU is synonymous with the process of transferring data between two different PCI-E devices. If you think of the PCI-E bus as a kind of tree-shaped network, it is intuitive to understand that the device with a closer network distance and wider bandwidth is more suitable for data transfer. The photo below shows the motherboard of an HPC server, with eight PCI-E x16 slots connected to their counterparts via a PCI-E switch. The slot on the left side of the photo is connected to CPU1, and the slot on the right side is connected to CPU2. In general, a dedicated PCI-E switch is faster than a PCI-E controller built into the CPU, so for example, when scanning a table built on SSD-2 using GPU Direct SQL, the optimal GPU selection would be GPU-2. It is also possible to use GPU-1, which is connected under the same CPU, but it may be better to avoid using GPU-3 and GPU-4, which cross NUMA and involve CPU-to-CPU communication. PG-Strom calculate logical distances on any pairs of GPU and NVME-SSD using PCIe bus topology information of the system on startup time. It is displayed at the start up log. Each NVME-SSD determines the preferable GPU based on the distance, for example, GPU1 shall be used on scan of the /dev/nvme2 in the default. $ pg_ctl restart : LOG: PG-Strom version 6.0.1 built for PostgreSQL 16 (githash: 1fe955f845063236725631c83434b00f68a8d4cf) LOG: PG-Strom binary built for CUDA 12.6 (CUDA runtime 12.4) LOG: PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: PG-Strom: GPU1 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: [0000:41:00:0] GPU1 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78979c134606) LOG: [0000:01:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-cca38cf1-ddcc-6230-57fe-d42ad0dc3315) LOG: [0000:c3:00:0] nvme2 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c2:00:0] nvme1 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:82:00:0] nvme4 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c6:00:0] nvme3 (Corsair MP600 CORE) --> GPU0,GPU1 [dist=130] : For small systems such as single-socket servers without PCI-E switches, the automatic configuration is fine. However, if the GPU and NVME-SSD are connected to different CPUs, or if you connect to remote storage (NVME-oF or NFS-over-RDMA, etc.) via a high-speed network, the location of the storage on the PCI-E bus cannot be detected automatically, so you must manually configure their correspondence. For example, if you want to assign gpu2 and gpu3 to nvme3 , and mount remote storage to /mnt/nfs and assign it to gpu1 , write the following settings in postgresql.conf . Note that this manual configuration takes precedence over the automatic configuration. pg_strom.manual_optimal_gpus = 'nvme3=gpu2:gpu3,/mnt/nfs=gpu1' Controls using GUC parameters There are four GPU parameters related to GPU Direct SQL Execution. pg_strom.gpudirect_enabled This parameter turn on/off the functionality of GPU-Direct SQL. Default is on . pg_strom.gpudirect_driver You can specify the driver to be used for GPU Direct Execution. If the heterodb-extra module is installed, the default value will be cufile , nvme_strom , or vfs , which is an emulation of GPU Direct SQL by CPU, depending on the system state. If the module is not installed, only vfs will be used. nvme_strom is a proprietary Linux kernel module that was previously used to use GPU Direct SQL in RHEL7/8 environments, and is now deprecated. pg_strom.gpudirect_threshold Specifies the minimum table size for which GPU -Direct SQL should be used. If the physical layout of the table satisfies the prerequisites for GPU Direct SQL and the table size is larger than the value of this parameter, PG-Strom will select GPU Direct SQL Execution. The default value of this parameter is 2GB . In other words, for obviously small tables, it prioritizes reading from PostgreSQL buffers rather than GPU Direct SQL. This is based on the assumption that even if GPU Direct SQL Execution has an advantage for one read, it is not necessarily advantageous for tables that can be processed on memory, considering the use of disk cache from the second time onwards. pg_strom.gpu_selection_policy When using GPU Direct SQL, the GPU selected from the location of the target table will be used (unless manually configured), but specify the selection policy. There are three policies that can be specified: optimal (default) ... Use the nearest GPU numa ... Use GPUs that belong to the same NUMA node system ... Use all GPUs installed in the system Details will be explained in the next section. GPU Selection Policy The following log was taken when PostgreSQL (PG-Strom) was started, and shows the GPUs and NVME-SSDs recognized by the system. In the line for each NVME-SSD, the name of the device is output along with a list of the nearest GPUs and their physical distance. This server is relatively small, and all NVME-SSDs are connected to GPU0 and GPU1 at equal distances, but on larger servers, the distance between each GPU and the NVME-SSD may vary. In this section, we explain the GPU selection policy based on the distance between the GPU and the NVME-SSD. $ pg_ctl restart : LOG: PG-Strom version 6.0.1 built for PostgreSQL 16 (githash: 1fe955f845063236725631c83434b00f68a8d4cf) LOG: PG-Strom binary built for CUDA 12.6 (CUDA runtime 12.4) LOG: PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: PG-Strom: GPU1 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: [0000:41:00:0] GPU1 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78979c134606) LOG: [0000:01:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-cca38cf1-ddcc-6230-57fe-d42ad0dc3315) LOG: [0000:c3:00:0] nvme2 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c2:00:0] nvme1 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:82:00:0] nvme4 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c6:00:0] nvme3 (Corsair MP600 CORE) --> GPU0,GPU1 [dist=130] : The following figure is a block diagram of a large-scale GPU server with dual sockets and a built-in PCI-E switch. For example, when reading data from SSD3 and loading it to a GPU somewhere, the GPU that is \"closest\" in terms of distance between PCI-E devices is GPU1. In this case, if you specify the configuration parameter pg_strom.gpu_selection_policy=optimal , only GPU1 will be used for GPU-Direct SQL, and other GPUs will not be involved in scanning tables located on SSD3. However, this can sometimes lead to problems. Let's consider the case where SSD0, SSD1, SSD4 and SSD5 are configured as a logical volume using md-raid, and a table is built on top of the RAID volume. The nearest GPU for SSD0 and SSD1 is GPU0, and the nearest GPU for SSD4 and SSD5 is GPU2. If the GPU selection policy is optimal , there will be no optimal GPU for the md-raid volumn. In this case, since there is no target GPU, it will be decided not to use GPU-Direct SQL. To change this, set pg_strom.gpu_selection_policy=numa . This policy selects GPUs belonging to the same NUMA node as targets for GPU-Direct SQL. When the GPU selection policy is numa , the schedulable GPUs for all SSDs (SSD0, SSD1, SSD4, SSD5) included in the md-raid partition are GPU0 to GPU3 in common. Therefore, reads from the md-raid partition are evenly distributed to these four GPUs. Because communication from the SSD to the GPU goes through the CPU's PCI-E controller, throughput may decrease slightly, but with recent CPUs that support PCI-E4.0 or later, there does not seem to be much difference in speed. Finally, we will explain pg_strom.gpu_selection_policy=system . This policy selects all GPUs as targets for scheduling in GPU-Direct SQL, regardless of the distance of the PCI-E device. For example, in an extreme case, if all GPUs from SSD0 to SSD15 are configured as a single md-raid partition, there will be no GPU that is commonly \"preferred\" for all SSDs. Therefore, a selection policy is provided that allows all GPUs to be scheduled in GPU-Direct SQL, taking into account the fact that some data communication between CPUs will occur. In summary, PG-Strom determines the GPU to be used for GPU Direct SQL as follows: Identify the file path of the table or Arrow file to be scanned. If the path (or the directory containing it) and the GPU combination are described in pg_strom.manual_optimal_gpus , use that GPU. Identify the NVME-SSD that stores the file. If the file is stored on an md-raid partition, all NVME-SSDs that make up the RAID are targeted. Derive a set of GPUs that can be scheduled for each NVME-SSD according to pg_strom.gpu_selection_policy . If there are multiple NVME-SSDs, the logical AND of these GPUs is the schedulable GPU. If there is no schedulable GPU, GPU-Direct SQL is not used. This includes the case where the table is not built on an NVME-SSD. Miscellaneous Ensure usage of GPU Direct SQL EXPLAIN command allows to ensure whether GPU Direct SQL Execution shall be used in the target query, or not. In the example below, a scan on the lineorder table by Custom Scan (GpuPreAgg) shows GPU-Direct with 2 GPUs <0,1> . In this case, GPU-Direct SQL shall be used to scan from the lineorder table with GPU0 and GPU1. ssbm=# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_category = 'MFGR#12' and s_region = 'AMERICA' group by d_year, p_brand1; QUERY PLAN --------------------------------------------------------------------------------------------------------------- HashAggregate (cost=13806407.68..13806495.18 rows=7000 width=46) Group Key: date1.d_year, part.p_brand1 -> Gather (cost=13805618.73..13806355.18 rows=7000 width=46) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=13804618.73..13804655.18 rows=7000 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (p_partkey = lo_partkey) [plan: 2500011000 -> 98584180 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (s_suppkey = lo_suppkey) [plan: 98584180 -> 19644550 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (d_datekey = lo_orderdate) [plan: 19644550 -> 19644550 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Custom Scan (GpuScan) on part (cost=100.00..12682.86 rows=32861 width=14) GPU Projection: p_brand1, p_partkey GPU Scan Quals: (p_category = 'MFGR#12'::bpchar) [plan: 2000000 -> 32861] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Custom Scan (GpuScan) on supplier (cost=100.00..78960.47 rows=830255 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'AMERICA'::bpchar) [plan: 9999718 -> 830255] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Seq Scan on date1 (cost=0.00..62.04 rows=1504 width=8) (26 rows) Attension for visibility map Right now, GPU routines of PG-Strom cannot run MVCC visibility checks per row, because only host code has a special data structure for visibility checks. It also leads a problem. We cannot know which row is visible, or invisible at the time when PG-Strom requires P2P DMA for NVME-SSD, because contents of the storage blocks are not yet loaded to CPU/RAM, and MVCC related attributes are written with individual records. PostgreSQL had similar problem when it supports IndexOnlyScan. To address the problem, PostgreSQL has an infrastructure of visibility map which is a bunch of flags to indicate whether any records in a particular data block are visible from all the transactions. If associated bit is set, we can know the associated block has no invisible records without reading the block itself. GPU Direct SQL Execution utilizes this infrastructure. It checks the visibility map first, then only \"all-visible\" blocks are required to read with P2P DMA. VACUUM constructs visibility map, so you can enforce PostgreSQL to construct visibility map by explicit launch of VACUUM command. VACUUM ANALYZE linerorder;","title":"GPUDirect SQL"},{"location":"ssd2gpu/#gpu-direct-sql","text":"","title":"GPU-Direct SQL"},{"location":"ssd2gpu/#overview","text":"For the fast execution of SQL workloads, it needs to provide processors rapid data stream from storage or memory, in addition to processor's execution efficiency. Processor will run idle if data stream would not be delivered. GPUDirect SQL Execution directly connects NVMe-SSD which enables high-speed I/O processing by direct attach to the PCIe bus and GPU device that is also attached on the same PCIe bus, and runs SQL workloads very high speed by supplying data stream close to the wired speed of the hardware. Usually, PostgreSQL data blocks on the storage shall be once loaded to CPU/RAM through the PCIe bus, then, PostgreSQL runs WHERE-clause for filtering or JOIN/GROUP BY according to the query execution plan. Due to the characteristics of analytic workloads, the amount of result data set is much smaller than the source data set. For example, it is not rare case to read billions rows but output just hundreds rows after the aggregation operations with GROUP BY. In the other words, we consume bandwidth of the PCIe bus to move junk data, however, we cannot determine whether rows are necessary or not prior to the evaluation by SQL workloads on CPU. So, it is not avoidable restriction in usual implementation. GPU Direct SQL Execution changes the flow to read blocks from the storage sequentially. It directly loads data blocks to GPU using peer-to-peer DMA over PCIe bus, then runs SQL workloads on GPU device to reduce number of rows to be processed by CPU. In other words, it utilizes GPU as a pre-processor of SQL which locates in the middle of the storage and CPU/RAM for reduction of CPU's load, then tries to accelerate I/O processing in the results. This feature internally uses the NVIDIA GPUDirect Storage module ( nvidia-fs ) to coordinate P2P data transfer from NVME storage to GPU device memory. So, this feature requires this Linux kernel module, in addition to PG-Strom as an extension of PostgreSQL. In addition, an NVME-compatible SSD is required as a storage device, but other storage systems supported by NVIDIA GPUDirect Storage (such as remote disks connected via NVME-oF or NFS-over-RDMA) can also be used. However, it cannot be said that these have been sufficiently tested, so it is best to perform separate operation testing before use.","title":"Overview"},{"location":"ssd2gpu/#system-setup","text":"","title":"System Setup"},{"location":"ssd2gpu/#driver-installation","text":"The previous version of PG-Strom required its original Linux kernel module developed by HeteroDB for GPU-Direct SQL support, however, the version 3.0 revised the software design to use GPUDirect Storage provided by NVIDIA, as a part of CUDA Toolkit. The Linux kernel module for GPUDirect Storage ( nvidia-fs ) is integrated into the CUDA Toolkit installation process and requires no additional configuration if you have set up your system as described in the Installation chapter of this manual. You can check whether the required Linux kernel drivers are installed using the modinfo command or lsmod command. $ modinfo nvidia-fs filename: /lib/modules/5.14.0-427.18.1.el9_4.x86_64/extra/nvidia-fs.ko.xz description: NVIDIA GPUDirect Storage license: GPL v2 version: 2.20.5 rhelversion: 9.4 srcversion: 096A726CAEC0A059E24049E depends: retpoline: Y name: nvidia_fs vermagic: 5.14.0-427.18.1.el9_4.x86_64 SMP preempt mod_unload modversions sig_id: PKCS#7 signer: DKMS module signing key sig_key: 18:B4:AE:27:B8:7D:74:4F:C2:27:68:2A:EB:E0:6A:F0:84:B2:94:EE sig_hashalgo: sha512 : : $ lsmod | grep nvidia nvidia_fs 323584 32 nvidia_uvm 6877184 4 nvidia 8822784 43 nvidia_uvm,nvidia_fs drm 741376 2 drm_kms_helper,nvidia","title":"Driver Installation"},{"location":"ssd2gpu/#designing-tablespace","text":"GPU Direct SQL Execution shall be invoked in the following case. The target table to be scanned locates on the partition being consist of NVMe-SSD. /dev/nvmeXXXX block device, or md-raid0 volume which consists of NVMe-SSDs only. Or, the table to be scanned is located in a directory explicitly specified by pg_strom.manual_optimal_gpus . This setting is required when using remote storage connected via a high-speed network such as NFS-over-RDMA. The target table size is larger than pg_strom.gpudirect_threshold . You can adjust this configuration. Its default is physical RAM size of the system plus 1/3 of shared_buffers configuration. Note Striping reads from multiple NVME-SSD volumes using md-raid0 or reading from remote disks over high-speed network requires the enterprise subscription provided by HeteroDB,Inc. In order to deploy the tables on the partition consists of NVME-SSD, you can use the tablespace function of PostgreSQL to specify particular tables or databases to place them on NVME-SSD volume, in addition to construction of the entire database cluster on the NVME-SSD volume. For example, you can create a new tablespace below, if NVME-SSD is mounted at /opt/nvme . CREATE TABLESPACE my_nvme LOCATION '/opt/nvme'; In order to create a new table on the tablespace, specify the TABLESPACE option at the CREATE TABLE command below. CREATE TABLE my_table (...) TABLESPACE my_nvme; Or, use ALTER DATABASE command as follows, to change the default tablespace of the database. Note that tablespace of the existing tables are not changed in thie case. ALTER DATABASE my_database SET TABLESPACE my_nvme;","title":"Designing Tablespace"},{"location":"ssd2gpu/#operations","text":"","title":"Operations"},{"location":"ssd2gpu/#distance-between-gpu-and-nvme","text":"On selection of server hardware and installation of GPU and NVME-SSD, hardware configuration needs to pay attention to the distance between devices, to pull out maximum performance of the device. The process of reading from an NVME-SSD (or a high-speed NIC used for remote reading) and loading directly to a GPU is synonymous with the process of transferring data between two different PCI-E devices. If you think of the PCI-E bus as a kind of tree-shaped network, it is intuitive to understand that the device with a closer network distance and wider bandwidth is more suitable for data transfer. The photo below shows the motherboard of an HPC server, with eight PCI-E x16 slots connected to their counterparts via a PCI-E switch. The slot on the left side of the photo is connected to CPU1, and the slot on the right side is connected to CPU2. In general, a dedicated PCI-E switch is faster than a PCI-E controller built into the CPU, so for example, when scanning a table built on SSD-2 using GPU Direct SQL, the optimal GPU selection would be GPU-2. It is also possible to use GPU-1, which is connected under the same CPU, but it may be better to avoid using GPU-3 and GPU-4, which cross NUMA and involve CPU-to-CPU communication. PG-Strom calculate logical distances on any pairs of GPU and NVME-SSD using PCIe bus topology information of the system on startup time. It is displayed at the start up log. Each NVME-SSD determines the preferable GPU based on the distance, for example, GPU1 shall be used on scan of the /dev/nvme2 in the default. $ pg_ctl restart : LOG: PG-Strom version 6.0.1 built for PostgreSQL 16 (githash: 1fe955f845063236725631c83434b00f68a8d4cf) LOG: PG-Strom binary built for CUDA 12.6 (CUDA runtime 12.4) LOG: PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: PG-Strom: GPU1 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: [0000:41:00:0] GPU1 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78979c134606) LOG: [0000:01:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-cca38cf1-ddcc-6230-57fe-d42ad0dc3315) LOG: [0000:c3:00:0] nvme2 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c2:00:0] nvme1 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:82:00:0] nvme4 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c6:00:0] nvme3 (Corsair MP600 CORE) --> GPU0,GPU1 [dist=130] : For small systems such as single-socket servers without PCI-E switches, the automatic configuration is fine. However, if the GPU and NVME-SSD are connected to different CPUs, or if you connect to remote storage (NVME-oF or NFS-over-RDMA, etc.) via a high-speed network, the location of the storage on the PCI-E bus cannot be detected automatically, so you must manually configure their correspondence. For example, if you want to assign gpu2 and gpu3 to nvme3 , and mount remote storage to /mnt/nfs and assign it to gpu1 , write the following settings in postgresql.conf . Note that this manual configuration takes precedence over the automatic configuration. pg_strom.manual_optimal_gpus = 'nvme3=gpu2:gpu3,/mnt/nfs=gpu1'","title":"Distance between GPU and NVME"},{"location":"ssd2gpu/#controls-using-guc-parameters","text":"There are four GPU parameters related to GPU Direct SQL Execution. pg_strom.gpudirect_enabled This parameter turn on/off the functionality of GPU-Direct SQL. Default is on . pg_strom.gpudirect_driver You can specify the driver to be used for GPU Direct Execution. If the heterodb-extra module is installed, the default value will be cufile , nvme_strom , or vfs , which is an emulation of GPU Direct SQL by CPU, depending on the system state. If the module is not installed, only vfs will be used. nvme_strom is a proprietary Linux kernel module that was previously used to use GPU Direct SQL in RHEL7/8 environments, and is now deprecated. pg_strom.gpudirect_threshold Specifies the minimum table size for which GPU -Direct SQL should be used. If the physical layout of the table satisfies the prerequisites for GPU Direct SQL and the table size is larger than the value of this parameter, PG-Strom will select GPU Direct SQL Execution. The default value of this parameter is 2GB . In other words, for obviously small tables, it prioritizes reading from PostgreSQL buffers rather than GPU Direct SQL. This is based on the assumption that even if GPU Direct SQL Execution has an advantage for one read, it is not necessarily advantageous for tables that can be processed on memory, considering the use of disk cache from the second time onwards. pg_strom.gpu_selection_policy When using GPU Direct SQL, the GPU selected from the location of the target table will be used (unless manually configured), but specify the selection policy. There are three policies that can be specified: optimal (default) ... Use the nearest GPU numa ... Use GPUs that belong to the same NUMA node system ... Use all GPUs installed in the system Details will be explained in the next section.","title":"Controls using GUC parameters"},{"location":"ssd2gpu/#gpu-selection-policy","text":"The following log was taken when PostgreSQL (PG-Strom) was started, and shows the GPUs and NVME-SSDs recognized by the system. In the line for each NVME-SSD, the name of the device is output along with a list of the nearest GPUs and their physical distance. This server is relatively small, and all NVME-SSDs are connected to GPU0 and GPU1 at equal distances, but on larger servers, the distance between each GPU and the NVME-SSD may vary. In this section, we explain the GPU selection policy based on the distance between the GPU and the NVME-SSD. $ pg_ctl restart : LOG: PG-Strom version 6.0.1 built for PostgreSQL 16 (githash: 1fe955f845063236725631c83434b00f68a8d4cf) LOG: PG-Strom binary built for CUDA 12.6 (CUDA runtime 12.4) LOG: PG-Strom: GPU0 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: PG-Strom: GPU1 NVIDIA A100-PCIE-40GB (108 SMs; 1410MHz, L2 40960kB), RAM 39.38GB (5120bits, 1.16GHz), PCI-E Bar1 64GB, CC 8.0 LOG: [0000:41:00:0] GPU1 (NVIDIA A100-PCIE-40GB; GPU-13943bfd-5b30-38f5-0473-78979c134606) LOG: [0000:01:00:0] GPU0 (NVIDIA A100-PCIE-40GB; GPU-cca38cf1-ddcc-6230-57fe-d42ad0dc3315) LOG: [0000:c3:00:0] nvme2 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c2:00:0] nvme1 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:82:00:0] nvme4 (INTEL SSDPF2KX038TZ) --> GPU0,GPU1 [dist=130] LOG: [0000:c6:00:0] nvme3 (Corsair MP600 CORE) --> GPU0,GPU1 [dist=130] : The following figure is a block diagram of a large-scale GPU server with dual sockets and a built-in PCI-E switch. For example, when reading data from SSD3 and loading it to a GPU somewhere, the GPU that is \"closest\" in terms of distance between PCI-E devices is GPU1. In this case, if you specify the configuration parameter pg_strom.gpu_selection_policy=optimal , only GPU1 will be used for GPU-Direct SQL, and other GPUs will not be involved in scanning tables located on SSD3. However, this can sometimes lead to problems. Let's consider the case where SSD0, SSD1, SSD4 and SSD5 are configured as a logical volume using md-raid, and a table is built on top of the RAID volume. The nearest GPU for SSD0 and SSD1 is GPU0, and the nearest GPU for SSD4 and SSD5 is GPU2. If the GPU selection policy is optimal , there will be no optimal GPU for the md-raid volumn. In this case, since there is no target GPU, it will be decided not to use GPU-Direct SQL. To change this, set pg_strom.gpu_selection_policy=numa . This policy selects GPUs belonging to the same NUMA node as targets for GPU-Direct SQL. When the GPU selection policy is numa , the schedulable GPUs for all SSDs (SSD0, SSD1, SSD4, SSD5) included in the md-raid partition are GPU0 to GPU3 in common. Therefore, reads from the md-raid partition are evenly distributed to these four GPUs. Because communication from the SSD to the GPU goes through the CPU's PCI-E controller, throughput may decrease slightly, but with recent CPUs that support PCI-E4.0 or later, there does not seem to be much difference in speed. Finally, we will explain pg_strom.gpu_selection_policy=system . This policy selects all GPUs as targets for scheduling in GPU-Direct SQL, regardless of the distance of the PCI-E device. For example, in an extreme case, if all GPUs from SSD0 to SSD15 are configured as a single md-raid partition, there will be no GPU that is commonly \"preferred\" for all SSDs. Therefore, a selection policy is provided that allows all GPUs to be scheduled in GPU-Direct SQL, taking into account the fact that some data communication between CPUs will occur. In summary, PG-Strom determines the GPU to be used for GPU Direct SQL as follows: Identify the file path of the table or Arrow file to be scanned. If the path (or the directory containing it) and the GPU combination are described in pg_strom.manual_optimal_gpus , use that GPU. Identify the NVME-SSD that stores the file. If the file is stored on an md-raid partition, all NVME-SSDs that make up the RAID are targeted. Derive a set of GPUs that can be scheduled for each NVME-SSD according to pg_strom.gpu_selection_policy . If there are multiple NVME-SSDs, the logical AND of these GPUs is the schedulable GPU. If there is no schedulable GPU, GPU-Direct SQL is not used. This includes the case where the table is not built on an NVME-SSD.","title":"GPU Selection Policy"},{"location":"ssd2gpu/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"ssd2gpu/#ensure-usage-of-gpu-direct-sql","text":"EXPLAIN command allows to ensure whether GPU Direct SQL Execution shall be used in the target query, or not. In the example below, a scan on the lineorder table by Custom Scan (GpuPreAgg) shows GPU-Direct with 2 GPUs <0,1> . In this case, GPU-Direct SQL shall be used to scan from the lineorder table with GPU0 and GPU1. ssbm=# explain select sum(lo_revenue), d_year, p_brand1 from lineorder, date1, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_category = 'MFGR#12' and s_region = 'AMERICA' group by d_year, p_brand1; QUERY PLAN --------------------------------------------------------------------------------------------------------------- HashAggregate (cost=13806407.68..13806495.18 rows=7000 width=46) Group Key: date1.d_year, part.p_brand1 -> Gather (cost=13805618.73..13806355.18 rows=7000 width=46) Workers Planned: 2 -> Parallel Custom Scan (GpuPreAgg) on lineorder (cost=13804618.73..13804655.18 rows=7000 width=46) GPU Projection: pgstrom.psum(lo_revenue), d_year, p_brand1 GPU Join Quals [1]: (p_partkey = lo_partkey) [plan: 2500011000 -> 98584180 ] GPU Outer Hash [1]: lo_partkey GPU Inner Hash [1]: p_partkey GPU Join Quals [2]: (s_suppkey = lo_suppkey) [plan: 98584180 -> 19644550 ] GPU Outer Hash [2]: lo_suppkey GPU Inner Hash [2]: s_suppkey GPU Join Quals [3]: (d_datekey = lo_orderdate) [plan: 19644550 -> 19644550 ] GPU Outer Hash [3]: lo_orderdate GPU Inner Hash [3]: d_datekey GPU Group Key: d_year, p_brand1 Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Custom Scan (GpuScan) on part (cost=100.00..12682.86 rows=32861 width=14) GPU Projection: p_brand1, p_partkey GPU Scan Quals: (p_category = 'MFGR#12'::bpchar) [plan: 2000000 -> 32861] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Custom Scan (GpuScan) on supplier (cost=100.00..78960.47 rows=830255 width=6) GPU Projection: s_suppkey GPU Scan Quals: (s_region = 'AMERICA'::bpchar) [plan: 9999718 -> 830255] Scan-Engine: GPU-Direct with 2 GPUs <0,1> -> Parallel Seq Scan on date1 (cost=0.00..62.04 rows=1504 width=8) (26 rows)","title":"Ensure usage of GPU Direct SQL"},{"location":"ssd2gpu/#attension-for-visibility-map","text":"Right now, GPU routines of PG-Strom cannot run MVCC visibility checks per row, because only host code has a special data structure for visibility checks. It also leads a problem. We cannot know which row is visible, or invisible at the time when PG-Strom requires P2P DMA for NVME-SSD, because contents of the storage blocks are not yet loaded to CPU/RAM, and MVCC related attributes are written with individual records. PostgreSQL had similar problem when it supports IndexOnlyScan. To address the problem, PostgreSQL has an infrastructure of visibility map which is a bunch of flags to indicate whether any records in a particular data block are visible from all the transactions. If associated bit is set, we can know the associated block has no invisible records without reading the block itself. GPU Direct SQL Execution utilizes this infrastructure. It checks the visibility map first, then only \"all-visible\" blocks are required to read with P2P DMA. VACUUM constructs visibility map, so you can enforce PostgreSQL to construct visibility map by explicit launch of VACUUM command. VACUUM ANALYZE linerorder;","title":"Attension for visibility map"},{"location":"troubles/","text":"Trouble Shooting Identify the problem In case when a particular workloads produce problems, it is the first step to identify which stuff may cause the problem. Unfortunately, much smaller number of developer supports the PG-Strom development community than PostgreSQL developer's community, thus, due to the standpoint of software quality and history, it is a reasonable estimation to suspect PG-Strom first. The pg_strom.enabled parameter allows to turn on/off all the functionality of PG-Strom at once. The configuration below disables PG-Strom, thus identically performs with the standard PostgreSQL. # SET pg_strom.enabled = off; In addition, we provide parameters to disable particular execution plan like GpuScan, GpuJoin and GpuPreAgg. See references/GUC Parameters for more details. Collecting crash dump Crash dump is very helpful for analysis of serious problems which lead system crash for example. This session introduces the way to collect crash dump of the PostgreSQL and PG-Strom process (CPU side) and PG-Strom's GPU kernel, and show the back trace on the serious problems. Add configuration on PostgreSQL startup For generation of crash dump (CPU-side) on process crash, you need to change the resource limitation of the operating system for size of core file PostgreSQL server process can generate. For generation of crash dump (GPU-size) on errors of GPU kernel, PostgreSQL server process has CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable, and its value has 1 . You can put a configuration file at /etc/systemd/system/postgresql-<version>.service.d/ when PostgreSQL is kicked by systemd. In case of RPM installation, a configuration file pg_strom.conf is also installed on the directory, and contains the following initial configuration. [Service] LimitNOFILE=65536 LimitCORE=infinity #Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 In CUDA 9.1, it usually takes more than several minutes to generate crash dump of GPU kernel, and it entirely stops response of the PostgreSQL session which causes an error. So, we recommend to set CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable only if you investigate errors of GPU kernels which happen on a certain query. The default configuration on RPM installation comments out the line of CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable. PostgreSQL server process should have unlimited Max core file size configuration, after the next restart. You can check it as follows. # cat /proc/<PID of postmaster>/limits Limit Soft Limit Hard Limit Units : : : : Max core file size unlimited unlimited bytes : : : : Installation of debuginfo package # yum install postgresql10-debuginfo pg_strom-PG10-debuginfo : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: pg_strom-PG10-debuginfo x86_64 1.9-180301.el7 heterodb-debuginfo 766 k postgresql10-debuginfo x86_64 10.3-1PGDG.rhel7 pgdg10 9.7 M Transaction Summary ================================================================================ Install 2 Packages : Installed: pg_strom-PG10-debuginfo.x86_64 0:1.9-180301.el7 postgresql10-debuginfo.x86_64 0:10.3-1PGDG.rhel7 Complete! Checking the back-trace on CPU side The kernel parameter kernel.core_pattern and kernel.core_uses_pid determine the path where crash dump is written out. It is usually created on the current working directory of the process, check /var/lib/pgdata where the database cluster is deployed, if you start PostgreSQL server using systemd. Once core.<PID> file gets generated, you can check its back-trace to reach system crash using gdb . gdb speficies the core file by -c option, and the crashed program by -f option. # gdb -c /var/lib/pgdata/core.134680 -f /usr/pgsql-10/bin/postgres GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1 : (gdb) bt #0 0x00007fb942af3903 in __epoll_wait_nocancel () from /lib64/libc.so.6 #1 0x00000000006f71ae in WaitEventSetWaitBlock (nevents=1, occurred_events=0x7ffee51e1d70, cur_timeout=-1, set=0x2833298) at latch.c:1048 #2 WaitEventSetWait (set=0x2833298, timeout=timeout@entry-1, occurred_events=occurred_events@entry0x7ffee51e1d70, nevents=nevents@entry1, wait_event_info=wait_event_info@entry100663296) at latch.c:1000 #3 0x00000000006210fb in secure_read (port=0x2876120, ptr=0xcaa7e0 <PqRecvBuffer>, len=8192) at be-secure.c:166 #4 0x000000000062b6e8 in pq_recvbuf () at pqcomm.c:963 #5 0x000000000062c345 in pq_getbyte () at pqcomm.c:1006 #6 0x0000000000718682 in SocketBackend (inBuf=0x7ffee51e1ef0) at postgres.c:328 #7 ReadCommand (inBuf=0x7ffee51e1ef0) at postgres.c:501 #8 PostgresMain (argc=<optimized out>, argv=argv@entry0x287bb68, dbname=0x28333f8 \"postgres\", username=<optimized out>) at postgres.c:4030 #9 0x000000000047adbc in BackendRun (port=0x2876120) at postmaster.c:4405 #10 BackendStartup (port=0x2876120) at postmaster.c:4077 #11 ServerLoop () at postmaster.c:1755 #12 0x00000000006afb7f in PostmasterMain (argc=argc@entry3, argv=argv@entry0x2831280) at postmaster.c:1363 #13 0x000000000047bbef in main (argc=3, argv=0x2831280) at main.c:228 bt command of gdb displays the backtrace. In this case, I sent SIGSEGV signal to the PostgreSQL backend which is waiting for queries from the client for intentional crash, the process got crashed at __epoll_wait_nocancel invoked by WaitEventSetWait . Checking the backtrace on GPU Crash dump of GPU kernel is generated on the current working directory of PostgreSQL server process, unless you don't specify the path using CUDA_COREDUMP_FILE environment variable explicitly. Check /var/lib/pgdata where the database cluster is deployed, if systemd started PostgreSQL. Dump file will have the following naming convension. core_<timestamp>_<hostname>_<PID>.nvcudmp Note that the dump-file of GPU kernel contains no debug information like symbol information in the default configuration. It is nearly impossible to investigate the problem, so enable inclusion of debug information for the GPU programs generated by PG-Strom, as follows. Also note than we don't recommend to turn on the configuration for daily usage, because it makes query execution performan slow down. Turn on only when you investigate the troubles. nvme=# set pg_strom.debug_jit_compile_options = on; SET You can check crash dump of the GPU kernel using cuda-gdb command. # /usr/local/cuda/bin/cuda-gdb NVIDIA (R) CUDA Debugger 9.1 release Portions Copyright (C) 2007-2017 NVIDIA Corporation : For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\". (cuda-gdb) Run cuda-gdb command, then load the crash dump file above using target command on the prompt. (cuda-gdb) target cudacore /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp Opening GPU coredump: /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp [New Thread 216240] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x7ff4dc82f930 (cuda_gpujoin.h:1159) [Current focus set to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 1159 while (khitem && khitem->hash != hash_value) You can check backtrace where the error happened on GPU kernel using bt command. (cuda-gdb) bt #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 #1 0x00007ff4dc9428f0 in gpujoin_main<<<(30,1,1),(256,1,1)>>> (kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, kds_dst=0x7fe9e8800030, kparams_gpreagg=0x0) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1347 Please check CUDA Toolkit Documentation - CUDA-GDB for more detailed usage of cuda-gdb command.","title":"Trouble Shooting"},{"location":"troubles/#trouble-shooting","text":"","title":"Trouble Shooting"},{"location":"troubles/#identify-the-problem","text":"In case when a particular workloads produce problems, it is the first step to identify which stuff may cause the problem. Unfortunately, much smaller number of developer supports the PG-Strom development community than PostgreSQL developer's community, thus, due to the standpoint of software quality and history, it is a reasonable estimation to suspect PG-Strom first. The pg_strom.enabled parameter allows to turn on/off all the functionality of PG-Strom at once. The configuration below disables PG-Strom, thus identically performs with the standard PostgreSQL. # SET pg_strom.enabled = off; In addition, we provide parameters to disable particular execution plan like GpuScan, GpuJoin and GpuPreAgg. See references/GUC Parameters for more details.","title":"Identify the problem"},{"location":"troubles/#collecting-crash-dump","text":"Crash dump is very helpful for analysis of serious problems which lead system crash for example. This session introduces the way to collect crash dump of the PostgreSQL and PG-Strom process (CPU side) and PG-Strom's GPU kernel, and show the back trace on the serious problems.","title":"Collecting crash dump"},{"location":"troubles/#add-configuration-on-postgresql-startup","text":"For generation of crash dump (CPU-side) on process crash, you need to change the resource limitation of the operating system for size of core file PostgreSQL server process can generate. For generation of crash dump (GPU-size) on errors of GPU kernel, PostgreSQL server process has CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable, and its value has 1 . You can put a configuration file at /etc/systemd/system/postgresql-<version>.service.d/ when PostgreSQL is kicked by systemd. In case of RPM installation, a configuration file pg_strom.conf is also installed on the directory, and contains the following initial configuration. [Service] LimitNOFILE=65536 LimitCORE=infinity #Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 In CUDA 9.1, it usually takes more than several minutes to generate crash dump of GPU kernel, and it entirely stops response of the PostgreSQL session which causes an error. So, we recommend to set CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable only if you investigate errors of GPU kernels which happen on a certain query. The default configuration on RPM installation comments out the line of CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable. PostgreSQL server process should have unlimited Max core file size configuration, after the next restart. You can check it as follows. # cat /proc/<PID of postmaster>/limits Limit Soft Limit Hard Limit Units : : : : Max core file size unlimited unlimited bytes : : : :","title":"Add configuration on PostgreSQL startup"},{"location":"troubles/#installation-of-debuginfo-package","text":"# yum install postgresql10-debuginfo pg_strom-PG10-debuginfo : ================================================================================ Package Arch Version Repository Size ================================================================================ Installing: pg_strom-PG10-debuginfo x86_64 1.9-180301.el7 heterodb-debuginfo 766 k postgresql10-debuginfo x86_64 10.3-1PGDG.rhel7 pgdg10 9.7 M Transaction Summary ================================================================================ Install 2 Packages : Installed: pg_strom-PG10-debuginfo.x86_64 0:1.9-180301.el7 postgresql10-debuginfo.x86_64 0:10.3-1PGDG.rhel7 Complete!","title":"Installation of debuginfo package"},{"location":"troubles/#checking-the-back-trace-on-cpu-side","text":"The kernel parameter kernel.core_pattern and kernel.core_uses_pid determine the path where crash dump is written out. It is usually created on the current working directory of the process, check /var/lib/pgdata where the database cluster is deployed, if you start PostgreSQL server using systemd. Once core.<PID> file gets generated, you can check its back-trace to reach system crash using gdb . gdb speficies the core file by -c option, and the crashed program by -f option. # gdb -c /var/lib/pgdata/core.134680 -f /usr/pgsql-10/bin/postgres GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1 : (gdb) bt #0 0x00007fb942af3903 in __epoll_wait_nocancel () from /lib64/libc.so.6 #1 0x00000000006f71ae in WaitEventSetWaitBlock (nevents=1, occurred_events=0x7ffee51e1d70, cur_timeout=-1, set=0x2833298) at latch.c:1048 #2 WaitEventSetWait (set=0x2833298, timeout=timeout@entry-1, occurred_events=occurred_events@entry0x7ffee51e1d70, nevents=nevents@entry1, wait_event_info=wait_event_info@entry100663296) at latch.c:1000 #3 0x00000000006210fb in secure_read (port=0x2876120, ptr=0xcaa7e0 <PqRecvBuffer>, len=8192) at be-secure.c:166 #4 0x000000000062b6e8 in pq_recvbuf () at pqcomm.c:963 #5 0x000000000062c345 in pq_getbyte () at pqcomm.c:1006 #6 0x0000000000718682 in SocketBackend (inBuf=0x7ffee51e1ef0) at postgres.c:328 #7 ReadCommand (inBuf=0x7ffee51e1ef0) at postgres.c:501 #8 PostgresMain (argc=<optimized out>, argv=argv@entry0x287bb68, dbname=0x28333f8 \"postgres\", username=<optimized out>) at postgres.c:4030 #9 0x000000000047adbc in BackendRun (port=0x2876120) at postmaster.c:4405 #10 BackendStartup (port=0x2876120) at postmaster.c:4077 #11 ServerLoop () at postmaster.c:1755 #12 0x00000000006afb7f in PostmasterMain (argc=argc@entry3, argv=argv@entry0x2831280) at postmaster.c:1363 #13 0x000000000047bbef in main (argc=3, argv=0x2831280) at main.c:228 bt command of gdb displays the backtrace. In this case, I sent SIGSEGV signal to the PostgreSQL backend which is waiting for queries from the client for intentional crash, the process got crashed at __epoll_wait_nocancel invoked by WaitEventSetWait .","title":"Checking the back-trace on CPU side"},{"location":"troubles/#checking-the-backtrace-on-gpu","text":"Crash dump of GPU kernel is generated on the current working directory of PostgreSQL server process, unless you don't specify the path using CUDA_COREDUMP_FILE environment variable explicitly. Check /var/lib/pgdata where the database cluster is deployed, if systemd started PostgreSQL. Dump file will have the following naming convension. core_<timestamp>_<hostname>_<PID>.nvcudmp Note that the dump-file of GPU kernel contains no debug information like symbol information in the default configuration. It is nearly impossible to investigate the problem, so enable inclusion of debug information for the GPU programs generated by PG-Strom, as follows. Also note than we don't recommend to turn on the configuration for daily usage, because it makes query execution performan slow down. Turn on only when you investigate the troubles. nvme=# set pg_strom.debug_jit_compile_options = on; SET You can check crash dump of the GPU kernel using cuda-gdb command. # /usr/local/cuda/bin/cuda-gdb NVIDIA (R) CUDA Debugger 9.1 release Portions Copyright (C) 2007-2017 NVIDIA Corporation : For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\". (cuda-gdb) Run cuda-gdb command, then load the crash dump file above using target command on the prompt. (cuda-gdb) target cudacore /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp Opening GPU coredump: /var/lib/pgdata/core_1521131828_magro.heterodb.com_216238.nvcudmp [New Thread 216240] CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x7ff4dc82f930 (cuda_gpujoin.h:1159) [Current focus set to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 1159 while (khitem && khitem->hash != hash_value) You can check backtrace where the error happened on GPU kernel using bt command. (cuda-gdb) bt #0 0x00007ff4dc82f938 in _INTERNAL_8_pg_strom_0124cb94::gpujoin_exec_hashjoin (kcxt=0x7ff4f7fffbf8, kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, depth=3, rd_stack=0x7fe9f4806118, wr_stack=0x7fe9f480c118, l_state=0x7ff4f7fffc48, matched=0x7ff4f7fffc7c \"\") at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1159 #1 0x00007ff4dc9428f0 in gpujoin_main<<<(30,1,1),(256,1,1)>>> (kgjoin=0x7fe9f4800078, kmrels=0x7fe9f8800000, kds_src=0x7fe9f0800030, kds_dst=0x7fe9e8800030, kparams_gpreagg=0x0) at /usr/pgsql-10/share/extension/cuda_gpujoin.h:1347 Please check CUDA Toolkit Documentation - CUDA-GDB for more detailed usage of cuda-gdb command.","title":"Checking the backtrace on GPU"}]}