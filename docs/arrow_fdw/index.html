<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="PG-Strom Development Team" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Apache Arrow - PG-Strom Manual</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet" />
        <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet" />
        <link href="../custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Apache Arrow";
        var mkdocs_page_input_path = "arrow_fdw.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> PG-Strom Manual
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
  [<a href="../ja/arrow_fdw/" style="color: #cccccc">Japanese</a> | <strong>English</strong>]
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../install/">Install</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Tutorial</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../operations/">Basic Operations</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../brin/">BRIN Index</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../partition/">Partitioning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../postgis/">PostGIS</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../gpusort/">GPU-Sort</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../troubles/">Trouble Shooting</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Features</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../ssd2gpu/">GPUDirect SQL</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Apache Arrow</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-apache-arrow">What is Apache Arrow?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#operations">Operations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#creation-of-foreign-tables">Creation of foreign tables</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#foreign-table-options">Foreign table options</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#foreign-table-options_1">Foreign Table Options</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#foreign-column-options">Foreign Column Options</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-type-mapping">Data type mapping</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#how-to-read-explain">How to read EXPLAIN</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#arrow_fdw-virtual-column">Arrow_Fdw Virtual Column</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#how-to-make-arrow-files">How to make Arrow files</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#using-pyarrowpandas">Using PyArrow+Pandas</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-pg2arrow">Using Pg2Arrow</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#advanced-usage">Advanced Usage</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#ssdtogpu-direct-sql">SSDtoGPU Direct SQL</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#partition-configuration">Partition configuration</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../gpucache/">GPU Cache</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../pinned_buffer/">Pinned Inner Buffer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../fluentd/">connect with Fluentd</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">References</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_types/">Data Types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_devfuncs/">Functions and Operators</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_sqlfuncs/">SQL Objects</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_params/">GUC Parameters</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Release Note</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v6.0/">PG-Strom v6.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.2/">PG-Strom v5.2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.1/">PG-Strom v5.1</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.0/">PG-Strom v5.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v3.0/">PG-Strom v3.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.3/">PG-Strom v2.3</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.2/">PG-Strom v2.2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.0/">PG-Strom v2.0</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">PG-Strom Manual</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Advanced Features</li>
      <li class="breadcrumb-item active">Apache Arrow</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="apache-arrow-columnar-store">Apache Arrow (Columnar Store)</h1>
<h2 id="overview">Overview</h2>
<p>PostgreSQL tables internally consist of 8KB blocks<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>, and block contains tuples which is a data structure of all the attributes and metadata per row. It collocates date of a row closely, so it works effectively for INSERT/UPDATE-major workloads, but not suitable for summarizing or analytics of mass-data.</p>
<p>It is not usual to reference all the columns in a table on mass-data processing, and we tend to reference a part of columns in most cases. In this case, the storage I/O bandwidth consumed by unreferenced columns are waste, however, we have no easy way to fetch only particular columns referenced from the row-oriented data structure.</p>
<p>In case of column oriented data structure, in an opposite manner, it has extreme disadvantage on INSERT/UPDATE-major workloads, however, it can pull out maximum performance of storage I/O on mass-data processing workloads because it can loads only referenced columns. From the standpoint of processor efficiency also, column-oriented data structure looks like a flat array that pulls out maximum bandwidth of memory subsystem for GPU, by special memory access pattern called Coalesced Memory Access.</p>
<p><img alt="Row/Column data structure" src="../img/row_column_structure.png" /></p>
<h2 id="what-is-apache-arrow">What is Apache Arrow?</h2>
<p>Apache Arrow is a data format of structured data to save in columnar-form and to exchange other applications. Some applications for big-data processing support the format, and it is easy for self-developed applications to use Apache Arrow format since they provides libraries for major programming languages like C,C++ or Python.</p>
<p><img alt="Row/Column data structure" src="../img/arrow_shared_memory.png" /></p>
<p>Apache Arrow format file internally contains Schema portion to define data structure, and one or more RecordBatch to save columnar-data based on the schema definition. For data types, it supports integers, strint (variable-length), date/time types and so on. Indivisual columnar data has its internal representation according to the data types.</p>
<p>Data representation in Apache Arrow is not identical with the representation in PostgreSQL. For example, epoch of timestamp in Arrow is <code>1970-01-01</code> and it supports multiple precision. In contrast, epoch of timestamp in PostgreSQL is <code>2001-01-01</code> and it has microseconds accuracy.</p>
<p>Arrow_Fdw allows to read Apache Arrow files on PostgreSQL using foreign table mechanism. If an Arrow file contains 8 of record batches that has million items for each column data, for example, we can access 8 million rows on the Arrow files through the foreign table.</p>
<h2 id="operations">Operations</h2>
<h3 id="creation-of-foreign-tables">Creation of foreign tables</h3>
<p>Usually it takes the 3 steps below to create a foreign table.</p>
<ul>
<li>Define a foreign-data-wrapper using <code>CREATE FOREIGN DATA WRAPPER</code> command</li>
<li>Define a foreign server using <code>CREATE SERVER</code> command</li>
<li>Define a foreign table using <code>CREATE FOREIGN TABLE</code> command</li>
</ul>
<p>The first 2 steps above are included in the <code>CREATE EXTENSION pg_strom</code> command. All you need to run individually is <code>CREATE FOREIGN TABLE</code> command last.</p>
<pre><code>CREATE FOREIGN TABLE flogdata (
    ts        timestamp,
    sensor_id int,
    signal1   smallint,
    signal2   smallint,
    signal3   smallint,
    signal4   smallint,
) SERVER arrow_fdw
  OPTIONS (file '/path/to/logdata.arrow');
</code></pre>
<p>Data type of columns specified by the <code>CREATE FOREIGN TABLE</code> command must be matched to schema definition of the Arrow files to be mapped.</p>
<p>Arrow_Fdw also supports a useful manner using <code>IMPORT FOREIGN SCHEMA</code> statement. It automatically generates a foreign table definition using schema definition of the Arrow files. It specifies the foreign table name, schema name to import, and path name of the Arrow files using OPTION-clause. Schema definition of Arrow files contains data types and optional column name for each column. It declares a new foreign table using these information.</p>
<pre><code>IMPORT FOREIGN SCHEMA flogdata
  FROM SERVER arrow_fdw
  INTO public
OPTIONS (file '/path/to/logdata.arrow');
</code></pre>
<h3 id="foreign-table-options">Foreign table options</h3>
<p>Arrow_Fdw supports the options below.</p>
<h4 id="foreign-table-options_1">Foreign Table Options</h4>
<dl>
<dt><code>file=PATHNAME</code></dt>
<dd>It maps an Arrow file specified on the foreign table.</dd>
<dt><code>files=PATHNAME1[,PATHNAME2...]</code></dt>
<dd>It maps multiple Arrow files specified by comma (,) separated files list on the foreign table.</dd>
<dt><code>dir=DIRNAME</code></dt>
<dd>It maps all the Arrow files in the directory specified on the foreign table.</dd>
<dt><code>suffix=SUFFIX</code></dt>
<dd><code>When</code>dir<code>option is given, it maps only files with the specified suffix, like</code>.arrow` for example.</dd>
<dt><code>parallel_workers=N_WORKERS</code></dt>
<dd>It tells the number of workers that should be used to assist a parallel scan of this foreign table; equivalent to <code>parallel_workers</code> storage parameter at normal tables.</dd>
<dt><code>pattern=PATTERN</code></dt>
<dd>Maps only files specified by the <code>file</code>, <code>files</code>, or <code>dir</code> option that match the <code>PATTERN</code>, including wildcards, to the foreign table.</dd>
<dd>The following wildcards can be used:</dd>
<dd>
<ul>
<li><code>?</code> ... matches any 1 character.</li>
</ul>
</dd>
<dd>
<ul>
<li><code>*</code> ... matches any string of 0 or more characters.</li>
</ul>
</dd>
<dd>
<ul>
<li><code>${KEY}</code> ... matches any string of 0 or more characters.</li>
</ul>
</dd>
<dd>
<ul>
<li><code>@{KEY}</code> ... matches any numeric string of 0 or more characters.</li>
</ul>
</dd>
<dd></dd>
<dd>An interesting use of this option is to refer to a portion of a file name matched by the wildcard <code>${KEY}</code> or <code>@{KEY}</code> as a virtual column. For more information, see the '''Arrow_Fdw virtual column''' section below.</dd>
</dl>
<h4 id="foreign-column-options">Foreign Column Options</h4>
<dl>
<dt><code>field=FIELD</code></dt>
<dd>It specifies the field name of the Arrow file to map to that column.</dd>
<dd>In the default, Arrow_Fdw maps the first occurrence of a field that has the same column name as this foreign table's column name.</dd>
<dt><code>virtual=KEY</code></dt>
<dd>It configures the column is a virtual column. <code>KEY</code> specifies the wildcard key name in the pattern specified by the <code>pattern</code> option of the foreign table option.</dd>
<dd>A virtual column allows to refer to the part of the file name pattern that matches <code>KEY</code> in a query.</dd>
<dt><code>virtual_metadata=KEY</code></dt>
<dd>It specifies that the column is a virtual column. <code>KEY</code> specifies a KEY-VALUE pair embedded in the CustomMetadata field of the Arrow file. If the specified KEY-VALUE pair is not found, the column returns a NULL value.</dd>
<dd>There are two types of CustomMetadata in Arrow files: embedded in the schema (corresponding to a PostgreSQL table) and embedded in the field (corresponding to a PostgreSQL column).</dd>
<dd>For example, you can reference CustomMetadata embedded in a field by writing the field name separated by the <code>.</code> character before the KEY value, such as <code>lo_orderdate.max_values</code>. If there is no field name, it will be treated as a KEY-VALUE pair embedded in the schema.</dd>
<dt><code>virtual_metadata_split=KEY</code></dt>
<dd>It specifies that the column is a virtual column. <code>KEY</code> specifies the KEY-VALUE pair embedded in the CustomMetadata field of the Arrow file. If the specified KEY-VALUE pair is not found, this column returns a NULL value.</dd>
<dd>The difference from <code>virtual_metadata</code> is that the values of the CustomMetadata field are separated by a delimiter(<code>,</code>) and applied to each Record Batch in order from the beginning. For example, if the specified CustomMetadata value is <code>Tokyo,Osaka,Kyoto,Yokohama</code>, the row read from RecordBatch-0 will display <code>'Tokyo'</code>, the row read from RecordBatch-1 will display <code>'Osaka'</code>, and the row read from RecordBatch-2 will display <code>'Osaka'</code> as the value of this virtual column.</dd>
</dl>
<h3 id="data-type-mapping">Data type mapping</h3>
<p>Arrow data types are mapped on PostgreSQL data types as follows.</p>
<dl>
<dt><code>Int</code></dt>
<dd>mapped to either of <code>int1</code>, <code>int2</code>, <code>int4</code> or <code>int8</code> according to the <code>bitWidth</code> attribute.</dd>
<dd><code>is_signed</code> attribute shall be ignored.</dd>
<dd><code>int1</code> is an enhanced data type by PG-Strom.</dd>
<dt><code>FloatingPoint</code></dt>
<dd>mapped to either of <code>float2</code>, <code>float4</code> or <code>float8</code> according to the <code>precision</code> attribute.</dd>
<dd><code>float2</code> is an enhanced data type by PG-Strom.</dd>
<dt><code>Utf8</code>, <code>LargeUtf8</code></dt>
<dd>mapped to <code>text</code> data type</dd>
<dt><code>Binary</code>, <code>LargeBinary</code></dt>
<dd>mapped to <code>bytea</code> data type</dd>
<dt><code>Decimal</code></dt>
<dd>mapped to <code>numeric</code> data type</dd>
<dt><code>Date</code></dt>
<dd>mapped to <code>date</code> data type; to be adjusted as if it has <code>unit=Day</code> precision.</dd>
<dt><code>Time</code></dt>
<dd>mapped to <code>time</code> data type; to be adjusted as if it has <code>unit=MicroSecond</code> precision.</dd>
<dt><code>Timestamp</code></dt>
<dd>mapped to <code>timestamp</code> data type; to be adjusted as if it has <code>unit=MicroSecond</code> precision.</dd>
<dt><code>Interval</code></dt>
<dd>mapped to <code>interval</code> data type.</dd>
<dt><code>List</code>, <code>LargeList</code></dt>
<dd>mapped to 1-dimensional array of the element data type.</dd>
<dt><code>Struct</code></dt>
<dd>mapped to compatible composite data type; that shall be defined preliminary.</dd>
<dt><code>FixedSizeBinary</code></dt>
<dd>mapped to <code>char(n)</code> data type according to the <code>byteWidth</code> attribute.</dd>
<dd>If <code>pg_type=TYPENAME</code> is configured, PG-Strom may assign the configured data type. Right now, <code>inet</code> and <code>macaddr</code> are supported.</dd>
<dt><code>Union</code>, <code>Map</code>, <code>Duration</code></dt>
<dd>Right now, PG-Strom cannot map these Arrow data types onto any of PostgreSQL data types.</dd>
</dl>
<h3 id="how-to-read-explain">How to read EXPLAIN</h3>
<p><code>EXPLAIN</code> command show us information about Arrow files reading.</p>
<p>The example below is an output of query execution plan that includes f_lineorder foreign table that mapps an Arrow file of 503GB.</p>
<pre><code>=# EXPLAIN
    SELECT sum(lo_extendedprice*lo_discount) as revenue
      FROM f_lineorder,date1
     WHERE lo_orderdate = d_datekey
       AND d_year = 1993
       AND lo_discount between 1 and 3
       AND lo_quantity &lt; 25;
                                        QUERY PLAN
--------------------------------------------------------------------------------
 Aggregate  (cost=14535261.08..14535261.09 rows=1 width=8)
   -&gt;  Custom Scan (GpuPreAgg) on f_lineorder  (cost=14535261.06..14535261.07 rows=1 width=32)
         GPU Projection: pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint)
         GPU Scan Quals: ((f_lineorder.lo_discount &gt;= 1) AND (f_lineorder.lo_discount &lt;= 3) AND (f_lineorder.lo_quantity &lt; 25)) [rows: 5999990000 -&gt; 9999983]
         GPU Join Quals [1]: (f_lineorder.lo_orderdate = date1.d_datekey) ... [nrows: 9999983 -&gt; 1428010]
         GPU Outer Hash [1]: f_lineorder.lo_orderdate
         GPU Inner Hash [1]: date1.d_datekey
         referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount
         file0: /opt/nvme/f_lineorder_s999.arrow (read: 89.41GB, size: 502.92GB)
         GPU-Direct SQL: enabled (GPU-0)
         -&gt;  Seq Scan on date1  (cost=0.00..78.95 rows=365 width=4)
               Filter: (d_year = 1993)
(12 rows)
</code></pre>
<p>According to the <code>EXPLAIN</code> output, we can see Custom Scan (GpuPreAgg) scans <code>f_lineorder</code> foreign table. <code>file0</code> item shows the filename (<code>/opt/nvme/lineorder_s999.arrow</code>) on behalf of the foreign table and its size. If multiple files are mapped, any files are individually shown, like <code>file1</code>, <code>file2</code>, ... The <code>referenced</code> item shows the list of referenced columns. We can see this query touches <code>lo_orderdate</code>, <code>lo_quantity</code>, <code>lo_extendedprice</code> and <code>lo_discount</code> columns.</p>
<p>In addition, <code>GPU-Direct SQL: enabled (GPU-0)</code> shows us the scan on <code>f_lineorder</code> uses GPU-Direct SQL mechanism.</p>
<p>VERBOSE option outputs more detailed information.</p>
<pre><code>=# EXPLAIN VERBOSE
    SELECT sum(lo_extendedprice*lo_discount) as revenue
      FROM f_lineorder,date1
     WHERE lo_orderdate = d_datekey
       AND d_year = 1993
       AND lo_discount between 1 and 3
       AND lo_quantity &lt; 25;
                                        QUERY PLAN
--------------------------------------------------------------------------------
 Aggregate  (cost=14535261.08..14535261.09 rows=1 width=8)
   Output: pgstrom.sum_int((pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint)))
   -&gt;  Custom Scan (GpuPreAgg) on public.f_lineorder  (cost=14535261.06..14535261.07 rows=1 width=32)
         Output: (pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint))
         GPU Projection: pgstrom.psum(((f_lineorder.lo_extendedprice * f_lineorder.lo_discount))::bigint)
         GPU Scan Quals: ((f_lineorder.lo_discount &gt;= 1) AND (f_lineorder.lo_discount &lt;= 3) AND (f_lineorder.lo_quantity &lt; 25)) [rows: 5999990000 -&gt; 9999983]
         GPU Join Quals [1]: (f_lineorder.lo_orderdate = date1.d_datekey) ... [nrows: 9999983 -&gt; 1428010]
         GPU Outer Hash [1]: f_lineorder.lo_orderdate
         GPU Inner Hash [1]: date1.d_datekey
         referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount
         file0: /opt/nvme/f_lineorder_s999.arrow (read: 89.41GB, size: 502.92GB)
           lo_orderdate: 22.35GB
           lo_quantity: 22.35GB
           lo_extendedprice: 22.35GB
           lo_discount: 22.35GB
         GPU-Direct SQL: enabled (GPU-0)
         KVars-Slot: &lt;slot=0, type='int4', expr='f_lineorder.lo_discount'&gt;, &lt;slot=1, type='int4', expr='f_lineorder.lo_quantity'&gt;, &lt;slot=2, type='int8', expr='(f_lineorder.lo_extendedprice * f_lineorder.lo_discount)'&gt;, &lt;slot=3, type='int4', expr='f_lineorder.lo_extendedprice'&gt;, &lt;slot=4, type='int4', expr='f_lineorder.lo_orderdate'&gt;, &lt;slot=5, type='int4', expr='date1.d_datekey'&gt;
         KVecs-Buffer: nbytes: 51200, ndims: 3, items=[kvec0=&lt;0x0000-27ff, type='int4', expr='lo_discount'&gt;, kvec1=&lt;0x2800-4fff, type='int4', expr='lo_quantity'&gt;, kvec2=&lt;0x5000-77ff, type='int4', expr='lo_extendedprice'&gt;, kvec3=&lt;0x7800-9fff, type='int4', expr='lo_orderdate'&gt;, kvec4=&lt;0xa000-c7ff, type='int4', expr='d_datekey'&gt;]
         LoadVars OpCode: {Packed items[0]={LoadVars(depth=0): kvars=[&lt;slot=4, type='int4' resno=6(lo_orderdate)&gt;, &lt;slot=1, type='int4' resno=9(lo_quantity)&gt;, &lt;slot=3, type='int4' resno=10(lo_extendedprice)&gt;, &lt;slot=0, type='int4' resno=12(lo_discount)&gt;]}, items[1]={LoadVars(depth=1): kvars=[&lt;slot=5, type='int4' resno=1(d_datekey)&gt;]}}
         MoveVars OpCode: {Packed items[0]={MoveVars(depth=0): items=[&lt;slot=0, offset=0x0000-27ff, type='int4', expr='lo_discount'&gt;, &lt;slot=3, offset=0x5000-77ff, type='int4', expr='lo_extendedprice'&gt;, &lt;slot=4, offset=0x7800-9fff, type='int4', expr='lo_orderdate'&gt;]}}, items[1]={MoveVars(depth=1): items=[&lt;offset=0x0000-27ff, type='int4', expr='lo_discount'&gt;, &lt;offset=0x5000-77ff, type='int4', expr='lo_extendedprice'&gt;]}}}
         Scan Quals OpCode: {Bool::AND args=[{Func(bool)::int4ge args=[{Var(int4): slot=0, expr='lo_discount'}, {Const(int4): value='1'}]}, {Func(bool)::int4le args=[{Var(int4): slot=0, expr='lo_discount'}, {Const(int4): value='3'}]}, {Func(bool)::int4lt args=[{Var(int4): slot=1, expr='lo_quantity'}, {Const(int4): value='25'}]}]}
         Join Quals OpCode: {Packed items[1]={JoinQuals:  {Func(bool)::int4eq args=[{Var(int4): kvec=0x7800-a000, expr='lo_orderdate'}, {Var(int4): slot=5, expr='d_datekey'}]}}}
         Join HashValue OpCode: {Packed items[1]={HashValue arg={Var(int4): kvec=0x7800-a000, expr='lo_orderdate'}}}
         Partial Aggregation OpCode: {AggFuncs &lt;psum::int[slot=2, expr='(lo_extendedprice * lo_discount)']&gt; arg={SaveExpr: &lt;slot=2, type='int8'&gt; arg={Func(int8)::int8 arg={Func(int4)::int4mul args=[{Var(int4): kvec=0x5000-7800, expr='lo_extendedprice'}, {Var(int4): kvec=0x0000-2800, expr='lo_discount'}]}}}}
         Partial Function BufSz: 16
         -&gt;  Seq Scan on public.date1  (cost=0.00..78.95 rows=365 width=4)
               Output: date1.d_datekey
               Filter: (date1.d_year = 1993)
(28 rows)
</code></pre>
<p>The verbose output additionally displays amount of column-data to be loaded on reference of columns. The load of <code>lo_orderdate</code>, <code>lo_quantity</code>, <code>lo_extendedprice</code> and <code>lo_discount</code> columns needs to read 89.41GB in total. It is 17.8% towards the filesize (502.93GB).</p>
<h2 id="arrow_fdw-virtual-column">Arrow_Fdw Virtual Column</h2>
<p>Arrow_Fdw allows to map multiple Apache Arrow files with compatible schema structures to a single foreign table. For example, if `dir '/opt/arrow/mydata' is configured for foreign table option, all files under that directory will be mapped.</p>
<p>When you are converting the contents of a transactional database into an Apache Arrow file, we often dump them to separate files by year and month or specific categories, and its file names reflects these properties.</p>
<p>The example below shows an example to convert the transactional table <code>lineorder</code> into Arrow files by year of <code>lo_orderdate</code> and by category of <code>lo_shipmode</code>.</p>
<pre><code>$ for s in RAIL AIR TRUCK SHIP FOB MAIL;
  do
    for y in 1993 1994 1995 1996 1997;
    do
      pg2arrow -d ssbm -c &quot;SELECT * FROM lineorder_small \
                            WHERE lo_orderdate between ${y}0101 and ${y}1231 \
                              AND lo_shipmode = '${s}'&quot; \
               -o /opt/arrow/mydata/f_lineorder_${y}_${s}.arrow
    done
  done
$ ls /opt/arrow/mydata/
f_lineorder_1993_AIR.arrow    f_lineorder_1995_RAIL.arrow
f_lineorder_1993_FOB.arrow    f_lineorder_1995_SHIP.arrow
f_lineorder_1993_MAIL.arrow   f_lineorder_1995_TRUCK.arrow
f_lineorder_1993_RAIL.arrow   f_lineorder_1996_AIR.arrow
f_lineorder_1993_SHIP.arrow   f_lineorder_1996_FOB.arrow
f_lineorder_1993_TRUCK.arrow  f_lineorder_1996_MAIL.arrow
f_lineorder_1994_AIR.arrow    f_lineorder_1996_RAIL.arrow
f_lineorder_1994_FOB.arrow    f_lineorder_1996_SHIP.arrow
f_lineorder_1994_MAIL.arrow   f_lineorder_1996_TRUCK.arrow
f_lineorder_1994_RAIL.arrow   f_lineorder_1997_AIR.arrow
f_lineorder_1994_SHIP.arrow   f_lineorder_1997_FOB.arrow
f_lineorder_1994_TRUCK.arrow  f_lineorder_1997_MAIL.arrow
f_lineorder_1995_AIR.arrow    f_lineorder_1997_RAIL.arrow
f_lineorder_1995_FOB.arrow    f_lineorder_1997_SHIP.arrow
f_lineorder_1995_MAIL.arrow   f_lineorder_1997_TRUCK.arrow
</code></pre>
<p>All these Apache Arrow files have the same schema structure and can be mapped to a single foreign table using the <code>dir</code> option.</p>
<p>Also, the Arrow file that has '1995' token in the file name only contains records with <code>lo_orderdate</code> in the range 19950101 to 19951231. The Arrow file that has 'RAIL' token in the file name only contains records with <code>lo_shipmode</code> of <code>RAIL</code>.</p>
<p>In other words, even if you define the Arrow_Fdw foreign  table that maps these multiple Arrow files, when reading data from a file whose file name includes 1995, it is assumed that the value of <code>lo_orderdate</code> is in the range of 19950101 to 19951231. It is possible for the optimizer to utilize this knowledge.</p>
<p>In Arrow_Fdw, you can refer to part of the file name as a column by using the foreign table option <code>pattern</code>. This is called a virtual column and is configured as follows.</p>
<pre><code>=# IMPORT FOREIGN SCHEMA f_lineorder
     FROM SERVER arrow_fdw INTO public
  OPTIONS (dir '/opt/arrow/mydata', pattern 'f_lineorder_@{year}_${shipping}.arrow');
IMPORT FOREIGN SCHEMA

=# \d f_lineorder
                             Foreign table &quot;public.f_lineorder&quot;
       Column       |     Type      | Collation | Nullable | Default |     FDW options
--------------------+---------------+-----------+----------+---------+----------------------
 lo_orderkey        | numeric       |           |          |         |
 lo_linenumber      | integer       |           |          |         |
 lo_custkey         | numeric       |           |          |         |
 lo_partkey         | integer       |           |          |         |
 lo_suppkey         | numeric       |           |          |         |
 lo_orderdate       | integer       |           |          |         |
 lo_orderpriority   | character(15) |           |          |         |
 lo_shippriority    | character(1)  |           |          |         |
 lo_quantity        | numeric       |           |          |         |
 lo_extendedprice   | numeric       |           |          |         |
 lo_ordertotalprice | numeric       |           |          |         |
 lo_discount        | numeric       |           |          |         |
 lo_revenue         | numeric       |           |          |         |
 lo_supplycost      | numeric       |           |          |         |
 lo_tax             | numeric       |           |          |         |
 lo_commit_date     | character(8)  |           |          |         |
 lo_shipmode        | character(10) |           |          |         |
 year               | bigint        |           |          |         | (virtual 'year')
 shipping           | text          |           |          |         | (virtual 'shipping')
Server: arrow_fdw
FDW options: (dir '/opt/arrow/mydata', pattern 'f_lineorder_@{year}_${shipping}.arrow')
</code></pre>
<p>This foreign table option <code>pattern</code> contains two wildcards.
<code>@{year}</code> matches a numeric string larger than or equal to 0 characters, and <code>${shipping}</code> matches a string larger than or equal to 0 characters.
The patterns that match this part of the file name can be referenced in the part specified by the <code>virtual</code> column option.</p>
<p>In this case, <code>IMPORT FOREIGN SCHEMA</code> automatically adds column definitions, in addition to the fields contained in the Arrow file itself, as well as the virtual column <code>year</code> (a <code>bigint</code> column) that references the wildcard <code>@{year}</code>, and the virtual column <code>shipping</code> that references the wildcard <code>${shipping}</code>.</p>
<pre><code>=# SELECT lo_orderkey, lo_orderdate, lo_shipmode, year, shipping
     FROM f_lineorder
    WHERE year = 1995 AND shipping = 'AIR'
    LIMIT 10;
 lo_orderkey | lo_orderdate | lo_shipmode | year | shipping
-------------+--------------+-------------+------+----------
      637892 |     19950512 | AIR         | 1995 | AIR
      638243 |     19950930 | AIR         | 1995 | AIR
      638273 |     19951214 | AIR         | 1995 | AIR
      637443 |     19950805 | AIR         | 1995 | AIR
      637444 |     19950803 | AIR         | 1995 | AIR
      637510 |     19950831 | AIR         | 1995 | AIR
      637504 |     19950726 | AIR         | 1995 | AIR
      637863 |     19950802 | AIR         | 1995 | AIR
      637892 |     19950512 | AIR         | 1995 | AIR
      637987 |     19950211 | AIR         | 1995 | AIR
(10 rows)
</code></pre>
<p>In other words, you can know what values the virtual columns have before reading the Arrow file mapped by the Arrow_Fdw foreign table. By this feature, if it is obvious that there is no match at all from the search conditions before reading a certain Arrow file, it is possible to skip reading the file itself.</p>
<p>See the query and its <code>EXPLAIN ANALYZE</code> output below.</p>
<p>This aggregation query reads the <code>f_lineorder</code> foreign table, filters it by some conditions, and then aggregates the total value of <code>lo_extendedprice * lo_discount</code>.
At that time, the conditional clause <code>WHERE year = 1994</code> is added. This is effectively the same as <code>WHERE lo_orderdate BETWEEN 19940101 AND 19942131</code>, but since <code>year</code> is a virtual column, you can determine whether a matching row exists before reading the Arrow files.</p>
<p>In fact, looking at the <code>Stats-Hint:</code> line, 12 Record-Batches were loaded due to the condition <code>(year = 1994)</code>, but 48 Record-Batches were skipped. This is a simple but extremely effective means of reducing I/O load.</p>
<pre><code>=# EXPLAIN ANALYZE
   SELECT sum(lo_extendedprice*lo_discount) as revenue
     FROM f_lineorder
    WHERE year = 1994
      AND lo_discount between 1 and 3
      AND lo_quantity &lt; 25;
                                               QUERY PLAN
--------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=421987.07..421987.08 rows=1 width=32) (actual time=82.914..82.915 rows=1 loops=1)
   -&gt;  Custom Scan (GpuPreAgg) on f_lineorder  (cost=421987.05..421987.06 rows=1 width=32)      \
                                               (actual time=82.901..82.903 rows=2 loops=1)
         GPU Projection: pgstrom.psum(((lo_extendedprice * lo_discount))::double precision)
         GPU Scan Quals: ((year = 1994) AND (lo_discount &lt;= '3'::numeric) AND                   \
                          (lo_quantity &lt; '25'::numeric) AND                                     \
                          (lo_discount &gt;= '1'::numeric)) [plan: 65062080 -&gt; 542, exec: 13001908 -&gt; 1701726]
         referenced: lo_quantity, lo_extendedprice, lo_discount, year
         Stats-Hint: (year = 1994)  [loaded: 12, skipped: 48]
         file0: /opt/arrow/mydata/f_lineorder_1996_MAIL.arrow (read: 99.53MB, size: 427.16MB)
         file1: /opt/arrow/mydata/f_lineorder_1996_SHIP.arrow (read: 99.52MB, size: 427.13MB)
         file2: /opt/arrow/mydata/f_lineorder_1994_FOB.arrow (read: 99.18MB, size: 425.67MB)
              :                :                                       :             :
         file27: /opt/arrow/mydata/f_lineorder_1997_MAIL.arrow (read: 99.23MB, size: 425.87MB)
         file28: /opt/arrow/mydata/f_lineorder_1995_MAIL.arrow (read: 99.16MB, size: 425.58MB)
         file29: /opt/arrow/mydata/f_lineorder_1993_TRUCK.arrow (read: 99.24MB, size: 425.91MB)
         GPU-Direct SQL: enabled (N=2,GPU0,1; direct=76195, ntuples=13001908)
 Planning Time: 2.402 ms
 Execution Time: 83.857 ms
(39 rows)
</code></pre>
<h2 id="how-to-make-arrow-files">How to make Arrow files</h2>
<p>This section introduces the way to transform dataset already stored in PostgreSQL database system into Apache Arrow file.</p>
<h3 id="using-pyarrowpandas">Using PyArrow+Pandas</h3>
<p>A pair of PyArrow module, developed by Arrow developers community, and Pandas data frame can dump PostgreSQL database into an Arrow file.</p>
<p>The example below reads all the data in table <code>t0</code>, then write out them into <code>/tmp/t0.arrow</code>.</p>
<pre><code>import pyarrow as pa
import pandas as pd

X = pd.read_sql(sql=&quot;SELECT * FROM t0&quot;, con=&quot;postgresql://localhost/postgres&quot;)
Y = pa.Table.from_pandas(X)
f = pa.RecordBatchFileWriter('/tmp/t0.arrow', Y.schema)
f.write_table(Y,1000000)      # RecordBatch for each million rows
f.close()
</code></pre>
<p>Please note that the above operation once keeps query result of the SQL on memory, so should pay attention on memory consumption if you want to transfer massive rows at once.</p>
<h3 id="using-pg2arrow">Using Pg2Arrow</h3>
<p>On the other hand, <code>pg2arrow</code> command, developed by PG-Strom Development Team, enables us to write out query result into Arrow file. This tool is designed to write out massive amount of data into storage device like NVME-SSD. It fetch query results from PostgreSQL database system, and write out Record Batches of Arrow format for each data size specified by the <code>-s|--segment-size</code> option. Thus, its memory consumption is relatively reasonable.</p>
<p><code>pg2arrow</code> command is distributed with PG-Strom. It shall be installed on the <code>bin</code> directory of PostgreSQL related utilities.</p>
<pre><code>$ pg2arrow --help
Usage:
  pg2arrow [OPTION] [database] [username]

General options:
  -d, --dbname=DBNAME   Database name to connect to
  -c, --command=COMMAND SQL command to run
  -t, --table=TABLENAME Equivalent to '-c SELECT * FROM TABLENAME'
      (-c and -t are exclusive, either of them must be given)
      --inner-join=SUB_COMMAND
      --outer-join=SUB_COMMAND
  -o, --output=FILENAME result file in Apache Arrow format
      --append=FILENAME result Apache Arrow file to be appended
      (--output and --append are exclusive. If neither of them
       are given, it creates a temporary file.)
  -S, --stat[=COLUMNS] embeds min/max statistics for each record batch
                       COLUMNS is a comma-separated list of the target
                       columns if partially enabled.

Arrow format options:
  -s, --segment-size=SIZE size of record batch for each

Connection options:
  -h, --host=HOSTNAME  database server host
  -p, --port=PORT      database server port
  -u, --user=USERNAME  database user name
  -w, --no-password    never prompt for password
  -W, --password       force password prompt

Other options:
      --dump=FILENAME  dump information of arrow file
      --progress       shows progress of the job
      --set=NAME:VALUE config option to set before SQL execution
      --help           shows this message

Report bugs to &lt;pgstrom@heterodbcom&gt;.
</code></pre>
<p>The <code>-h</code> or <code>-U</code> option specifies the connection parameters of PostgreSQL, like <code>psql</code> or <code>pg_dump</code>. The simplest usage of this command is running a SQL command specified by <code>-c|--command</code> option on PostgreSQL server, then write out results into the file specified by <code>-o|--output</code> option in Arrow format.</p>
<p><code>--append</code> option is available, instead of <code>-o|--output</code> option. It means appending data to existing Apache Arrow file. In this case, the target Apache Arrow file must have fully identical schema definition towards the specified SQL command.</p>
<p>The example below reads all the data in table <code>t0</code>, then write out them into the file <code>/tmp/t0.arrow</code>.</p>
<pre><code>$ pg2arrow -U kaigai -d postgres -c &quot;SELECT * FROM t0&quot; -o /tmp/t0.arrow
</code></pre>
<p>Although it is an option for developers, <code>--dump &lt;filename&gt;</code> prints schema definition and record-batch location and size of Arrow file in human readable form.</p>
<p><code>--progress</code> option enables to show progress of the task. It is useful when a huge table is transformed to Apache Arrow format.</p>
<h2 id="advanced-usage">Advanced Usage</h2>
<h3 id="ssdtogpu-direct-sql">SSDtoGPU Direct SQL</h3>
<p>In case when all the Arrow files mapped on the Arrow_Fdw foreign table satisfies the terms below, PG-Strom enables SSD-to-GPU Direct SQL to load columnar data.</p>
<ul>
<li>Arrow files are on NVME-SSD volume.</li>
<li>NVME-SSD volume is managed by Ext4 filesystem.</li>
<li>Total size of Arrow files exceeds the <code>pg_strom.nvme_strom_threshold</code> configuration.</li>
</ul>
<h3 id="partition-configuration">Partition configuration</h3>
<p>Arrow_Fdw foreign tables can be used as a part of partition leafs. Usual PostgreSQL tables can be mixtured with Arrow_Fdw foreign tables. So, pay attention Arrow_Fdw foreign table does not support any writer operations. And, make boundary condition of the partition consistent to the contents of the mapped Arrow file. It is a responsibility of the database administrators.</p>
<p><img alt="Example of partition configuration" src="../img/partition-logdata.png" /></p>
<p>A typical usage scenario is processing of long-standing accumulated log-data.</p>
<p>Unlike transactional data, log-data is mostly write-once and will never be updated / deleted. Thus, by migration of the log-data after a lapse of certain period into Arrow_Fdw foreign table that is read-only but rapid processing, we can accelerate summarizing and analytics workloads. In addition, log-data likely have timestamp, so it is quite easy design to add partition leafs periodically, like monthly, weekly or others.</p>
<p>The example below defines a partitioned table that mixes a normal PostgreSQL table and Arrow_Fdw foreign tables.</p>
<p>The normal PostgreSQL table, is read-writable, is specified as default partition, so DBA can migrate only past log-data into Arrow_Fdw foreign table under the database system operations.</p>
<pre><code>CREATE TABLE lineorder (
    lo_orderkey numeric,
    lo_linenumber integer,
    lo_custkey numeric,
    lo_partkey integer,
    lo_suppkey numeric,
    lo_orderdate integer,
    lo_orderpriority character(15),
    lo_shippriority character(1),
    lo_quantity numeric,
    lo_extendedprice numeric,
    lo_ordertotalprice numeric,
    lo_discount numeric,
    lo_revenue numeric,
    lo_supplycost numeric,
    lo_tax numeric,
    lo_commit_date character(8),
    lo_shipmode character(10)
) PARTITION BY RANGE (lo_orderdate);

CREATE TABLE lineorder__now PARTITION OF lineorder default;

CREATE FOREIGN TABLE lineorder__1993 PARTITION OF lineorder
   FOR VALUES FROM (19930101) TO (19940101)
SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1993.arrow');

CREATE FOREIGN TABLE lineorder__1994 PARTITION OF lineorder
   FOR VALUES FROM (19940101) TO (19950101)
SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1994.arrow');

CREATE FOREIGN TABLE lineorder__1995 PARTITION OF lineorder
   FOR VALUES FROM (19950101) TO (19960101)
SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1995.arrow');

CREATE FOREIGN TABLE lineorder__1996 PARTITION OF lineorder
   FOR VALUES FROM (19960101) TO (19970101)
SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1996.arrow');
</code></pre>
<p>Below is the query execution plan towards the table. By the query condition <code>lo_orderdate between 19950701 and 19960630</code> that touches boundary condition of the partition, the partition leaf <code>lineorder__1993</code> and <code>lineorder__1994</code> are pruned, so it makes a query execution plan to read other (foreign) tables only.</p>
<pre><code>=# EXPLAIN
    SELECT sum(lo_extendedprice*lo_discount) as revenue
      FROM lineorder,date1
     WHERE lo_orderdate = d_datekey
       AND lo_orderdate between 19950701 and 19960630
       AND lo_discount between 1 and 3
       ABD lo_quantity &lt; 25;

                                 QUERY PLAN
--------------------------------------------------------------------------------
 Aggregate  (cost=172088.90..172088.91 rows=1 width=32)
   -&gt;  Hash Join  (cost=10548.86..172088.51 rows=77 width=64)
         Hash Cond: (lineorder__1995.lo_orderdate = date1.d_datekey)
         -&gt;  Append  (cost=10444.35..171983.80 rows=77 width=67)
               -&gt;  Custom Scan (GpuScan) on lineorder__1995  (cost=10444.35..33671.87 rows=38 width=68)
                     GPU Filter: ((lo_orderdate &gt;= 19950701) AND (lo_orderdate &lt;= 19960630) AND
                                  (lo_discount &gt;= '1'::numeric) AND (lo_discount &lt;= '3'::numeric) AND
                                  (lo_quantity &lt; '25'::numeric))
                     referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount
                     files0: /opt/tmp/lineorder_1995.arrow (size: 892.57MB)
               -&gt;  Custom Scan (GpuScan) on lineorder__1996  (cost=10444.62..33849.21 rows=38 width=68)
                     GPU Filter: ((lo_orderdate &gt;= 19950701) AND (lo_orderdate &lt;= 19960630) AND
                                  (lo_discount &gt;= '1'::numeric) AND (lo_discount &lt;= '3'::numeric) AND
                                  (lo_quantity &lt; '25'::numeric))
                     referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount
                     files0: /opt/tmp/lineorder_1996.arrow (size: 897.87MB)
               -&gt;  Custom Scan (GpuScan) on lineorder__now  (cost=11561.33..104462.33 rows=1 width=18)
                     GPU Filter: ((lo_orderdate &gt;= 19950701) AND (lo_orderdate &lt;= 19960630) AND
                                  (lo_discount &gt;= '1'::numeric) AND (lo_discount &lt;= '3'::numeric) AND
                                  (lo_quantity &lt; '25'::numeric))
         -&gt;  Hash  (cost=72.56..72.56 rows=2556 width=4)
               -&gt;  Seq Scan on date1  (cost=0.00..72.56 rows=2556 width=4)
(16 rows)

</code></pre>
<p>The operation below extracts the data in <code>1997</code> from <code>lineorder__now</code> table, then move to a new Arrow_Fdw foreign table.</p>
<pre><code>$ pg2arrow -d sample  -o /opt/tmp/lineorder_1997.arrow \
           -c &quot;SELECT * FROM lineorder WHERE lo_orderdate between 19970101 and 19971231&quot;
</code></pre>
<p><code>pg2arrow</code> command extracts the data in 1997 from the <code>lineorder</code> table into a new Arrow file.</p>
<pre><code>BEGIN;
--
-- remove rows in 1997 from the read-writable table
--
DELETE FROM lineorder WHERE lo_orderdate BETWEEN 19970101 AND 19971231;
--
-- define a new partition leaf which maps log-data in 1997
--
CREATE FOREIGN TABLE lineorder__1997 PARTITION OF lineorder
   FOR VALUES FROM (19970101) TO (19980101)
SERVER arrow_fdw OPTIONS (file '/opt/tmp/lineorder_1997.arrow');

COMMIT;
</code></pre>
<p>A series of operations above delete the data in 1997 from <code>lineorder__new</code> that is a PostgreSQL table, then maps an Arrow file (<code>/opt/tmp/lineorder_1997.arrow</code>) which contains an identical contents as a foreign table <code>lineorder__1997</code>.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>For correctness, block size is configurable on build from 4KB to 32KB.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../ssd2gpu/" class="btn btn-neutral float-left" title="GPUDirect SQL"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../gpucache/" class="btn btn-neutral float-right" title="GPU Cache">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../ssd2gpu/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../gpucache/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
