<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="PG-Strom Development Team" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Install - PG-Strom Manual</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="//fonts.googleapis.com/earlyaccess/notosansjp.css" rel="stylesheet" />
        <link href="//fonts.googleapis.com/css?family=Open+Sans:600,800" rel="stylesheet" />
        <link href="../custom.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Install";
        var mkdocs_page_input_path = "install.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> PG-Strom Manual
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
  [<a href="../ja/install/" style="color: #cccccc">Japanese</a> | <strong>English</strong>]
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Install</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#checklist">Checklist</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#steps-to-install">Steps to Install</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#os-installation">OS Installation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#disables-iommu">Disables IOMMU</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#configuration-at-rhel9rhel10">Configuration at RHEL9/RHEL10</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#configuration-at-rhel8">Configuration at RHEL8</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#disables-nouveau-driver">Disables nouveau driver</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#enables-extra-repositories">Enables extra repositories</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#epelextra-packages-for-enterprise-linux">EPEL(Extra Packages for Enterprise Linux)</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#red-hat-codeready-linux-builder">Red Hat CodeReady Linux Builder</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#docaofed-driver-installation">DOCA(OFED) Driver Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#cuda-toolkit-installation">CUDA Toolkit Installation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#check-gpudirect-storage-status">Check GPUDirect Storage status</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pci-bar1-memory-configuration">PCI Bar1 Memory Configuration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#edit-etccufilejson">Edit /etc/cufile.json</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#heterodb-extra-modules">HeteroDB extra modules</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#license-activation">License activation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#postgresql-installation">PostgreSQL Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#libarrowlibparquet-installation">libarrow/libparquet Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#pg-strom-installation">PG-Strom Installation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#rpm-installation">RPM Installation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#installation-from-the-source">Installation from the source</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#getting-the-source-code">Getting the source code</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#building-the-pg-strom">Building the PG-Strom</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#post-installation-setup">Post Installation Setup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#creation-of-database-cluster">Creation of database cluster</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#setup-postgresqlconf">Setup postgresql.conf</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#expand-os-resource-limits">Expand OS resource limits</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#start-postgresql">Start PostgreSQL</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#creation-of-pg-strom-extension">Creation of PG-Strom Extension</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#postgis-installation">PostGIS Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installation-on-ubuntu-linux">Installation on Ubuntu Linux</a>
    </li>
    </ul>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Tutorial</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../operations/">Basic Operations</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../brin/">BRIN Index</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../partition/">Partitioning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../postgis/">PostGIS</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../gpusort/">GPU-Sort</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../troubles/">Trouble Shooting</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Advanced Features</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ssd2gpu/">GPUDirect SQL</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../arrow_fdw/">Apache Arrow</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../gpucache/">GPU Cache</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../pinned_buffer/">Pinned Inner Buffer</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../fluentd/">connect with Fluentd</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">References</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_types/">Data Types</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_devfuncs/">Functions and Operators</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_sqlfuncs/">SQL Objects</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_params/">GUC Parameters</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Release Note</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v6.1/">PG-Strom v6.1</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v6.0/">PG-Strom v6.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.2/">PG-Strom v5.2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.1/">PG-Strom v5.1</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v5.0/">PG-Strom v5.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v3.0/">PG-Strom v3.0</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.3/">PG-Strom v2.3</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.2/">PG-Strom v2.2</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../release_v2.0/">PG-Strom v2.0</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">PG-Strom Manual</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Install</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="installation">Installation</h1>
<p>This chapter introduces the steps to install PG-Strom.</p>
<h2 id="checklist">Checklist</h2>
<ul>
<li><strong>Server Hardware</strong><ul>
<li>It requires generic x86_64 hardware that can run Linux operating system supported by CUDA Toolkit. We have no special requirement for CPU, storage and network devices.</li>
<li><a href="https://github.com/heterodb/pg-strom/wiki/002:-HW-Validation-List">note002:HW Validation List</a> may help you to choose the hardware.</li>
<li>GPU Direct SQL Execution needs NVME-SSD devices, or fast network card with RoCE support, and to be installed under the same PCIe Root Complex where GPU is located on.</li>
</ul>
</li>
<li><strong>GPU Device</strong><ul>
<li>PG-Strom requires at least one GPU device on the system, which is supported by CUDA Toolkit, has computing capability 7.5 (Turing generation) or later;</li>
<li>Please check at <a href="https://github.com/heterodb/pg-strom/wiki/002:-HW-Validation-List#list-of-supported-gpu-models">002: HW Validation List - List of supported GPU models</a> for GPU selection.</li>
</ul>
</li>
<li><strong>Operating System</strong><ul>
<li>PG-Strom requires Linux operating system for x86_64 architecture, and its distribution supported by CUDA Toolkit. Our recommendation is Red Hat Enterprise Linux or Rocky Linux version 10.x, 9.x or 8.x series.</li>
<li>GPU Direct SQL (with cuFile driver) needs the <code>nvidia-fs</code> driver distributed with CUDA Toolkit, and Mellanox OFED (OpenFabrics Enterprise Distribution) driver.</li>
</ul>
</li>
<li><strong>PostgreSQL</strong><ul>
<li>PG-Strom v6.1 requires PostgreSQL v15 or later.</li>
<li>Some of PostgreSQL APIs used by PG-Strom internally are not included in the former versions.</li>
</ul>
</li>
<li><strong>CUDA Toolkit</strong><ul>
<li>PG-Strom requires CUDA Toolkit version 12.2update1 or later.</li>
<li>Some of CUDA Driver APIs used by PG-Strom internally are not included in the former versions.</li>
</ul>
</li>
</ul>
<h2 id="steps-to-install">Steps to Install</h2>
<p>The overall steps to install are below:</p>
<ol>
<li>Hardware Configuration</li>
<li>OS Installation</li>
<li>MOFED Driver installation</li>
<li>CUDA Toolkit installation</li>
<li>HeteroDB Extra Module installation</li>
<li>PostgreSQL installation</li>
<li>PG-Strom installation</li>
<li>PostgreSQL Extensions installation<ul>
<li>PostGIS</li>
<li>contrib/cube</li>
</ul>
</li>
</ol>
<h2 id="os-installation">OS Installation</h2>
<p>Choose a Linux distribution which is supported by CUDA Toolkit, then install the system according to the installation process of the distribution. <a href="https://developer.nvidia.com/">NVIDIA DEVELOPER ZONE</a> introduces the list of Linux distributions which are supported by CUDA Toolkit.</p>
<p>In case of Red Hat Enterprise Linux 8.x series (including Rocky Linux 8.x series), choose "Minimal installation" as base environment, and also check the "Development Tools" add-ons for the software selection</p>
<p>In case of Red Hat Enterprise Linux 9.x series (including Rocky Linux 9.x series), choose "Minimal installation" as base environment, and also check the "Standard" and "Development Tools" add-ons for the software selection</p>
<p>In case of Red Hat Enterprise Linux 10.x series (including Rocky Linux 10.x series), choose "Minimal installation" as base environment, and also check the "Standard" and "Development Tools" add-ons for the software selection</p>
<p><img alt="RHEL10 Software Selection" src="../img/rhel10_package_selection.png" /></p>
<p>Next to the OS installation on the server, go on the package repository configuration to install the third-party packages.</p>
<p>If you didn't check the "Development Tools" at the installer, we can additionally install the software using the command below after the operating system installation.</p>
<pre><code># dnf groupinstall 'Development Tools'
</code></pre>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If GPU devices installed on the server are too new, it may cause system crash during system boot.
In this case, you may avoid the problem by adding <code>nouveau.modeset=0</code> onto the kernel boot option, to disable
the inbox graphic driver.</p>
</div>
<h3 id="disables-iommu">Disables IOMMU</h3>
<p>GPU-Direct SQL uses GPUDirect Storage (cuFile) API of CUDA.</p>
<p>Prior to using GPUDirect Storage, it needs to disable the IOMMU configuration on the OS side.</p>
<p>Configure the kernel boot option according to the <a href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs">NVIDIA GPUDirect Storage Installation and Troubleshooting Guide</a> description.</p>
<p>To disable IOMMU, add <code>amd_iommu=off</code> (for AMD CPU) or <code>intel_iommu=off</code> (for Intel CPU) to the kernel boot options.</p>
<h4 id="configuration-at-rhel9rhel10">Configuration at RHEL9/RHEL10</h4>
<p>The command below adds the kernel boot option.</p>
<pre><code># grubby --update-kernel=ALL --args=&quot;amd_iommu=off&quot;
</code></pre>
<h4 id="configuration-at-rhel8">Configuration at RHEL8</h4>
<p>Open <code>/etc/default/grub</code> with an editor and add the above option to the <code>GRUB_CMDLINE_LINUX_DEFAULT=</code> line.</p>
<p>For example, the settings should look like this:</p>
<pre><code>  :
GRUB_CMDLINE_LINUX=&quot;rhgb quiet amd_iommu=off&quot;
  :
</code></pre>
<p>Run the following commands to apply the configuration to the kernel bool options.</p>
<pre><code>-- for BIOS based system
# grub2-mkconfig -o /boot/grub2/grub.cfg
# shutdown -r now

-- for UEFI based system
# grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg
# shutdown -r now
</code></pre>
<h3 id="disables-nouveau-driver">Disables nouveau driver</h3>
<p>When the nouveau driver, that is an open source compatible driver for NVIDIA GPUs, is loaded, it prevent to load the nvidia driver.
In this case, reboot the operating system after a configuration to disable the nouveau driver.</p>
<p>To disable the nouveau driver, put the following configuration onto <code>/etc/modprobe.d/disable-nouveau.conf</code>, and run <code>dracut</code> command to apply them on the boot image of Linux kernel.
Then, restart the system once.</p>
<pre><code># cat &gt; /etc/modprobe.d/disable-nouveau.conf &lt;&lt;EOF
blacklist nouveau
options nouveau modeset=0
EOF
# dracut -f
# shutdown -r now
</code></pre>
<h3 id="enables-extra-repositories">Enables extra repositories</h3>
<h4 id="epelextra-packages-for-enterprise-linux">EPEL(Extra Packages for Enterprise Linux)</h4>
<p>Several software modules required by PG-Strom are distributed as a part of EPEL (Extra Packages for Enterprise Linux).
You need to add a repository definition of EPEL packages for yum system to obtain these software.</p>
<p>One of the package we will get from EPEL repository is DKMS (Dynamic Kernel Module Support). It is a framework to build Linux kernel module for the running Linux kernel on demand; used for NVIDIA's GPU driver and related.
Linux kernel module must be rebuilt according to version-up of Linux kernel, so we don't recommend to operate the system without DKMS.</p>
<p><code>epel-release</code> package provides the repository definition of EPEL. You can obtain the package from the <a href="https://docs.fedoraproject.org/en-US/epel/#_quickstart">Fedora Project</a> website.</p>
<pre><code>-- For RHEL10
# dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-10.noarch.rpm

-- For RHEL9
# dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm

-- For RHEL8
# dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm

-- For Rocky8/Rocky9
# dnf install epel-release
</code></pre>
<h4 id="red-hat-codeready-linux-builder">Red Hat CodeReady Linux Builder</h4>
<p>Installation of MOFED (Mellanox OpenFabrics Enterprise Distribution) driver requires the <strong><em>Red Hat CodeReady Linux Builder</em></strong> repository which is disabled in the default configuration of Red Hat Enterprise Linux 8.x installation. In Rocky Linux, it is called <strong><em>PowerTools</em></strong></p>
<p>To enable this repository, run the command below:</p>
<pre><code>-- For RHEL10
# subscription-manager repos --enable codeready-builder-for-rhel-10-x86_64-rpms

-- For RHEL9
# subscription-manager repos --enable codeready-builder-for-rhel-9-x86_64-rpms

-- For Rocky9/Rocky10
# dnf config-manager --set-enabled crb

-- For RHEL8
# subscription-manager repos --enable codeready-builder-for-rhel-8-x86_64-rpms

-- For Rocky8
# dnf config-manager --set-enabled powertools
</code></pre>
<h2 id="docaofed-driver-installation">DOCA(OFED) Driver Installation</h2>
<p>To use direct data from NVME-SSD to GPU (GPU-Direct SQL), you need to replace the operating system default inbox driver with the NVME drive distributed by DOCA (OFED) module.
This section introduces the steps to install the DOCA (OFED) module.</p>
<div class="admonition info">
<p class="admonition-title">Info</p>
</div>
<p>Note that the Linux kernel driver portion of the DOCA (OFED) module was formerly called MOFED (Mellanox Open Fabric Enterprise Driver),
   and can still be downloaded from <a href="https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/">here</a>. 
   However, it has already entered Long Term Support mode, and there is no prospect of the driver being provided for newer environments
   such as RHEL10. So, we recommend using the DOCA (OFED) package for the PG-Strom installation procedure.</p>
<p>The official installation instructions for the DOCA (OFED) module can be found <a href="https://docs.nvidia.com/dgx/dgx-rhel8-install-guide/installing-dofed-steps.html">here</a>.</p>
<p><strong><em>Add repository definition</em></strong></p>
<p>Add the DOCA repository definition as follows.
For RHEL 8/9 series, the <code>${releasever_major}</code> and <code>${releasever_minor}</code> macros are not defined, so write the OS version directly.</p>
<p>Example configuration for RHEL10</p>
<pre><code># cat &gt; /etc/yum.repos.d/doca.repo &lt;&lt;EOF
[doca]
name=DOCA Online Repo
baseurl=https://linux.mellanox.com/public/repo/doca/latest/rhel\${releasever_major}.\${releasever_minor}/\${basearch}/
enabled=1
gpgcheck=0
EOF
</code></pre>
<p>Example configuration for RHEL9</p>
<pre><code># cat &gt; /etc/yum.repos.d/doca.repo &lt;&lt;EOF
[doca]
name=DOCA Online Repo
baseurl=https://linux.mellanox.com/public/repo/doca/latest/rhel9.6/\${basearch}/
enabled=1
gpgcheck=0
EOF
</code></pre>
<p><strong><em>Installation of DOCA packages</em></strong></p>
<p>Install the necessary components from the DOCA package distributed by NVIDIA.
As shown in the example below, please install <code>kernel-module-extra-&lt;KERNEL VERSION&gt;</code> (which should already be installed), <code>doca-ofed</code>, and <code>doca-extra</code>.</p>
<pre><code># dnf install -y kernel-modules-extra-$(uname -r)
# dnf install -y doca-ofed doca-extra
</code></pre>
<p><strong><em>Building the NVME driver</em></strong></p>
<p>Although the DOCA (OFED) package distributed by NVIDIA includes the NVMe driver,
this driver is not built with the <code>--with-gds</code> option, which is necessary to enable GPU-Direct Storage.</p>
<p>So, we need to recompiler the driver to turn on GPU-Direct SQL.
The procedure is fully scripted and can be executed as follows: <code>/opt/mellanox/doca/tools/doca-kernel-support</code>.</p>
<pre><code># /opt/mellanox/doca/tools/doca-kernel-support
doca-kernel-support: Building under /tmp/DOCA.EhUCIyOzbU
           :
doca-kernel-support: Rebuilding kernel modules
doca-kernel-support: Building mlnx-ofa_kernel under /tmp/DOCA.EhUCIyOzbU/build/mlnx-ofa_kernel with log /tmp/DOCA.EhUCIyOzbU/logs/mlnx-ofa_kernel.log
doca-kernel-support: Building iser under /tmp/DOCA.EhUCIyOzbU/build/iser with log /tmp/DOCA.EhUCIyOzbU/logs/iser.log
doca-kernel-support: Building isert under /tmp/DOCA.EhUCIyOzbU/build/isert with log /tmp/DOCA.EhUCIyOzbU/logs/isert.log
doca-kernel-support: Building srp under /tmp/DOCA.EhUCIyOzbU/build/srp with log /tmp/DOCA.EhUCIyOzbU/logs/srp.log
doca-kernel-support: Building mlnx-nfsrdma under /tmp/DOCA.EhUCIyOzbU/build/mlnx-nfsrdma with log /tmp/DOCA.EhUCIyOzbU/logs/mlnx-nfsrdma.log
doca-kernel-support: Building mlnx-nvme under /tmp/DOCA.EhUCIyOzbU/build/mlnx-nvme with log /tmp/DOCA.EhUCIyOzbU/logs/mlnx-nvme.log
doca-kernel-support: Building virtiofs under /tmp/DOCA.EhUCIyOzbU/build/virtiofs with log /tmp/DOCA.EhUCIyOzbU/logs/virtiofs.log
doca-kernel-support: Building knem under /tmp/DOCA.EhUCIyOzbU/build/knem with log /tmp/DOCA.EhUCIyOzbU/logs/knem.log
doca-kernel-support: Building xpmem under /tmp/DOCA.EhUCIyOzbU/build/xpmem with log /tmp/DOCA.EhUCIyOzbU/logs/xpmem.log
doca-kernel-support: Building kernel-mft under /tmp/DOCA.EhUCIyOzbU/build/kernel-mft with log /tmp/DOCA.EhUCIyOzbU/logs/kernel-mft.log
doca-kernel-support: Creating a rpm meta package:
doca-kernel-support: Creating a package repository in /tmp/DOCA.EhUCIyOzbU/repo/usr/share/doca-host-25.07-0.9.7.0/Modules/6.12.0-55.41.1.el10_0.x86_64
doca-kernel-support: Built single package: /tmp/DOCA.EhUCIyOzbU/doca-kernel-repo-25.07.0.9.7.0-1.kver.6.12.0.55.41.1.el10.0.x86.64.x86_64.rpm
doca-kernel-support: Done
Now you should install the generated single repository package to make its
files available:

  rpm -Uvh /tmp/DOCA.EhUCIyOzbU/doca-kernel-repo-25.07.0.9.7.0-1.kver.6.12.0.55.41.1.el10.0.x86.64.x86_64.rpm

After installing the package, doca-kernel-6.12.0.55.41.1.el10.0.x86.64 metapackage should be available.

Next steps could probably be:
  dnf makecache

Then, install ofed packages (e.g. doca-all, doca-ofed):
        dnf install doca-ofed-userspace
        dnf install --disablerepo=doca doca-kernel-6.12.0.55.41.1.el10.0.x86.64
</code></pre>
<p>Once the script finishes successfully, it will suggest the next steps you should take.
First, install <code>doca-kernel-&lt;KERNEL_VERSION&gt;</code> to build a local repository containing the rebuilt kernel module.</p>
<pre><code>[root@saba~]# rpm -Uvh /tmp/DOCA.EhUCIyOzbU/doca-kernel-repo-25.07.0.9.7.0-1.kver.6.12.0.55.41.1.el10.0.x86.64.x86_64.rpm
Verifying...                          ################################# [100%]
Preparing...                          ################################# [100%]
Updating / installing...
   1:doca-kernel-repo-25.07.0.9.7.0-1.################################# [100%]
</code></pre>
<p>Next, install the <code>doca-kernel-&lt;KERNEL_VERSION&gt;</code>, <code>kmod-mlnx-nvme</code> and <code>kmod-mlnx-nfsrdma</code> packages
according to the <code>doca-kernel-support</code> script output.</p>
<pre><code># dnf makecache
# dnf install doca-ofed-userspace
# dnf install --disablerepo=doca doca-kernel-6.12.0.55.41.1.el10.0.x86.64
# dnf install --disablerepo=doca kmod-mlnx-nvme kmod-mlnx-nfsrdma
</code></pre>
<p>Confirm location of the <code>nvme</code> kernel module using <code>modinfo</code> command.
It was successfull installed if this kernel module is installed at <code>extra/mlnx-nvme/host</code> directory.
(The OS default INBOX driver is stored at <code>kernel/drivers/nvme/host</code>.)</p>
<pre><code># modinfo nvme
filename:       /lib/modules/6.12.0-55.41.1.el10_0.x86_64/extra/mlnx-nvme/host/nvme.ko
description:    NVMe host PCIe transport driver
version:        1.0
license:        GPL
author:         Matthew Wilcox &lt;willy@linuxintel.com&gt;
rhelversion:    10.0
srcversion:     951B0D9C33E4E9A9D30FF50
alias:          pci:v*d*sv*sd*bc01sc08i02*
alias:          pci:v0000106Bd00002005sv*sd*bc*sc*i*
           :
</code></pre>
<p>Update the boot image using <code>dracut</code> command and restart the system to reflect the new <code>nvme</code> driver.</p>
<pre><code># dracut -f
# shutdown -r now
</code></pre>
<h2 id="cuda-toolkit-installation">CUDA Toolkit Installation</h2>
<p>This section introduces the installation of CUDA Toolkit. If you already installed the latest CUDA Toolkit, you can check whether your installation is identical with the configuration described in this section.</p>
<p>NVIDIA offers two approach to install CUDA Toolkit; one is by self-extracting archive (runfile), and the other is by RPM packages.
We recommend the RPM installation for PG-Strom setup.</p>
<p>You can download the installation package for CUDA Toolkit from NVIDIA DEVELOPER ZONE. Choose your OS, architecture, distribution and version, then choose "rpm(network)" edition.</p>
<p><img alt="CUDA Toolkit download" src="../img/cuda-download.png" /></p>
<p>Once you choose the "rpm(network)" option, it shows a few step-by-step shell commands to register the CUDA repository and install the packages.
Run the installation according to the guidance.
The example below shows the steps to install CUDA Toolkit 13 on RHEL10 environment.</p>
<pre><code># dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel10/x86_64/cuda-rhel10.repo
# dnf clean all
# dnf install cuda-toolkit-13-0
# dnf install nvidia-gds-13-0
</code></pre>
<p>Next to the installation of the CUDA Toolkit, two types of commands are introduced to install the nvidia driver.</p>
<p>Please use the open source version of nvidia-driver here. Only the open source version supports the GPUDirect Storage feature, and PG-Strom's GPU-Direct SQL utilizes this feature.</p>
<p>Next, install the driver module <code>nvidia-gds</code> for the GPU-Direct Storage (GDS).
Please specify the same version name as the CUDA Toolkit version after the package name.</p>
<pre><code>-- For RHEL9
# dnf module install nvidia-driver:open-dkms
-- For RHEL10
# dnf install nvidia-open
</code></pre>
<p>Once installation completed successfully, CUDA Toolkit is deployed at <code>/usr/local/cuda</code>.</p>
<pre><code># ls /usr/local/cuda/
bin                             extras   man               share
compute-sanitizer               gds      nsightee_plugins  src
CUDA_Toolkit_Release_Notes.txt  include  nvml              targets
DOCS                            lib64    nvvm              tools
EULA.txt                        LICENSE  README            version.json
</code></pre>
<p>Once installation gets completed, ensure the system recognizes the GPU devices correctly.
<code>nvidia-smi</code> command shows GPU information installed on your system, as follows.</p>
<pre><code># nvidia-smi
Tue Oct 28 14:22:43 2025
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               Off |   00000000:42:00.0 Off |                    0 |
| N/A   37C    P0             48W /  350W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
</code></pre>
<div class="admonition tips">
<p class="admonition-title">Tips</p>
<p><strong>Additional configurations for systems with NVSwitch</strong></p>
<p>For systems with multiple GPUs that use NVSwitch for interconnect them, the <code>nvidia-fabricmanager</code> module must be installed.
If this package is not installed, <code>cuInit()</code>, which initializes CUDA, will fail with the <code>CUDA_ERROR_SYSTEM_NOT_READY</code> error, and PG-Strom will not start. 
Run the following commands to install the <code>nvidia-fabricmanager</code> package.
<a href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-error-system-not-ready-after-installation">(source)</a></p>
</div>
<pre><code># dnf install nvidia-fabricmanager
# systemctl enable nvidia-fabricmanager.service
# systemctl start nvidia-fabricmanager.service
</code></pre>
<h3 id="check-gpudirect-storage-status">Check GPUDirect Storage status</h3>
<p>After the installation of CUDA Toolkit according to the steps above, your system will become ready for the GPUDirect Storage.
Run <code>gdscheck</code> tool to confirm the configuration for each storage devices, as follows.
(Thie example loads not only <code>nvme</code>, but <code>nvme-rdma</code> and <code>rpcrdma</code> kernel modules also, therefore, it reports the related features as <code>Supported</code>)</p>
<pre><code># /usr/local/cuda/gds/tools/gdscheck -p
 GDS release version: 1.15.1.6
 nvidia_fs version:  2.26 libcufile version: 2.12
 Platform: x86_64
 ============
 ENVIRONMENT:
 ============
 =====================
 DRIVER CONFIGURATION:
 =====================
 NVMe P2PDMA        : Unsupported
 NVMe               : Supported
 NVMeOF             : Unsupported
 SCSI               : Unsupported
 ScaleFlux CSD      : Unsupported
 NVMesh             : Unsupported
 DDN EXAScaler      : Unsupported
 IBM Spectrum Scale : Unsupported
 NFS                : Supported
 BeeGFS             : Unsupported
 ScaTeFS            : Unsupported
 WekaFS             : Unsupported
 Userspace RDMA     : Unsupported
 --Mellanox PeerDirect : Disabled
 --rdma library        : Not Loaded (libcufile_rdma.so)
 --rdma devices        : Not configured
 --rdma_device_status  : Up: 0 Down: 0
 =====================
 CUFILE CONFIGURATION:
 =====================
 properties.use_pci_p2pdma : false
 properties.use_compat_mode : true
 properties.force_compat_mode : false
 properties.gds_rdma_write_support : true
 properties.use_poll_mode : false
 properties.poll_mode_max_size_kb : 4
 properties.max_batch_io_size : 128
 properties.max_batch_io_timeout_msecs : 5
 properties.max_direct_io_size_kb : 16384
 properties.max_device_cache_size_kb : 131072
 properties.per_buffer_cache_size_kb : 1024
 properties.max_device_pinned_mem_size_kb : 33554432
 properties.posix_pool_slab_size_kb : 4 1024 16384
 properties.posix_pool_slab_count : 128 64 64
 properties.rdma_peer_affinity_policy : RoundRobin
 properties.rdma_dynamic_routing : 0
 fs.generic.posix_unaligned_writes : false
 fs.lustre.posix_gds_min_kb: 0
 fs.beegfs.posix_gds_min_kb: 0
 fs.scatefs.posix_gds_min_kb: 0
 fs.weka.rdma_write_support: false
 fs.gpfs.gds_write_support: false
 fs.gpfs.gds_async_support: true
 profile.nvtx : false
 profile.cufile_stats : 0
 miscellaneous.api_check_aggressive : false
 execution.max_io_threads : 4
 execution.max_io_queue_depth : 128
 execution.parallel_io : true
 execution.min_io_threshold_size_kb : 8192
 execution.max_request_parallelism : 4
 properties.force_odirect_mode : false
 properties.prefer_iouring : false
 =========
 GPU INFO:
 =========
 GPU index 0 NVIDIA H100 PCIe bar:1 bar size (MiB):131072 supports GDS, IOMMU State: Disabled
 ==============
 PLATFORM INFO:
 ==============
 IOMMU: disabled
 Nvidia Driver Info Status: Supported(Nvidia Open Driver Installed)
 Cuda Driver Version Installed:  13000
 Platform: AS -2015CS-TNR, Arch: x86_64(Linux 6.12.0-55.41.1.el10_0.x86_64)
 Platform verification succeeded
</code></pre>
<div class="admonition tips">
<p class="admonition-title">Tips</p>
<p><strong>Additional configuration for RAID volume</strong></p>
<p>For data reading from software RAID (md-raid0) volumes by GPUDirect Storage,
the following line must be added to the <code>/lib/udev/rules.d/63-md-raid-arrays.rules</code> configuration file.</p>
<p><code>IMPORT{â€‹program}="/usr/sbin/mdadm --detail --export $devnode"</code></p>
<p>Then reboot the system to ensure the new configuration.
See <a href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#adding-udev-rules">NVIDIA GPUDirect Storage Installation and Troubleshooting Guide</a> for the details.</p>
</div>
<h3 id="pci-bar1-memory-configuration">PCI Bar1 Memory Configuration</h3>
<p>GPU-Direct SQL maps GPU device memory to the PCI BAR1 region (physical address space) on the host system, and sends P2P-RDMA requests to NVME devices with that as the destination for the shortest data transfer.</p>
<p>To perform P2P-RDMA with sufficient multiplicity, the GPU must have enough PCI BAR1 space to map the device buffer. The size of the PCI BAR1 area is fixed for most GPUs, and PG-Strom recommends products whose size exceeds the GPU device memory size.</p>
<p>However, some GPU products allow to change the size of the PCI BAR1 area by switching the operation mode. If your GPU is either of the following, refer to the <a href="https://developer.nvidia.com/displaymodeselector">NVIDIA Display Mode Selector Tool</a> and switch to the mode that maximizes the PCI BAR1 area size.</p>
<ul>
<li>NVIDIA L40S</li>
<li>NVIDIA L40</li>
<li>NVIDIA A40</li>
<li>NVIDIA RTX 6000 Ada</li>
<li>NVIDIA RTX A6000</li>
<li>NVIDIA RTX A5500</li>
<li>NVIDIA RTX A5000</li>
</ul>
<p>To check the GPU memory size and PCI BAR1 size installed in the system, use the <code>nvidia-smi -q</code> command. Memory-related status is displayed as shown below.</p>
<pre><code>$ nvidia-smi -q
        :
    FB Memory Usage
        Total                             : 46068 MiB
        Reserved                          : 685 MiB
        Used                              : 4 MiB
        Free                              : 45377 MiB
    BAR1 Memory Usage
        Total                             : 65536 MiB
        Used                              : 1 MiB
        Free                              : 65535 MiB
        :
</code></pre>
<h3 id="edit-etccufilejson">Edit <code>/etc/cufile.json</code></h3>
<p>Edit the configuration file <code>/etc/cufile.json</code>, which is used internally by GPU Direct SQL.</p>
<p>In the JSON-formatted configuration, change the value of <code>"parallel_io"</code> in the <code>"execution"</code> section to <code>false</code>, from the default value (<code>true</code>).</p>
<pre><code>  &quot;execution&quot; : {
          :
      // enable support for parallel IO
      &quot;parallel_io&quot; : false,
          :
  },
</code></pre>
<div class="admonition tips">
<p class="admonition-title">Tips</p>
<p>cuFile splits I/O requests larger than a certain size (default 8MB) and uses an internal thread pool to execute multiple I/O requests in parallel.
While this is an effective mechanism for single-threaded GPU applications to perform I/O efficiently, it actually increases overhead for applications like PG-Strom, which are multi-threaded and execute finely granular I/O requests concurrently.
Therefore, PG-Strom recommends disabling cuFile's Parallel-I/O functionality.</p>
</div>
<h2 id="heterodb-extra-modules">HeteroDB extra modules</h2>
<p>PG-Strom and related packages are distributed from <a href="https://heterodb.github.io/swdc/">HeteroDB Software Distribution Center</a>.
You need to add a repository definition of HeteroDB-SWDC for you system to obtain these software.</p>
<p><code>heterodb-swdc</code> package provides the repository definition of HeteroDB-SWDC.
Access to the <a href="https://heterodb.github.io/swdc/">HeteroDB Software Distribution Center</a> using Web browser, download the <code>heterodb-swdc</code> package on top of the file list, then install this package. (Use <code>heterodb-swdc-1.3-1.el10.noarch.rpm</code> for RHEL10)
Once heterodb-swdc package gets installed, yum system configuration is updated to get software from the HeteroDB-SWDC repository.</p>
<p>Install the <code>heterodb-swdc</code> package as follows.</p>
<pre><code># dnf install https://heterodb.github.io/swdc/yum/rhel10-noarch/heterodb-swdc-1.3-1.el10.noarch.rpm
</code></pre>
<p><code>heterodb-extra</code> module enhances PG-Strom the following features.</p>
<ul>
<li>multi-GPUs support</li>
<li>GPUDirect SQL</li>
<li>GiST index support on GPU</li>
<li>License management</li>
</ul>
<p>If you don't use the above features, only open source modules, you don't need to install the <code>heterodb-extra</code> module here.
Please skip this section.</p>
<p>Install the <code>heterodb-extra</code> package, downloaded from the SWDC, as follows.</p>
<pre><code># dnf install heterodb-extra
</code></pre>
<h3 id="license-activation">License activation</h3>
<p>License activation is needed to use all the features of <code>heterodb-extra</code>, provided by HeteroDB,Inc. You can operate the system without license, but features below are restricted.</p>
<ul>
<li>Multiple GPUs support</li>
<li>Striping of NVME-SSD drives (md-raid0) on GPUDirect SQL</li>
<li>Support of NVME-oF device on GPUDirect SQL</li>
<li>Support of GiST index on GPU-version of PostGIS workloads</li>
</ul>
<p>You can obtain a license file, like as a plain text below, from HeteroDB,Inc.</p>
<pre><code>IAgIVdKxhe+BSer3Y67jQW0+uTzYh00K6WOSH7xQ26Qcw8aeUNYqJB9YcKJTJb+QQhjmUeQpUnboNxVwLCd3HFuLXeBWMKp11/BgG0FSrkUWu/ZCtDtw0F1hEIUY7m767zAGV8y+i7BuNXGJFvRlAkxdVO3/K47ocIgoVkuzBfLvN/h9LffOydUnHPzrFHfLc0r3nNNgtyTrfvoZiXegkGM9GBTAKyq8uWu/OGonh9ybzVKOgofhDLk0rVbLohOXDhMlwDl2oMGIr83tIpCWG+BGE+TDwsJ4n71Sv6n4bi/ZBXBS498qShNHDGrbz6cNcDVBa+EuZc6HzZoF6UrljEcl=
----
VERSION:2
SERIAL_NR:HDB-TRIAL
ISSUED_AT:2019-05-09
EXPIRED_AT:2019-06-08
GPU_UUID:GPU-a137b1df-53c9-197f-2801-f2dccaf9d42f
</code></pre>
<p>Copy the license file to <code>/etc/heterodb.license</code>, then restart PostgreSQL.</p>
<p>The startup log messages of PostgreSQL dumps the license information, and it tells us the license activation is successfully done.</p>
<pre><code>    :
 LOG:  HeteroDB Extra module loaded [api_version=20231105,cufile=on,nvme_strom=off,githash=9ca2fe4d2fbb795ad2d741dcfcb9f2fe499a5bdf]
 LOG:  HeteroDB License: { &quot;version&quot; : 2, &quot;serial_nr&quot; : &quot;HDB-TRIAL&quot;, &quot;issued_at&quot; : &quot;2022-11-19&quot;, &quot;expired_at&quot; : &quot;2099-12-31&quot;, &quot;nr_gpus&quot; : 1, &quot;gpus&quot; : [ { &quot;uuid&quot; : &quot;GPU-13943bfd-5b30-38f5-0473-78979c134606&quot; } ]}
 LOG:  PG-Strom version 5.0.1 built for PostgreSQL 15 (githash: 972441dbafed6679af86af40bc8613be2d73c4fd)
    :
</code></pre>
<h2 id="postgresql-installation">PostgreSQL Installation</h2>
<p>This section introduces PostgreSQL installation with RPM.
We don't introduce the installation steps from the source because there are many documents for this approach, and there are also various options for the <code>./configure</code> script.</p>
<p>PostgreSQL is also distributed in the packages of Linux distributions, however, it is not the latest one, and often older than the version which supports PG-Strom. For example, Red Hat Enterprise Linux 7.x distributes PostgreSQL v9.2.x series. This version had been EOL by the PostgreSQL community.</p>
<p>PostgreSQL Global Development Group provides yum repository to distribute the latest PostgreSQL and related packages.
Like the configuration of EPEL, you can install a small package to set up yum repository, then install PostgreSQL and related software.</p>
<p>Here is the list of yum repository definition: <a href="http://yum.postgresql.org/repopackages.php">http://yum.postgresql.org/repopackages.php</a>.</p>
<p>Repository definitions are per PostgreSQL major version and Linux distribution. You need to choose the one for your Linux distribution, and for PostgreSQL v15 or later.</p>
<p>You can install PostgreSQL as following steps:</p>
<ul>
<li>Installation of yum repository definition.</li>
<li>Disables the distribution's default PostgreSQL module</li>
<li>Installation of PostgreSQL packages.</li>
</ul>
<p>When PostgreSQL v18 is used for instance, installation of PG-Strom requires <code>postgresql18-server</code> and <code>postgresql18-devel</code> packages.</p>
<p>Below is the steps to install PostgreSQL v18 for RHEL10.</p>
<pre><code># dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-10-x86_64/pgdg-redhat-repo-latest.noarch.rpm
# dnf install -y postgresql18-devel postgresql18-server
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On the Red Hat Enterprise Linux 8.x or 9.x, the package name <code>postgresql</code> conflicts to the default one at the distribution, thus, unable to install the packages from PGDG. So, disable the <code>postgresql</code> module by the distribution, using <code>dnf -y module disable postgresql</code>.
The AppStream feature was improved, Red Hat Enterprise Linux 10.x does not need this operation.</p>
</div>
<h2 id="libarrowlibparquet-installation">libarrow/libparquet Installation</h2>
<p>PG-Strom v6.1 and later requires <code>libarrow</code> and <code>libparquet</code> for building and installation.</p>
<p>Since the packages provided by the Linux distribution may be outdated, please follow the <a href="https://arrow.apache.org/install/">developer community guidance</a> and install the <code>arrow-devel</code> and <code>parquet-devel</code> packages.</p>
<p>The steps required for a minimal installation, excluding any overlaps with the installation steps above, are as follows.</p>
<pre><code>$ sudo dnf install -y https://packages.apache.org/artifactory/arrow/almalinux/$(cut -d: -f5 /etc/system-release-cpe | cut -d. -f1)/apache-arrow-release-latest.rpm
$ sudo dnf install -y arrow-devel
$ sudo dnf install -y parquet-devel
</code></pre>
<h2 id="pg-strom-installation">PG-Strom Installation</h2>
<h3 id="rpm-installation">RPM Installation</h3>
<p>PG-Strom and related packages are distributed from <a href="https://heterodb.github.io/swdc/">HeteroDB Software Distribution Center</a>.
If you repository definition has been added, not many tasks are needed.</p>
<p>We provide individual RPM packages of PG-Strom for each PostgreSQL major version. <code>pg_strom-PG17</code> package is built for PostgreSQL v17, and <code>pg_strom-PG18</code> is also built for PostgreSQL v18.</p>
<p>It is a restriction due to binary compatibility of extension modules for PostgreSQL.</p>
<pre><code># dnf install -y pg_strom-PG18
</code></pre>
<p>That's all for package installation.</p>
<h3 id="installation-from-the-source">Installation from the source</h3>
<p>For developers, we also introduces the steps to build and install PG-Strom from the source code.</p>
<h4 id="getting-the-source-code">Getting the source code</h4>
<p>Like RPM packages, you can download tarball of the source code from <a href="https://heterodb.github.io/swdc/">HeteroDB Software Distribution Center</a>.
On the other hands, here is a certain time-lags to release the tarball, it may be preferable to checkout the master branch of <a href="https://github.com/heterodb/pg-strom">PG-Strom on GitHub</a> to use the latest development branch.</p>
<pre><code>$ git clone https://github.com/heterodb/pg-strom.git
Cloning into 'pg-strom'...
remote: Counting objects: 13797, done.
remote: Compressing objects: 100% (215/215), done.
remote: Total 13797 (delta 208), reused 339 (delta 167), pack-reused 13400
Receiving objects: 100% (13797/13797), 11.81 MiB | 1.76 MiB/s, done.
Resolving deltas: 100% (10504/10504), done.
</code></pre>
<h4 id="building-the-pg-strom">Building the PG-Strom</h4>
<p>Configuration to build PG-Strom must match to the target PostgreSQL strictly. For example, if a particular <code>strcut</code> has inconsistent layout by the configuration at build, it may lead problematic bugs; not easy to find out.
Thus, not to have inconsistency, PG-Strom does not have own configure script, but references the build configuration of PostgreSQL using <code>pg_config</code> command.</p>
<p>If PATH environment variable is set to the <code>pg_config</code> command of the target PostgreSQL, run <code>make</code> and <code>make install</code>.
Elsewhere, give <code>PG_CONFIG=...</code> parameter on <code>make</code> command to tell the full path of the <code>pg_config</code> command.</p>
<pre><code>$ cd pg-strom/src
$ make PG_CONFIG=/usr/pgsql-18/bin/pg_config
$ sudo make install PG_CONFIG=/usr/pgsql-18/bin/pg_config
</code></pre>
<h3 id="post-installation-setup">Post Installation Setup</h3>
<h3 id="creation-of-database-cluster">Creation of database cluster</h3>
<p>Database cluster is not constructed yet, run <code>initdb</code> command to set up initial database of PostgreSQL.</p>
<p>The default path of the database cluster on RPM installation is <code>/var/lib/pgsql/&lt;version number&gt;/data</code>.
If you install <code>postgresql-alternatives</code> package, this default path can be referenced by <code>/var/lib/pgdata</code> regardless of the PostgreSQL version.</p>
<pre><code># su - postgres
$ /usr/pgsql-18/bin/initdb -D /var/lib/pgsql/18/data
            :
Success. You can now start the database server using:

    /usr/pgsql-18/bin/pg_ctl -D /var/lib/pgsql/18/data -l logfile start
</code></pre>
<h3 id="setup-postgresqlconf">Setup postgresql.conf</h3>
<p>Next, edit <code>postgresql.conf</code> which is a configuration file of PostgreSQL.
The parameters below should be edited at least to work PG-Strom.
Investigate other parameters according to usage of the system and expected workloads.</p>
<ul>
<li><strong>shared_preload_libraries</strong><ul>
<li>PG-Strom module must be loaded on startup of the postmaster process by the <code>shared_preload_libraries</code>. Unable to load it on demand. Therefore, you must add the configuration below.</li>
<li><code>shared_preload_libraries = '$libdir/pg_strom'</code></li>
</ul>
</li>
<li><strong>max_worker_processes</strong><ul>
<li>PG-Strom internally uses several background workers, so the default configuration (= 8) is too small for other usage. So, we recommand to expand the variable for a certain margin.</li>
<li><code>max_worker_processes = 100</code></li>
</ul>
</li>
<li><strong>shared_buffers</strong><ul>
<li>Although it depends on the workloads, the initial configuration of <code>shared_buffers</code> is too small for the data size where PG-Strom tries to work, thus storage workloads restricts the entire performance, and may be unable to work GPU efficiently.</li>
<li>So, we recommend to expand the variable for a certain margin.</li>
<li><code>shared_buffers = 10GB</code></li>
<li>Please consider to apply <strong>SSD-to-GPU Direct SQL Execution</strong> to process larger than system's physical RAM size.</li>
</ul>
</li>
<li><strong>work_mem</strong><ul>
<li>Although it depends on the workloads, the initial configuration of <code>work_mem</code> is too small to choose the optimal query execution plan on analytic queries.</li>
<li>An typical example is, disk-based merge sort may be chosen instead of the in-memory quick-sorting.</li>
<li>So, we recommend to expand the variable for a certain margin.</li>
<li><code>work_mem = 1GB</code></li>
</ul>
</li>
</ul>
<h3 id="expand-os-resource-limits">Expand OS resource limits</h3>
<p>GPU Direct SQL especially tries to open many files simultaneously, so resource limit for number of file descriptors per process should be expanded.</p>
<p>Also, we recommend not to limit core file size to generate core dump of PostgreSQL certainly on system crash.</p>
<p>If PostgreSQL service is launched by systemd, you can put the configurations of resource limit at <code>/etc/systemd/system/postgresql-XX.service.d/pg_strom.conf</code>.</p>
<p>RPM installation setups the configuration below by the default.</p>
<p>It comments out configuration to the environment variable <code>CUDA_ENABLE_COREDUMP_ON_EXCEPTION</code>. This is a developer option that enables to generate GPU's core dump on any CUDA/GPU level errors, if enabled. See <a href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gpu-coredump">CUDA-GDB:GPU core dump support</a> for more details.</p>
<pre><code>[Service]
LimitNOFILE=65536
LimitCORE=infinity
#Environment=CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1
</code></pre>
<h3 id="start-postgresql">Start PostgreSQL</h3>
<p>Start PostgreSQL service.</p>
<p>If PG-Strom is set up appropriately, it writes out log message which shows PG-Strom recognized GPU devices.
The example below recognized two NVIDIA A100 (PCIE; 40GB), and displays the closest GPU identifier foe each NVME-SSD drive.</p>
<pre><code># systemctl start postgresql-18
# journalctl -u postgresql-18
# journalctl -u postgresql-18
Oct 28 16:17:16 saba.heterodb.in systemd[1]: Starting postgresql-18.service - PostgreSQL 18 database server...
Oct 28 16:17:16 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:16.574 JST [4325] LOG:  HeteroDB Extra module loaded [api_version=20250316,cufile=on,nvme_strom=off,nvidia-fs=on,githash=d5044aceca80aecee04c7c1a662c066122986291]
Oct 28 16:17:16 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:16.574 JST [4325] LOG:  HeteroDB License: { &quot;version&quot; : 2, &quot;serial_nr&quot; : &quot;HDB-TRIAL&quot;, &quot;issued_at&quot; : &quot;2025-10-28&quot;, &quot;expired_at&quot; : &quot;2099-12-31&quot;, &quot;nr_gpus&quot; : 1, &quot;gpus&quot; : [ { &quot;uuid&quot; : &quot;GPU-156b86ad-864c-d97d-4729-5c3405d8...
Oct 28 16:17:16 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:16.574 JST [4325] LOG:  PG-Strom version 6.1.0.el9 built for PostgreSQL 18 (githash: )
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.011 JST [4325] LOG:  PG-Strom binary built for CUDA 12.9 (CUDA runtime 13.0)
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.011 JST [4325] LOG:  PG-Strom: GPU0 NVIDIA H100 PCIe (114 SMs; 1755MHz, L2 51200kB), RAM 79.18GB (5120bits, 1.52GHz), CC 9.0
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:42:00:0] GPU0 (NVIDIA H100 PCIe; GPU-156b86ad-864c-d97d-4729-5c3405d88e76)
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:09:00:0] nvme8 (SAMSUNG MZ1LB960HAJQ-00007) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:c5:00:0] nvme2 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:88:00:0] nvme4 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:c3:00:0] nvme1 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:c7:00:0] nvme6 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:c1:00:0] nvme0 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.012 JST [4325] LOG:  [0000:41:00:0] mlx5_0 (MT4125) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.013 JST [4325] LOG:  [0000:82:00:0] nvme3 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.013 JST [4325] LOG:  [0000:01:00:0] nvme5 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.013 JST [4325] LOG:  [0000:41:00:1] mlx5_1 (MT4125) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.013 JST [4325] LOG:  [0000:84:00:0] nvme7 (INTEL SSDPF2KX038TZ) --&gt; GPU0 [dist=130]
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.142 JST [4325] LOG:  redirecting log output to logging collector process
Oct 28 16:17:17 saba.heterodb.in postgres[4325]: 2025-10-28 16:17:17.142 JST [4325] HINT:  Future log output will appear in directory &quot;log&quot;.
Oct 28 16:17:17 saba.heterodb.in systemd[1]: Started postgresql-18.service - PostgreSQL 18 database server.
</code></pre>
<h3 id="creation-of-pg-strom-extension">Creation of PG-Strom Extension</h3>
<p>At the last, create database objects related to PG-Strom, like SQL functions.
This steps are packaged using EXTENSION feature of PostgreSQL. So, all you needs to run is <code>CREATE EXTENSION</code> on the SQL command line.</p>
<p>Please note that this step is needed for each new database.
If you want PG-Strom is pre-configured on new database creation, you can create PG-Strom extension on the <code>template1</code> database, its configuration will be copied to the new database on <code>CREATE DATABASE</code> command.</p>
<pre><code>$ psql -U postgres
psql (16.3)
Type &quot;help&quot; for help.

postgres=# CREATE EXTENSION pg_strom ;
CREATE EXTENSION
</code></pre>
<p>That's all for the installation.</p>
<h2 id="postgis-installation">PostGIS Installation</h2>
<p>PG-Strom supports execution of a part of PostGIS functions on GPU devices.
This section introduces the steps to install PostGIS module. Skip it on your demand.</p>
<p>PostGIS module can be installed from the yum repository by PostgreSQL Global Development Group, like PostgreSQL itself.
The example below shows the command to install PostGIS v3.6 built for PostgreSQL v18.</p>
<pre><code># dnf install postgis36_18
</code></pre>
<p>Start PostgreSQL server after the initial setup of database cluster, then run <code>CREATE EXTENSION</code> command from SQL client to define geometry data type and SQL functions for geoanalytics.</p>
<pre><code>postgres=# CREATE EXTENSION postgis;
CREATE EXTENSION
</code></pre>
<h2 id="installation-on-ubuntu-linux">Installation on Ubuntu Linux</h2>
<p>Although PG-Strom packages are not available for Ubuntu Linux right now, you can build and run PG-Strom from the source code.</p>
<p>After the installation of Ubuntu Linux, install the MOFED driver, CUDA Toolkit, and PostgreSQL for Ubuntu Linux, respectively.</p>
<p>Next, install the <code>heterodb-extra</code> package.
A <code>.deb</code> package for Ubuntu Linux is provided, so please obtain the latest version from the <a href="https://heterodb.github.io/swdc/">SWDC</a>.</p>
<pre><code>$ wget https://heterodb.github.io/swdc/deb/heterodb-extra_6.7-1_amd64.deb
$ sudo dpkg -i heterodb-extra_6.7-1_amd64.deb
</code></pre>
<p>Checkout the source code of PG-Strom, build and install as follows.
At this time, do not forget to specify the target PostgreSQL by <code>pg_config</code>.</p>
<p>Post-installation configuration is almost same as for Red Hat Enterprise Linux or Rocky Linux.</p>
<pre><code>$ git clone https://github.com/heterodb/pg-strom.git
$ cd pg-strom/src
$ make PG_CONFIG=/path/to/pgsql/bin/pg_config -j 8
$ sudo make PG_CONFIG=/path/to/pgsql/bin/pg_config install
</code></pre>
<p>However, if you use a packaged PostgreSQL and start it by systemctl command, the PATH environment variable will be cleared (probably for security reasons).
As a result, the script launched to build the GPU binary on the first startup will not work properly.
To avoid this, if you are using Ubuntu Linux, add the following line to <code>/etc/postgresql/main/PGVERSION/environment</code>.</p>
<pre><code>/etc/postgresql/PGVERSION/main/environment:


PATH='/usr/local/cuda/bin:/usr/bin:/bin'
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href=".." class="btn btn-neutral float-left" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../operations/" class="btn btn-neutral float-right" title="Basic Operations">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href=".." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../operations/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
